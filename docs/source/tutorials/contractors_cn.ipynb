{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae160d4",
   "metadata": {},
   "source": [
    "# contractor 使用\n",
    "\n",
    "## 概述\n",
    "\n",
    "在本教程中，我们将演示如何利用不同类型的张量网络 contractor 进行电路仿真，以实现更好的时空消耗平衡。contractor 的定制是 TensorCircuit 库的主要亮点之一，因为更好的 contractor 可以更好地利用 TensorNetwok 仿真引擎的强大功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e689831",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770fe50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorcircuit as tc\n",
    "import numpy as np\n",
    "import cotengra as ctg\n",
    "import opt_einsum as oem\n",
    "\n",
    "K = tc.set_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a1e8d",
   "metadata": {},
   "source": [
    "## 试验体系\n",
    "\n",
    "我们为两个电路提供张量网络，并测试这两个系统的收缩效率，第一个系统小，第二个系统大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef09ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取小系统的量子态\n",
    "def small_tn():\n",
    "    n = 10\n",
    "    d = 4\n",
    "    param = K.ones([2 * d, n])\n",
    "    c = tc.Circuit(n)\n",
    "    c = tc.templates.blocks.example_block(c, param, nlayers=d)\n",
    "    return c.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80c3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得对超大型系统的期望\n",
    "def large_tn():\n",
    "    n = 60\n",
    "    d = 8\n",
    "    param = K.ones([2 * d, n])\n",
    "    c = tc.Circuit(n)\n",
    "    c = tc.templates.blocks.example_block(c, param, nlayers=d, is_split=True)\n",
    "    #\n",
    "    return c.expectation([tc.gates.z(), [n // 2]], reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd06739",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## opt-einsum\n",
    "\n",
    "opt-einsum 提供了几个 contractor 优化器，并与 TensorNetwork 包一起提供。 由于 TensorCircuit 建立在 TensorNetwork 之上，我们可以使用这些简单的 contractor 优化器。 尽管对于任何中等系统，只有贪婪优化器有效，但其他优化器具有指数缩放并且在电路仿真场景中失效。我们总是为 contractor 系统设置``contraction_info=True``（默认为``False``），它将打印包括 contraction size、flops 和 writes 在内的收缩信息摘要。 有关这些指标的定义，另请参阅 cotengra 文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35a1974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e281f0>, memory_limit=None, debug_level=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果我们什么都不设置，默认优化器是贪婪的，即:\n",
    "tc.set_contractor(\"greedy\", debug_level=2, contraction_info=True)\n",
    "# 我们设置 debug_level=2 以不真正运行收缩计算\n",
    "# 即通过设置debug_level>0，只有收缩信息和返回形状正确，结果错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25180630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  5.132  log2[SIZE]:  11  log2[WRITE]:  13.083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1024,), dtype=complex64, numpy=\n",
       "array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "      dtype=complex64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8b26b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  17.766  log2[SIZE]:  44  log2[WRITE]:  49.636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=complex64, numpy=0j>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cb17e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function custom_stateful at 0x7fd5a0a3d4c0>, optimizer=<class 'opt_einsum.path_random.RandomGreedy'>, opt_conf=None, contraction_info=True, debug_level=2, max_time=60, max_repeats=128, minimize='size')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以在 opt-einsum 中使用更多花哨的 contractor，他们不在 TensorNetwork 中定义\n",
    "# custom_stateful 用于具有一次性生命周期的路径求解器的收缩路径查找器\n",
    "tc.set_contractor(\n",
    "    \"custom_stateful\",\n",
    "    optimizer=oem.RandomGreedy,\n",
    "    max_time=60,\n",
    "    max_repeats=128,\n",
    "    minimize=\"size\",\n",
    "    debug_level=2,\n",
    "    contraction_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3441b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  4.925  log2[SIZE]:  10  log2[WRITE]:  12.531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1024,), dtype=complex64, numpy=\n",
       "array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "      dtype=complex64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef946a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  11.199  log2[SIZE]:  26  log2[WRITE]:  28.183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=complex64, numpy=0j>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_tn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf774c11",
   "metadata": {},
   "source": [
    "## cotengra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466a596",
   "metadata": {},
   "source": [
    "对于更高级的 contractor，我们向市场上的 sota contractor 优化器寻求帮助：cotengra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07b777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e28ee0>, memory_limit=None, debug_level=2, preprocessing=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ctg.ReusableHyperOptimizer(\n",
    "    methods=[\"greedy\", \"kahypar\"],\n",
    "    parallel=True,\n",
    "    minimize=\"write\",\n",
    "    max_time=120,\n",
    "    max_repeats=1024,\n",
    "    progbar=True,\n",
    ")\n",
    "tc.set_contractor(\n",
    "    \"custom\", optimizer=opt, preprocessing=True, contraction_info=True, debug_level=2\n",
    ")\n",
    "## 有关 cotengra 优化器的更多设置，请参阅参考资料\n",
    "## https://cotengra.readthedocs.io/en/latest/advanced.html\n",
    "## preprocessing=True 将所有单量子比特门合并到相邻的双量子比特门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4ffe73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log2[SIZE]: 10.00 log10[FLOPs]: 4.90: 100%|█████████████████████████████████████████| 1024/1024 [00:28<00:00, 35.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  4.900  log2[SIZE]:  10  log2[WRITE]:  12.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1024,), dtype=complex64, numpy=\n",
       "array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "      dtype=complex64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8a440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log2[SIZE]: 20.00 log10[FLOPs]: 9.50:   4%|█▊                                         | 43/1024 [02:09<49:22,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  9.501  log2[SIZE]:  20  log2[WRITE]:  24.090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=complex64, numpy=0j>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_tn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5e59f",
   "metadata": {},
   "source": [
    "我们也可以在 cotengra 找到路径之后应用 subtree reconf，这通常会进一步（并且通常会大大）为 contraction 改善 flops 和 writes。实际上, subtree reconf 后期处理通常比增加优化器的搜索时间或重复次数更重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2594e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd58c832700>, memory_limit=None, debug_level=2, preprocessing=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ctg.ReusableHyperOptimizer(\n",
    "    minimize=\"combo\",\n",
    "    max_repeats=1024,\n",
    "    max_time=240,\n",
    "    progbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "def opt_reconf(inputs, output, size, **kws):\n",
    "    tree = opt.search(inputs, output, size)\n",
    "    tree_r = tree.subtree_reconfigure_forest(\n",
    "        progbar=True, num_trees=10, num_restarts=20, subtree_weight_what=(\"size\",)\n",
    "    )\n",
    "    return tree_r.get_path()\n",
    "\n",
    "\n",
    "tc.set_contractor(\n",
    "    \"custom\",\n",
    "    optimizer=opt_reconf,\n",
    "    contraction_info=True,\n",
    "    preprocessing=True,\n",
    "    debug_level=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5214d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log2[SIZE]: 10.00 log10[FLOPs]: 4.87: 100%|█████████████████████████████████████████| 1024/1024 [02:29<00:00,  6.86it/s]\n",
      "log2[SIZE]: 10.00 log10[FLOPs]: 4.86: 100%|█████████████████████████████████████████████| 20/20 [00:31<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  4.859  log2[SIZE]:  10  log2[WRITE]:  12.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1024,), dtype=complex64, numpy=\n",
       "array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "      dtype=complex64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23eabe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log2[SIZE]: 21.00 log10[FLOPs]: 9.62:   9%|███▉                                       | 93/1024 [04:04<40:49,  2.63s/it]\n",
      "log2[SIZE]: 17.00 log10[FLOPs]: 8.66: 100%|█████████████████████████████████████████████| 20/20 [03:08<00:00,  9.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ contraction cost summary ------\n",
      "log10[FLOPs]:  8.657  log2[SIZE]:  17  log2[WRITE]:  25.035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=complex64, numpy=0j>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_tn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355b1dc",
   "metadata": {},
   "source": [
    "我们也可以直接提取张量网络用于电路或可观测计算，我们可以使用我们喜欢的任何方法进行收缩。此外，所有这些 contractor 或我们定制的外部收缩仍然可以与 jit，自动微分等兼容。具体来说，收缩路径求解器虽然需要一些时间成本，但由于 jit 基础设施只测量一次（注意，为了演示使用，我们在这里不使用``K.jit``装饰我们的收缩函数）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-02 14:19+0800\n"
"PO-Revision-Date: 2022-04-13 14:58+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: cn <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/api/abstractcircuit.rst:2
msgid "tensorcircuit.abstractcircuit"
msgstr ""

#: of tensorcircuit.abstractcircuit:1
msgid "Methods for abstract circuits independent of nodes, edges and contractions"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit:1
#: tensorcircuit.applications.utils.FakeModule:1
#: tensorcircuit.applications.vqes.VQNHE:1
#: tensorcircuit.backends.jax_backend.optax_optimizer:1
#: tensorcircuit.backends.pytorch_backend.torch_optimizer:1
#: tensorcircuit.backends.tensorflow_backend.keras_optimizer:1
#: tensorcircuit.gates.GateF:1 tensorcircuit.noisemodel.NoiseConf:1
#: tensorcircuit.quantum.QuOperator:1
#: tensorcircuit.results.readout_mitigation.ReadoutMit:1
#: tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Bases: :py:class:`object`"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append:1
msgid "append circuit ``c`` before"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append
#: tensorcircuit.templates.measurements.any_local_measurements
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.spin_glass_measurements
msgid "example"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss
#: keras.engine.base_layer.Layer.add_metric
#: keras.engine.base_layer.Layer.add_update
#: keras.engine.base_layer.Layer.add_weight keras.engine.base_layer.Layer.apply
#: keras.engine.base_layer.Layer.build
#: keras.engine.base_layer.Layer.compute_mask
#: keras.engine.base_layer.Layer.compute_output_shape
#: keras.engine.base_layer.Layer.compute_output_signature
#: keras.engine.base_layer.Layer.from_config
#: keras.engine.base_layer.Layer.get_input_at
#: keras.engine.base_layer.Layer.get_input_mask_at
#: keras.engine.base_layer.Layer.get_input_shape_at
#: keras.engine.base_layer.Layer.get_losses_for
#: keras.engine.base_layer.Layer.get_output_at
#: keras.engine.base_layer.Layer.get_output_mask_at
#: keras.engine.base_layer.Layer.get_output_shape_at
#: keras.engine.base_layer.Layer.get_updates_for
#: keras.engine.base_layer.Layer.set_weights keras.engine.training.Model.build
#: keras.engine.training.Model.compile keras.engine.training.Model.evaluate
#: keras.engine.training.Model.fit keras.engine.training.Model.from_config
#: keras.engine.training.Model.get_layer
#: keras.engine.training.Model.load_weights
#: keras.engine.training.Model.make_predict_function
#: keras.engine.training.Model.make_test_function
#: keras.engine.training.Model.make_train_function
#: keras.engine.training.Model.predict
#: keras.engine.training.Model.predict_on_batch
#: keras.engine.training.Model.predict_step keras.engine.training.Model.save
#: keras.engine.training.Model.save_spec
#: keras.engine.training.Model.save_weights keras.engine.training.Model.summary
#: keras.engine.training.Model.test_on_batch
#: keras.engine.training.Model.test_step keras.engine.training.Model.to_json
#: keras.engine.training.Model.to_yaml
#: keras.engine.training.Model.train_on_batch
#: keras.engine.training.Model.train_step
#: keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule.from_config
#: of tensorcircuit.abstractcircuit.AbstractCircuit.append
#: tensorcircuit.abstractcircuit.AbstractCircuit.append_from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.abstractcircuit.AbstractCircuit.barrier_instruction
#: tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement
#: tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_count
#: tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping
#: tensorcircuit.abstractcircuit.AbstractCircuit.inverse
#: tensorcircuit.abstractcircuit.AbstractCircuit.measure_instruction
#: tensorcircuit.abstractcircuit.AbstractCircuit.prepend
#: tensorcircuit.abstractcircuit.AbstractCircuit.reset_instruction
#: tensorcircuit.abstractcircuit.AbstractCircuit.select_gate
#: tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_cirq
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qiskit
#: tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.van.MADE.call
#: tensorcircuit.applications.van.MaskedConv2D.build
#: tensorcircuit.applications.van.MaskedConv2D.call
#: tensorcircuit.applications.van.MaskedLinear.call
#: tensorcircuit.applications.van.NMF.call
#: tensorcircuit.applications.van.PixelCNN.call
#: tensorcircuit.applications.van.ResidualBlock.call
#: tensorcircuit.applications.vqes.Linear.call
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.get_random_state
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.set_random_state
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.arange
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.atanh
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.concat
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.device
#: tensorcircuit.backends.jax_backend.JaxBackend.device_move
#: tensorcircuit.backends.jax_backend.JaxBackend.dtype
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.eye
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.left_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.mod
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.searchsorted
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.std
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.jax_backend._svd_jax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.pytorch_backend._qr_torch
#: tensorcircuit.backends.pytorch_backend._rq_torch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.backends.tensorflow_backend._qr_tf
#: tensorcircuit.backends.tensorflow_backend._rq_tf
#: tensorcircuit.backends.tensorflow_backend._tensordot_tf
#: tensorcircuit.basecircuit.BaseCircuit.amplitude
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before
#: tensorcircuit.basecircuit.BaseCircuit.measure_jit
#: tensorcircuit.basecircuit.BaseCircuit.perfect_sampling
#: tensorcircuit.basecircuit.BaseCircuit.readouterror_bs
#: tensorcircuit.basecircuit.BaseCircuit.replace_inputs
#: tensorcircuit.basecircuit.BaseCircuit.sample
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.check_rep_transformation
#: tensorcircuit.channels.choi_to_kraus tensorcircuit.channels.choi_to_super
#: tensorcircuit.channels.composedkraus
#: tensorcircuit.channels.depolarizingchannel tensorcircuit.channels.evol_kraus
#: tensorcircuit.channels.evol_superop
#: tensorcircuit.channels.generaldepolarizingchannel
#: tensorcircuit.channels.is_hermitian_matrix
#: tensorcircuit.channels.kraus_identity_check
#: tensorcircuit.channels.kraus_to_choi tensorcircuit.channels.kraus_to_super
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.krausgate_to_krausmatrix
#: tensorcircuit.channels.krausmatrix_to_krausgate
#: tensorcircuit.channels.phasedampingchannel tensorcircuit.channels.reshuffle
#: tensorcircuit.channels.super_to_choi tensorcircuit.channels.super_to_kraus
#: tensorcircuit.channels.thermalrelaxationchannel
#: tensorcircuit.circuit.Circuit.__init__
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.depolarizing_reference
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.general_kraus
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.mid_measurement
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.runtime_backend
#: tensorcircuit.cons.runtime_dtype tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.cons.split_rules
#: tensorcircuit.densitymatrix.DMCircuit.__init__
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.to_circuit
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply
#: tensorcircuit.experimental.hamiltonian_evol
#: tensorcircuit.experimental.parameter_shift_grad
#: tensorcircuit.experimental.parameter_shift_grad_v2
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity
#: tensorcircuit.gates.get_u_parameter tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.multicontrol_gate
#: tensorcircuit.gates.num_to_tensor tensorcircuit.gates.phase_gate
#: tensorcircuit.gates.r_gate tensorcircuit.gates.rgate_theoretical
#: tensorcircuit.gates.rx_gate tensorcircuit.gates.rxx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.ryy_gate
#: tensorcircuit.gates.rz_gate tensorcircuit.gates.rzz_gate
#: tensorcircuit.gates.u_gate tensorcircuit.interfaces.numpy.numpy_interface
#: tensorcircuit.interfaces.scipy.scipy_optimize_interface
#: tensorcircuit.interfaces.tensorflow.tensorflow_interface
#: tensorcircuit.interfaces.tensortrans.args_to_tensor
#: tensorcircuit.interfaces.tensortrans.general_args_to_numpy
#: tensorcircuit.interfaces.tensortrans.numpy_args_to_backend
#: tensorcircuit.interfaces.tensortrans.which_backend
#: tensorcircuit.interfaces.torch.torch_interface
#: tensorcircuit.keras.QuantumLayer.__init__
#: tensorcircuit.keras.QuantumLayer.build tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss tensorcircuit.keras.save_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.__init__
#: tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.mid_measurement
#: tensorcircuit.mpscircuit.MPSCircuit.position
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.set_split_rules
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.noisemodel.NoiseConf.add_noise
#: tensorcircuit.noisemodel.apply_qir_with_noise
#: tensorcircuit.noisemodel.circuit_with_noise
#: tensorcircuit.noisemodel.expectation_noisfy
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy
#: tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2COO_tf
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.__init__
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.__init__
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.__init__
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.check_spaces
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.correlation_from_samples
#: tensorcircuit.quantum.count_d2s tensorcircuit.quantum.count_s2d
#: tensorcircuit.quantum.count_tuple2dict
#: tensorcircuit.quantum.count_vector2dict tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.sample2all tensorcircuit.quantum.sample2count
#: tensorcircuit.quantum.sample_bin2int tensorcircuit.quantum.sample_int2bin
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.results.counts.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.__init__
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_api
#: tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_system
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability
#: tensorcircuit.results.readout_mitigation.ReadoutMit.newrange
#: tensorcircuit.results.readout_mitigation.ReadoutMit.ubs
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.qft
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.__init__
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_local_measurements
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.torchnn.QuantumNet.__init__ tensorcircuit.translation.eqasm2tc
#: tensorcircuit.translation.perm_matrix tensorcircuit.translation.qir2cirq
#: tensorcircuit.translation.qir2json tensorcircuit.translation.qir2qiskit
#: tensorcircuit.translation.qiskit2tc
#: tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure
#: tensorcircuit.utils.append tensorcircuit.utils.arg_alias
#: tensorcircuit.utils.benchmark tensorcircuit.utils.return_partial
#: tensorcircuit.vis.gate_name_trans tensorcircuit.vis.qir2tex
#: tensorcircuit.vis.render_pdf
#: tensorflow.python.module.module.Module.with_name_scope
#: tensornetwork.backends.abstract_backend.AbstractBackend.deserialize_tensor
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigs
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres
#: tensornetwork.backends.abstract_backend.AbstractBackend.pivot
#: tensornetwork.backends.abstract_backend.AbstractBackend.power
#: tensornetwork.backends.abstract_backend.AbstractBackend.serialize_tensor
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigh
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eps
#: tensornetwork.backends.jax.jax_backend.JaxBackend.index_update
#: tensornetwork.backends.jax.jax_backend.JaxBackend.inv
#: tensornetwork.backends.jax.jax_backend.JaxBackend.item
#: tensornetwork.backends.jax.jax_backend.JaxBackend.power
#: tensornetwork.backends.jax.jax_backend.JaxBackend.reshape
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tensor
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tuple
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sign
#: tensornetwork.backends.jax.jax_backend.JaxBackend.slice
#: tensornetwork.backends.jax.jax_backend.JaxBackend.tensordot
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.deserialize_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.index_update
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.item
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.reshape
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.serialize_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tuple
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.sign
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.slice
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.tensordot
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.index_update
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.item
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.reshape
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tensor
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tuple
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.sign
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.slice
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.tensordot
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.index_update
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.item
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.reshape
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tensor
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tuple
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sign
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.slice
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.position
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs
#: tensornetwork.network_components.AbstractNode.add_axis_names
#: tensornetwork.network_components.AbstractNode.add_edge
#: tensornetwork.network_components.AbstractNode.get_dimension
#: tensornetwork.network_components.AbstractNode.reorder_axes
#: tensornetwork.network_components.AbstractNode.reorder_edges
#: tensornetwork.network_components.Node.__init__
#: tensornetwork.network_components.Node.from_serial_dict
#: torch.nn.modules.module.Module.add_module
#: torch.nn.modules.module.Module.apply torch.nn.modules.module.Module.buffers
#: torch.nn.modules.module.Module.cuda
#: torch.nn.modules.module.Module.get_buffer
#: torch.nn.modules.module.Module.get_parameter
#: torch.nn.modules.module.Module.get_submodule
#: torch.nn.modules.module.Module.load_state_dict
#: torch.nn.modules.module.Module.named_buffers
#: torch.nn.modules.module.Module.named_modules
#: torch.nn.modules.module.Module.named_parameters
#: torch.nn.modules.module.Module.parameters
#: torch.nn.modules.module.Module.register_buffer
#: torch.nn.modules.module.Module.register_parameter
#: torch.nn.modules.module.Module.requires_grad_
#: torch.nn.modules.module.Module.set_extra_state
#: torch.nn.modules.module.Module.to torch.nn.modules.module.Module.to_empty
#: torch.nn.modules.module.Module.train torch.nn.modules.module.Module.type
#: torch.nn.modules.module.Module.xpu torch.nn.modules.module.Module.zero_grad
msgid "Parameters"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append:19
msgid "The other circuit to be appended"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append:21
msgid ""
"the qubit indices to which ``c`` is appended on. Defaults to None, which "
"means plain concatenation."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight keras.engine.base_layer.Layer.apply
#: keras.engine.base_layer.Layer.compute_mask
#: keras.engine.base_layer.Layer.compute_output_shape
#: keras.engine.base_layer.Layer.compute_output_signature
#: keras.engine.base_layer.Layer.count_params
#: keras.engine.base_layer.Layer.from_config
#: keras.engine.base_layer.Layer.get_config
#: keras.engine.base_layer.Layer.get_input_at
#: keras.engine.base_layer.Layer.get_input_mask_at
#: keras.engine.base_layer.Layer.get_input_shape_at
#: keras.engine.base_layer.Layer.get_losses_for
#: keras.engine.base_layer.Layer.get_output_at
#: keras.engine.base_layer.Layer.get_output_mask_at
#: keras.engine.base_layer.Layer.get_output_shape_at
#: keras.engine.base_layer.Layer.get_updates_for
#: keras.engine.base_layer.Layer.get_weights
#: keras.engine.training.Model.evaluate keras.engine.training.Model.fit
#: keras.engine.training.Model.from_config
#: keras.engine.training.Model.get_config keras.engine.training.Model.get_layer
#: keras.engine.training.Model.get_weights
#: keras.engine.training.Model.load_weights
#: keras.engine.training.Model.make_predict_function
#: keras.engine.training.Model.make_test_function
#: keras.engine.training.Model.make_train_function
#: keras.engine.training.Model.predict
#: keras.engine.training.Model.predict_on_batch
#: keras.engine.training.Model.predict_step
#: keras.engine.training.Model.save_spec
#: keras.engine.training.Model.test_on_batch
#: keras.engine.training.Model.test_step keras.engine.training.Model.to_json
#: keras.engine.training.Model.to_yaml
#: keras.engine.training.Model.train_on_batch
#: keras.engine.training.Model.train_step
#: keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule.from_config
#: of tensorcircuit.abstractcircuit.AbstractCircuit.append
#: tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement
#: tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_count
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_summary
#: tensorcircuit.abstractcircuit.AbstractCircuit.get_positional_logical_mapping
#: tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping
#: tensorcircuit.abstractcircuit.AbstractCircuit.inverse
#: tensorcircuit.abstractcircuit.AbstractCircuit.prepend
#: tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_cirq
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_openqasm
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qiskit
#: tensorcircuit.abstractcircuit.AbstractCircuit.vis_tex
#: tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.van.MADE.call
#: tensorcircuit.applications.van.MADE.compute_dtype
#: tensorcircuit.applications.van.MADE.input
#: tensorcircuit.applications.van.MADE.input_mask
#: tensorcircuit.applications.van.MADE.input_shape
#: tensorcircuit.applications.van.MADE.input_spec
#: tensorcircuit.applications.van.MADE.losses
#: tensorcircuit.applications.van.MADE.non_trainable_variables
#: tensorcircuit.applications.van.MADE.non_trainable_weights
#: tensorcircuit.applications.van.MADE.output
#: tensorcircuit.applications.van.MADE.output_mask
#: tensorcircuit.applications.van.MADE.output_shape
#: tensorcircuit.applications.van.MADE.run_eagerly
#: tensorcircuit.applications.van.MADE.state_updates
#: tensorcircuit.applications.van.MADE.submodules
#: tensorcircuit.applications.van.MADE.trainable_variables
#: tensorcircuit.applications.van.MADE.trainable_weights
#: tensorcircuit.applications.van.MADE.variables
#: tensorcircuit.applications.van.MADE.weights
#: tensorcircuit.applications.van.MaskedConv2D.call
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype
#: tensorcircuit.applications.van.MaskedConv2D.input
#: tensorcircuit.applications.van.MaskedConv2D.input_mask
#: tensorcircuit.applications.van.MaskedConv2D.input_shape
#: tensorcircuit.applications.van.MaskedConv2D.input_spec
#: tensorcircuit.applications.van.MaskedConv2D.losses
#: tensorcircuit.applications.van.MaskedConv2D.metrics
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_variables
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_weights
#: tensorcircuit.applications.van.MaskedConv2D.output
#: tensorcircuit.applications.van.MaskedConv2D.output_mask
#: tensorcircuit.applications.van.MaskedConv2D.output_shape
#: tensorcircuit.applications.van.MaskedConv2D.submodules
#: tensorcircuit.applications.van.MaskedConv2D.trainable_variables
#: tensorcircuit.applications.van.MaskedConv2D.trainable_weights
#: tensorcircuit.applications.van.MaskedConv2D.variables
#: tensorcircuit.applications.van.MaskedConv2D.weights
#: tensorcircuit.applications.van.MaskedLinear.call
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype
#: tensorcircuit.applications.van.MaskedLinear.input
#: tensorcircuit.applications.van.MaskedLinear.input_mask
#: tensorcircuit.applications.van.MaskedLinear.input_shape
#: tensorcircuit.applications.van.MaskedLinear.input_spec
#: tensorcircuit.applications.van.MaskedLinear.losses
#: tensorcircuit.applications.van.MaskedLinear.metrics
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_variables
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_weights
#: tensorcircuit.applications.van.MaskedLinear.output
#: tensorcircuit.applications.van.MaskedLinear.output_mask
#: tensorcircuit.applications.van.MaskedLinear.output_shape
#: tensorcircuit.applications.van.MaskedLinear.submodules
#: tensorcircuit.applications.van.MaskedLinear.trainable_variables
#: tensorcircuit.applications.van.MaskedLinear.trainable_weights
#: tensorcircuit.applications.van.MaskedLinear.variables
#: tensorcircuit.applications.van.MaskedLinear.weights
#: tensorcircuit.applications.van.NMF.call
#: tensorcircuit.applications.van.NMF.compute_dtype
#: tensorcircuit.applications.van.NMF.input
#: tensorcircuit.applications.van.NMF.input_mask
#: tensorcircuit.applications.van.NMF.input_shape
#: tensorcircuit.applications.van.NMF.input_spec
#: tensorcircuit.applications.van.NMF.losses
#: tensorcircuit.applications.van.NMF.non_trainable_variables
#: tensorcircuit.applications.van.NMF.non_trainable_weights
#: tensorcircuit.applications.van.NMF.output
#: tensorcircuit.applications.van.NMF.output_mask
#: tensorcircuit.applications.van.NMF.output_shape
#: tensorcircuit.applications.van.NMF.run_eagerly
#: tensorcircuit.applications.van.NMF.state_updates
#: tensorcircuit.applications.van.NMF.submodules
#: tensorcircuit.applications.van.NMF.trainable_variables
#: tensorcircuit.applications.van.NMF.trainable_weights
#: tensorcircuit.applications.van.NMF.variables
#: tensorcircuit.applications.van.NMF.weights
#: tensorcircuit.applications.van.PixelCNN.call
#: tensorcircuit.applications.van.PixelCNN.compute_dtype
#: tensorcircuit.applications.van.PixelCNN.input
#: tensorcircuit.applications.van.PixelCNN.input_mask
#: tensorcircuit.applications.van.PixelCNN.input_shape
#: tensorcircuit.applications.van.PixelCNN.input_spec
#: tensorcircuit.applications.van.PixelCNN.losses
#: tensorcircuit.applications.van.PixelCNN.non_trainable_variables
#: tensorcircuit.applications.van.PixelCNN.non_trainable_weights
#: tensorcircuit.applications.van.PixelCNN.output
#: tensorcircuit.applications.van.PixelCNN.output_mask
#: tensorcircuit.applications.van.PixelCNN.output_shape
#: tensorcircuit.applications.van.PixelCNN.run_eagerly
#: tensorcircuit.applications.van.PixelCNN.state_updates
#: tensorcircuit.applications.van.PixelCNN.submodules
#: tensorcircuit.applications.van.PixelCNN.trainable_variables
#: tensorcircuit.applications.van.PixelCNN.trainable_weights
#: tensorcircuit.applications.van.PixelCNN.variables
#: tensorcircuit.applications.van.PixelCNN.weights
#: tensorcircuit.applications.van.ResidualBlock.call
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype
#: tensorcircuit.applications.van.ResidualBlock.input
#: tensorcircuit.applications.van.ResidualBlock.input_mask
#: tensorcircuit.applications.van.ResidualBlock.input_shape
#: tensorcircuit.applications.van.ResidualBlock.input_spec
#: tensorcircuit.applications.van.ResidualBlock.losses
#: tensorcircuit.applications.van.ResidualBlock.metrics
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_variables
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_weights
#: tensorcircuit.applications.van.ResidualBlock.output
#: tensorcircuit.applications.van.ResidualBlock.output_mask
#: tensorcircuit.applications.van.ResidualBlock.output_shape
#: tensorcircuit.applications.van.ResidualBlock.submodules
#: tensorcircuit.applications.van.ResidualBlock.trainable_variables
#: tensorcircuit.applications.van.ResidualBlock.trainable_weights
#: tensorcircuit.applications.van.ResidualBlock.variables
#: tensorcircuit.applications.van.ResidualBlock.weights
#: tensorcircuit.applications.vqes.Linear.call
#: tensorcircuit.applications.vqes.Linear.compute_dtype
#: tensorcircuit.applications.vqes.Linear.input
#: tensorcircuit.applications.vqes.Linear.input_mask
#: tensorcircuit.applications.vqes.Linear.input_shape
#: tensorcircuit.applications.vqes.Linear.input_spec
#: tensorcircuit.applications.vqes.Linear.losses
#: tensorcircuit.applications.vqes.Linear.metrics
#: tensorcircuit.applications.vqes.Linear.non_trainable_variables
#: tensorcircuit.applications.vqes.Linear.non_trainable_weights
#: tensorcircuit.applications.vqes.Linear.output
#: tensorcircuit.applications.vqes.Linear.output_mask
#: tensorcircuit.applications.vqes.Linear.output_shape
#: tensorcircuit.applications.vqes.Linear.submodules
#: tensorcircuit.applications.vqes.Linear.trainable_variables
#: tensorcircuit.applications.vqes.Linear.trainable_weights
#: tensorcircuit.applications.vqes.Linear.variables
#: tensorcircuit.applications.vqes.Linear.weights
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.abs
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.arange
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.atanh
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.device
#: tensorcircuit.backends.jax_backend.JaxBackend.device_move
#: tensorcircuit.backends.jax_backend.JaxBackend.dtype
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.left_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.mod
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.searchsorted
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.std
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.jax_backend._svd_jax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.numpy_backend._sum_numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.pytorch_backend._conj_torch
#: tensorcircuit.backends.pytorch_backend._qr_torch
#: tensorcircuit.backends.pytorch_backend._rq_torch
#: tensorcircuit.backends.pytorch_backend._sum_torch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.backends.tensorflow_backend._matmul_tf
#: tensorcircuit.backends.tensorflow_backend._qr_tf
#: tensorcircuit.backends.tensorflow_backend._rq_tf
#: tensorcircuit.basecircuit.BaseCircuit.amplitude
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement
#: tensorcircuit.basecircuit.BaseCircuit.copy
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before
#: tensorcircuit.basecircuit.BaseCircuit.get_quvector
#: tensorcircuit.basecircuit.BaseCircuit.measure_jit
#: tensorcircuit.basecircuit.BaseCircuit.perfect_sampling
#: tensorcircuit.basecircuit.BaseCircuit.probability
#: tensorcircuit.basecircuit.BaseCircuit.sample
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps
#: tensorcircuit.basecircuit.BaseCircuit.to_qir
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.choi_to_kraus tensorcircuit.channels.choi_to_super
#: tensorcircuit.channels.composedkraus
#: tensorcircuit.channels.depolarizingchannel tensorcircuit.channels.evol_kraus
#: tensorcircuit.channels.evol_superop
#: tensorcircuit.channels.generaldepolarizingchannel
#: tensorcircuit.channels.is_hermitian_matrix
#: tensorcircuit.channels.kraus_to_choi tensorcircuit.channels.kraus_to_super
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.krausgate_to_krausmatrix
#: tensorcircuit.channels.krausmatrix_to_krausgate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel tensorcircuit.channels.reshuffle
#: tensorcircuit.channels.super_to_choi tensorcircuit.channels.super_to_kraus
#: tensorcircuit.channels.thermalrelaxationchannel
#: tensorcircuit.circuit.Circuit.depolarizing_reference
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.get_quoperator
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.matrix
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_contractor
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.get_dm_as_quoperator
#: tensorcircuit.densitymatrix.DMCircuit.to_circuit
#: tensorcircuit.densitymatrix.DMCircuit.wavefunction
#: tensorcircuit.experimental.hamiltonian_evol
#: tensorcircuit.experimental.parameter_shift_grad
#: tensorcircuit.experimental.parameter_shift_grad_v2
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity
#: tensorcircuit.gates.get_u_parameter tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.multicontrol_gate
#: tensorcircuit.gates.num_to_tensor tensorcircuit.gates.phase_gate
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.rxx_gate tensorcircuit.gates.ry_gate
#: tensorcircuit.gates.ryy_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.gates.rzz_gate tensorcircuit.gates.u_gate
#: tensorcircuit.interfaces.numpy.numpy_interface
#: tensorcircuit.interfaces.scipy.scipy_optimize_interface
#: tensorcircuit.interfaces.tensorflow.tensorflow_interface
#: tensorcircuit.interfaces.tensortrans.args_to_tensor
#: tensorcircuit.interfaces.tensortrans.general_args_to_numpy
#: tensorcircuit.interfaces.tensortrans.numpy_args_to_backend
#: tensorcircuit.interfaces.tensortrans.which_backend
#: tensorcircuit.interfaces.torch.torch_interface
#: tensorcircuit.keras.QuantumLayer.compute_dtype
#: tensorcircuit.keras.QuantumLayer.input
#: tensorcircuit.keras.QuantumLayer.input_mask
#: tensorcircuit.keras.QuantumLayer.input_shape
#: tensorcircuit.keras.QuantumLayer.input_spec
#: tensorcircuit.keras.QuantumLayer.losses
#: tensorcircuit.keras.QuantumLayer.metrics
#: tensorcircuit.keras.QuantumLayer.non_trainable_variables
#: tensorcircuit.keras.QuantumLayer.non_trainable_weights
#: tensorcircuit.keras.QuantumLayer.output
#: tensorcircuit.keras.QuantumLayer.output_mask
#: tensorcircuit.keras.QuantumLayer.output_shape
#: tensorcircuit.keras.QuantumLayer.submodules
#: tensorcircuit.keras.QuantumLayer.trainable_variables
#: tensorcircuit.keras.QuantumLayer.trainable_weights
#: tensorcircuit.keras.QuantumLayer.variables
#: tensorcircuit.keras.QuantumLayer.weights tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_bond_dimensions
#: tensorcircuit.mpscircuit.MPSCircuit.get_center_position
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.get_quvector
#: tensorcircuit.mpscircuit.MPSCircuit.get_tensors
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.noisemodel.apply_qir_with_noise
#: tensorcircuit.noisemodel.circuit_with_noise
#: tensorcircuit.noisemodel.expectation_noisfy
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy
#: tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2COO_tf
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.projector
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.adjoint
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.copy tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.projector
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.correlation_from_samples
#: tensorcircuit.quantum.count_d2s tensorcircuit.quantum.count_s2d
#: tensorcircuit.quantum.count_tuple2dict
#: tensorcircuit.quantum.count_vector2dict tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.sample2all tensorcircuit.quantum.sample2count
#: tensorcircuit.quantum.sample_bin2int tensorcircuit.quantum.sample_int2bin
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.trace_product
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.results.counts.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix
#: tensorcircuit.results.readout_mitigation.ReadoutMit.global_miti_readout_circ
#: tensorcircuit.results.readout_mitigation.ReadoutMit.local_miti_readout_circ
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability
#: tensorcircuit.results.readout_mitigation.ReadoutMit.newrange
#: tensorcircuit.results.readout_mitigation.ReadoutMit.ubs
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.qft
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_local_measurements
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.translation.eqasm2tc tensorcircuit.translation.perm_matrix
#: tensorcircuit.translation.qir2cirq tensorcircuit.translation.qir2json
#: tensorcircuit.translation.qir2qiskit tensorcircuit.translation.qiskit2tc
#: tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure
#: tensorcircuit.utils.append tensorcircuit.utils.arg_alias
#: tensorcircuit.utils.benchmark tensorcircuit.utils.is_m1mac
#: tensorcircuit.utils.return_partial tensorcircuit.vis.gate_name_trans
#: tensorcircuit.vis.qir2tex tensorcircuit.vis.render_pdf
#: tensorflow.python.module.module.Module.with_name_scope
#: tensornetwork.backends.abstract_backend.AbstractBackend.deserialize_tensor
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigs
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos
#: tensornetwork.backends.abstract_backend.AbstractBackend.exp
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres
#: tensornetwork.backends.abstract_backend.AbstractBackend.log
#: tensornetwork.backends.abstract_backend.AbstractBackend.pivot
#: tensornetwork.backends.abstract_backend.AbstractBackend.power
#: tensornetwork.backends.abstract_backend.AbstractBackend.serialize_tensor
#: tensornetwork.backends.jax.jax_backend.JaxBackend.addition
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_left_multiplication
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_right_multiplication
#: tensornetwork.backends.jax.jax_backend.JaxBackend.conj
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagflat
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal
#: tensornetwork.backends.jax.jax_backend.JaxBackend.divide
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigh
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eps
#: tensornetwork.backends.jax.jax_backend.JaxBackend.exp
#: tensornetwork.backends.jax.jax_backend.JaxBackend.inv
#: tensornetwork.backends.jax.jax_backend.JaxBackend.item
#: tensornetwork.backends.jax.jax_backend.JaxBackend.log
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul
#: tensornetwork.backends.jax.jax_backend.JaxBackend.multiply
#: tensornetwork.backends.jax.jax_backend.JaxBackend.random_uniform
#: tensornetwork.backends.jax.jax_backend.JaxBackend.reshape
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tensor
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tuple
#: tensornetwork.backends.jax.jax_backend.JaxBackend.subtraction
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace
#: tensornetwork.backends.jax.jax_backend.JaxBackend.transpose
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.addition
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_left_multiplication
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_right_multiplication
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.conj
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.deserialize_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagflat
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.divide
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.exp
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.item
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.log
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.multiply
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.random_uniform
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.reshape
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.serialize_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tensor
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tuple
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.subtraction
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.transpose
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.abs
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.addition
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_left_multiplication
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_right_multiplication
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagflat
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.divide
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.item
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.multiply
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.random_uniform
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.reshape
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tensor
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tuple
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.subtraction
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.transpose
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.addition
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_left_multiplication
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_right_multiplication
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.conj
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagflat
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.divide
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.exp
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.item
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.log
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.multiply
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.random_uniform
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.reshape
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tensor
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tuple
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.subtraction
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.transpose
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.position
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.check_canonical
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs
#: tensornetwork.network_components.AbstractNode.get_dimension
#: tensornetwork.network_components.AbstractNode.reorder_axes
#: tensornetwork.network_components.AbstractNode.reorder_edges
#: tensornetwork.network_components.Node.from_serial_dict
#: torch.nn.modules.module.Module.apply torch.nn.modules.module.Module.bfloat16
#: torch.nn.modules.module.Module.cpu torch.nn.modules.module.Module.cuda
#: torch.nn.modules.module.Module.double torch.nn.modules.module.Module.eval
#: torch.nn.modules.module.Module.float
#: torch.nn.modules.module.Module.get_buffer
#: torch.nn.modules.module.Module.get_extra_state
#: torch.nn.modules.module.Module.get_parameter
#: torch.nn.modules.module.Module.get_submodule
#: torch.nn.modules.module.Module.half
#: torch.nn.modules.module.Module.load_state_dict
#: torch.nn.modules.module.Module.register_backward_hook
#: torch.nn.modules.module.Module.register_forward_hook
#: torch.nn.modules.module.Module.register_forward_pre_hook
#: torch.nn.modules.module.Module.register_full_backward_hook
#: torch.nn.modules.module.Module.requires_grad_
#: torch.nn.modules.module.Module.state_dict torch.nn.modules.module.Module.to
#: torch.nn.modules.module.Module.to_empty torch.nn.modules.module.Module.train
#: torch.nn.modules.module.Module.type torch.nn.modules.module.Module.xpu
msgid "Returns"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append:24
#: tensorcircuit.abstractcircuit.AbstractCircuit.prepend:5
msgid "The composed circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append
#: tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement
#: tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_count
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_summary
#: tensorcircuit.abstractcircuit.AbstractCircuit.get_positional_logical_mapping
#: tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping
#: tensorcircuit.abstractcircuit.AbstractCircuit.inverse
#: tensorcircuit.abstractcircuit.AbstractCircuit.prepend
#: tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_json
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_openqasm
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.vis_tex
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.abs
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.arange
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.atanh
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.device
#: tensorcircuit.backends.jax_backend.JaxBackend.device_move
#: tensorcircuit.backends.jax_backend.JaxBackend.dtype
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.left_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.mod
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.searchsorted
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.std
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.jax_backend._svd_jax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.numpy_backend._sum_numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.pytorch_backend._qr_torch
#: tensorcircuit.backends.pytorch_backend._rq_torch
#: tensorcircuit.backends.pytorch_backend._sum_torch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.backends.tensorflow_backend._matmul_tf
#: tensorcircuit.backends.tensorflow_backend._qr_tf
#: tensorcircuit.backends.tensorflow_backend._rq_tf
#: tensorcircuit.basecircuit.BaseCircuit.amplitude
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before
#: tensorcircuit.basecircuit.BaseCircuit.get_quvector
#: tensorcircuit.basecircuit.BaseCircuit.measure_jit
#: tensorcircuit.basecircuit.BaseCircuit.perfect_sampling
#: tensorcircuit.basecircuit.BaseCircuit.probability
#: tensorcircuit.basecircuit.BaseCircuit.readouterror_bs
#: tensorcircuit.basecircuit.BaseCircuit.sample
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps
#: tensorcircuit.basecircuit.BaseCircuit.to_qir
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.choi_to_kraus tensorcircuit.channels.choi_to_super
#: tensorcircuit.channels.composedkraus
#: tensorcircuit.channels.depolarizingchannel tensorcircuit.channels.evol_kraus
#: tensorcircuit.channels.evol_superop
#: tensorcircuit.channels.generaldepolarizingchannel
#: tensorcircuit.channels.is_hermitian_matrix
#: tensorcircuit.channels.kraus_to_choi tensorcircuit.channels.kraus_to_super
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.krausgate_to_krausmatrix
#: tensorcircuit.channels.krausmatrix_to_krausgate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel tensorcircuit.channels.reshuffle
#: tensorcircuit.channels.super_to_choi tensorcircuit.channels.super_to_kraus
#: tensorcircuit.channels.thermalrelaxationchannel
#: tensorcircuit.circuit.Circuit.depolarizing_reference
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.get_quoperator
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.matrix
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.runtime_backend
#: tensorcircuit.cons.runtime_contractor tensorcircuit.cons.runtime_dtype
#: tensorcircuit.cons.set_contractor tensorcircuit.cons.set_dtype
#: tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_contractor
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.get_dm_as_quoperator
#: tensorcircuit.densitymatrix.DMCircuit.to_circuit
#: tensorcircuit.densitymatrix.DMCircuit.wavefunction
#: tensorcircuit.experimental.hamiltonian_evol
#: tensorcircuit.experimental.parameter_shift_grad
#: tensorcircuit.experimental.parameter_shift_grad_v2
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity
#: tensorcircuit.gates.get_u_parameter tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.multicontrol_gate
#: tensorcircuit.gates.num_to_tensor tensorcircuit.gates.phase_gate
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.rxx_gate tensorcircuit.gates.ry_gate
#: tensorcircuit.gates.ryy_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.gates.rzz_gate tensorcircuit.gates.u_gate
#: tensorcircuit.interfaces.numpy.numpy_interface
#: tensorcircuit.interfaces.scipy.scipy_optimize_interface
#: tensorcircuit.interfaces.tensorflow.tensorflow_interface
#: tensorcircuit.interfaces.tensortrans.args_to_tensor
#: tensorcircuit.interfaces.tensortrans.general_args_to_numpy
#: tensorcircuit.interfaces.tensortrans.numpy_args_to_backend
#: tensorcircuit.interfaces.tensortrans.which_backend
#: tensorcircuit.interfaces.torch.torch_interface tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_bond_dimensions
#: tensorcircuit.mpscircuit.MPSCircuit.get_center_position
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.get_quvector
#: tensorcircuit.mpscircuit.MPSCircuit.get_tensors
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.noisemodel.apply_qir_with_noise
#: tensorcircuit.noisemodel.circuit_with_noise
#: tensorcircuit.noisemodel.expectation_noisfy
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy
#: tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2COO_tf
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.projector
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.adjoint
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.copy tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.projector
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.correlation_from_samples
#: tensorcircuit.quantum.count_d2s tensorcircuit.quantum.count_s2d
#: tensorcircuit.quantum.count_tuple2dict
#: tensorcircuit.quantum.count_vector2dict tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.sample2all tensorcircuit.quantum.sample2count
#: tensorcircuit.quantum.sample_bin2int tensorcircuit.quantum.sample_int2bin
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.trace_product
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.results.counts.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation
#: tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix
#: tensorcircuit.results.readout_mitigation.ReadoutMit.global_miti_readout_circ
#: tensorcircuit.results.readout_mitigation.ReadoutMit.local_miti_readout_circ
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability
#: tensorcircuit.results.readout_mitigation.ReadoutMit.newrange
#: tensorcircuit.results.readout_mitigation.ReadoutMit.ubs
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.qft
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_local_measurements
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.translation.eqasm2tc tensorcircuit.translation.perm_matrix
#: tensorcircuit.translation.qir2cirq tensorcircuit.translation.qir2json
#: tensorcircuit.translation.qir2qiskit tensorcircuit.translation.qiskit2tc
#: tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure
#: tensorcircuit.utils.append tensorcircuit.utils.arg_alias
#: tensorcircuit.utils.benchmark tensorcircuit.utils.is_m1mac
#: tensorcircuit.utils.return_partial tensorcircuit.vis.gate_name_trans
#: tensorcircuit.vis.qir2tex tensorcircuit.vis.render_pdf
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigs
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_left_multiplication
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_right_multiplication
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagflat
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigh
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eps
#: tensornetwork.backends.jax.jax_backend.JaxBackend.inv
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul
#: tensornetwork.backends.jax.jax_backend.JaxBackend.random_uniform
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_left_multiplication
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_right_multiplication
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagflat
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.random_uniform
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.abs
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_left_multiplication
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_right_multiplication
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagflat
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.random_uniform
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_left_multiplication
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_right_multiplication
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagflat
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.random_uniform
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.position
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs
#: torch.nn.modules.module.Module.apply torch.nn.modules.module.Module.bfloat16
#: torch.nn.modules.module.Module.cpu torch.nn.modules.module.Module.cuda
#: torch.nn.modules.module.Module.double torch.nn.modules.module.Module.eval
#: torch.nn.modules.module.Module.float
#: torch.nn.modules.module.Module.get_buffer
#: torch.nn.modules.module.Module.get_extra_state
#: torch.nn.modules.module.Module.get_parameter
#: torch.nn.modules.module.Module.get_submodule
#: torch.nn.modules.module.Module.half
#: torch.nn.modules.module.Module.load_state_dict
#: torch.nn.modules.module.Module.register_backward_hook
#: torch.nn.modules.module.Module.register_forward_hook
#: torch.nn.modules.module.Module.register_forward_pre_hook
#: torch.nn.modules.module.Module.register_full_backward_hook
#: torch.nn.modules.module.Module.requires_grad_
#: torch.nn.modules.module.Module.state_dict torch.nn.modules.module.Module.to
#: torch.nn.modules.module.Module.to_empty torch.nn.modules.module.Module.train
#: torch.nn.modules.module.Module.type torch.nn.modules.module.Module.xpu
msgid "Return type"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append_from_qir:1
msgid ""
"Apply the ciurict in form of quantum intermediate representation after "
"the current cirucit."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append_from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement
#: tensorcircuit.abstractcircuit.AbstractCircuit.draw
#: tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qir
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit
#: tensorcircuit.abstractcircuit.AbstractCircuit.gate_count
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qir
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend._qr_torch
#: tensorcircuit.backends.pytorch_backend._rq_torch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend._qr_tf
#: tensorcircuit.backends.tensorflow_backend._rq_tf
#: tensorcircuit.basecircuit.BaseCircuit.amplitude
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement
#: tensorcircuit.basecircuit.BaseCircuit.readouterror_bs
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps
#: tensorcircuit.basecircuit.BaseCircuit.to_qir
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.generaldepolarizingchannel
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.channels.thermalrelaxationchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.interfaces.numpy.numpy_interface
#: tensorcircuit.interfaces.scipy.scipy_optimize_interface
#: tensorcircuit.interfaces.tensorflow.tensorflow_interface
#: tensorcircuit.interfaces.tensortrans.args_to_tensor
#: tensorcircuit.interfaces.torch.torch_interface tensorcircuit.keras.load_func
#: tensorcircuit.keras.save_func
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.count_d2s tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.renyi_free_energy tensorcircuit.quantum.spin_by_basis
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.torchnn.QuantumNet.__init__ tensorcircuit.translation.qir2cirq
#: tensorcircuit.translation.qir2qiskit tensorcircuit.translation.qiskit2tc
#: tensorcircuit.utils.append tensorcircuit.utils.return_partial
#: tensorcircuit.vis.gate_name_trans tensorcircuit.vis.qir2tex
#: tensorcircuit.vis.render_pdf
msgid "Example"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.append_from_qir:18
msgid "The quantum intermediate representation."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate:1
#: tensorcircuit.basecircuit.BaseCircuit.apply_general_gate:1
msgid ""
"An implementation of this method should also append gate directionary to "
"self._qir"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.barrier_instruction:1
msgid "add a barrier instruction flag, no effect on numerical simulation"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.barrier_instruction:3
msgid "the corresponding qubits"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement:1
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement:1
msgid ""
"Measurement on z basis at ``index`` qubit based on quantum amplitude (not"
" post-selection). The highlight is that this method can return the "
"measured result as a int Tensor and thus maintained a jittable pipeline."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement:16
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement:16
msgid ""
"In terms of ``DMCircuit``, this method returns nothing and the density "
"matrix after this method is kept in mixed state without knowing the "
"measuremet resuslts"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement:22
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement:22
msgid "the qubit for the z-basis measurement"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.cond_measurement:24
#: tensorcircuit.basecircuit.BaseCircuit.cond_measurement:24
msgid "0 or 1 for z measurement on up and down freedom"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.select_gate:1
msgid "Apply ``which``-th gate from ``kraus`` list, i.e. apply kraus[which]"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.select_gate:3
msgid "Tensor of shape [] and dtype int"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.select_gate:5
msgid "A list of gate in the form of ``tc.gate`` or Tensor"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.select_gate:7
msgid "the qubit lines the gate applied on"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.draw:1
msgid ""
"Visualise the circuit. This method recevies the keywords as same as "
"qiskit.circuit.QuantumCircuit.draw. More details can be found here: "
"https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.draw.html."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:1
msgid ""
"Shortcut for Pauli string expectation. x, y, z list are for X, Y, Z "
"positions"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:26
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:5
msgid "sites to apply X gate, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:28
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:7
msgid "sites to apply Y gate, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:30
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:9
msgid "sites to apply Z gate, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:32
msgid "whether to cache and reuse the wavefunction, defaults to True"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:34
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:46
#: tensorcircuit.circuit.Circuit.expectation:32
#: tensorcircuit.densitymatrix.DMCircuit.expectation:8
#: tensorcircuit.noisemodel.expectation_noisfy:5
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:11
msgid "Noise Configuration, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:36
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:48
#: tensorcircuit.circuit.Circuit.expectation:34
#: tensorcircuit.noisemodel.expectation_noisfy:7
msgid ""
"repetition time for Monte Carlo sampling for noisfy calculation, defaults"
" to 1000"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:38
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:50
#: tensorcircuit.circuit.Circuit.expectation:36
#: tensorcircuit.densitymatrix.DMCircuit.expectation:10
#: tensorcircuit.noisemodel.expectation_noisfy:9
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:17
msgid ""
"external randomness given by tensor uniformly from [0, 1], defaults to "
"None, used for noisfy circuit sampling"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.expectation_ps:41
msgid "Expectation value"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json:1
msgid "load json str as a Circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json:3
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json:8
#: tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file:7
#: tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping:9
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack:5
#: tensorcircuit.backends.jax_backend.JaxBackend.arange:9
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack:5
#: tensorcircuit.backends.jax_backend.JaxBackend.mean:9
#: tensorcircuit.backends.jax_backend.JaxBackend.std:3
#: tensorcircuit.backends.jax_backend.JaxBackend.std:12
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack:3
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack:5
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before:6
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before:7
#: tensorcircuit.channels.is_hermitian_matrix:9
#: tensorcircuit.cons.runtime_contractor:3
#: tensorcircuit.cons.set_function_contractor:3
#: tensorcircuit.experimental.hamiltonian_evol:4
#: tensorcircuit.experimental.hamiltonian_evol:6
#: tensorcircuit.experimental.hamiltonian_evol:8 tensorcircuit.gates.u_gate:16
#: tensorcircuit.quantum.count_vector2dict:9
#: tensorcircuit.quantum.sample2count:3 tensorcircuit.quantum.sample2count:5
#: tensorcircuit.quantum.sample2count:9
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:6
#: tensorcircuit.translation.eqasm2tc:3 tensorcircuit.translation.eqasm2tc:9
#: tensorcircuit.translation.qir2json:3 tensorcircuit.translation.qir2json:8
#: tensorcircuit.utils.arg_alias:3 tensorcircuit.utils.arg_alias:5
#: tensorcircuit.utils.benchmark:3 tensorcircuit.utils.benchmark:9
msgid "_description_"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json:5
msgid "Extra circuit parameters in the format of ``__init__``, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file:1
msgid "load json file and convert it to a circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file:3
msgid "filename"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_json_file:5
#: tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping:7
#: tensorcircuit.experimental.hamiltonian_evol:10
#: tensorcircuit.translation.eqasm2tc:5
msgid "_description_, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qir:1
msgid "Restore the circuit from the quantum intermediate representation."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qir:21
#: tensorcircuit.translation.qir2cirq:13
#: tensorcircuit.translation.qir2qiskit:14
msgid "The quantum intermediate representation of a circuit."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qir:23
msgid "Extra circuit parameters."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qir:25
msgid "The circuit have same gates in the qir."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:1
msgid "Import Qiskit QuantumCircuit object as a ``tc.Circuit`` object."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:12
msgid "Qiskit Circuit object"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:14
msgid "The number of qubits for the circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:16
msgid "possible input wavefunction for ``tc.Circuit``, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:18
#: tensorcircuit.translation.qiskit2tc:20
msgid "kwargs given in Circuit.__init__ construction function, default to None."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:20
#: tensorcircuit.translation.qiskit2tc:22
msgid ""
"(variational) parameters for the circuit. Could be either a sequence or "
"dictionary depending on the type of parameters in the Qiskit circuit. For"
" ``ParameterVectorElement`` use sequence. For ``Parameter`` use "
"dictionary"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.from_qiskit:24
msgid "The same circuit but as tensorcircuit object"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.gate_count:1
msgid "count the gate number of the circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.gate_count:14
msgid "gate name list to be counted, defaults to None (counting all gates)"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.gate_count:16
msgid "the total number of all gates or gates in the ``gate_list``"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.gate_summary:1
msgid "return the summary dictionary on gate type - gate count pair"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.gate_summary:3
msgid "the gate count dict by gate type"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.get_positional_logical_mapping:1
msgid ""
"Get positional logical mapping dict based on measure instruction. This "
"function is useful when we only measure part of the qubits in the "
"circuit, to process the count result from partial measurement, we must be"
" aware of the mapping, i.e. for each position in the count bitstring, "
"what is the corresponding qubits (logical) defined on the circuit"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.get_positional_logical_mapping:7
msgid "``positional_logical_mapping``"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping:1
msgid ""
"generate a new circuit with the qubit mapping given by "
"``logical_physical_mapping``"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping:3
msgid "how to map logical qubits to the physical qubits on the new circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.initial_mapping:5
msgid ""
"number of qubit of the new circuit, can be different from the original "
"one, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.inverse:1
msgid "inverse the circuit, return a new inversed circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.inverse
msgid "EXAMPLE"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.inverse:10
msgid "keywords dict for initialization the new circuit, defaults to None"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.inverse:12
msgid "the inversed circuit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.measure_instruction:1
msgid "add a measurement instruction flag, no effect on numerical simulation"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.measure_instruction:3
#: tensorcircuit.abstractcircuit.AbstractCircuit.reset_instruction:3
msgid "the corresponding qubit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.prepend:1
msgid "prepend circuit ``c`` before"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.prepend:3
msgid "The other circuit to be prepended"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.reset_instruction:1
msgid "add a reset instruction flag, no effect on numerical simulation"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate:1
msgid "standardize the gate name to tc common gate sets"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate:3
msgid "non-standard gate name"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.standardize_gate:5
msgid "the standard gate name"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.vis_tex:1
msgid "Generate latex string based on quantikz latex package"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.vis_tex:3
msgid "Latex string that can be directly compiled via, e.g. latexit"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_cirq:1
msgid "Translate ``tc.Circuit`` to a cirq circuit object."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_cirq:3
#: tensorcircuit.abstractcircuit.AbstractCircuit.to_qiskit:3
msgid "whether also export measurement and reset instructions"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_cirq:5
msgid "A cirq circuit of this circuit."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_json:1
msgid "circuit dumps to json"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_json:3
msgid "file str to dump the json to, defaults to None, return the json str"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_json:5
#: tensorcircuit.translation.qir2json:5
msgid ""
"If False, keep all info for each gate, defaults to be False. If True, "
"suitable for IO since less information is required"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_json:8
msgid "None if dumps to file otherwise the json str"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_openqasm:1
msgid ""
"transform circuit to openqasm via qiskit circuit, see "
"https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.qasm.html"
" for usage on possible options for ``kws``"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_openqasm:5
msgid "circuit representation in openqasm format"
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_qir:1
#: tensorcircuit.basecircuit.BaseCircuit.to_qir:1
msgid "Return the quantum intermediate representation of the circuit."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_qir:32
#: tensorcircuit.basecircuit.BaseCircuit.to_qir:32
msgid "The quantum intermediate representation of the circuit."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_qiskit:1
msgid "Translate ``tc.Circuit`` to a qiskit QuantumCircuit object."
msgstr ""

#: of tensorcircuit.abstractcircuit.AbstractCircuit.to_qiskit:5
msgid "A qiskit object of this circuit."
msgstr ""

#: ../../source/api/applications.rst:2
msgid "tensorcircuit.applications"
msgstr ""

#: ../../source/api/applications/dqas.rst:2
msgid "tensorcircuit.applications.dqas"
msgstr ""

#: of tensorcircuit.applications.dqas:1
msgid "Modules for DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:1
msgid "DQAS framework entrypoint"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:3
msgid ""
"function with input of data instance, circuit parameters theta and "
"structural paramter k, return tuple of objective value and gradient with "
"respect to theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:5
msgid "data generator as dataset"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:6
msgid "list of operations as primitive operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:7
msgid "the default layer number of the circuit ansatz"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:8
msgid ""
"shape of circuit parameter pool, in general p_stp*l, where l is the max "
"number of circuit parameters for op in the operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:10
msgid "the same as p in the most times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:11
msgid "batch size of one epoch"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:12
msgid "prethermal update times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:13
msgid "training epochs"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:14
msgid "parallel thread number, 0 to disable multiprocessing model by default"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:15
msgid "set verbose log to print"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:16
msgid "function to output verbose information"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:17
msgid "function return intermiediate result for final history list"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:18
msgid "cutoff probability to avoid peak distribution"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:19
msgid ""
"function accepting list of objective values and return the baseline value"
" used in the next round"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:21
msgid "return noise with the same shape as circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:22
msgid "initial values for circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:23
msgid "initial values for probabilistic model parameters"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:24
msgid "optimizer for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:25
msgid "optimizer for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:26
msgid "optimizer for circuit parameters in prethermal stage"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:27
msgid "fixed structural parameters for prethermal training"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:28
msgid "regularization function for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:29
msgid "regularization function for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:1
msgid ""
"The probabilistic model based DQAS, can use extensively for DQAS case for"
" ``NMF`` probabilistic model."
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:3
msgid "vag func, return loss and nabla lnp"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:4
msgid "keras model"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:5
msgid "sample func of logic with keras model input"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:6
msgid "input data pipeline generator"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:7
msgid "operation pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:8
msgid "depth for DQAS"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:12
msgid "parallel kernels"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:23
msgid "final loss function in terms of average of sub loss for each circuit"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:24
msgid "derivative function for ``loss_func``"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:1
msgid ""
"Call in customized functions and grab variables within DQAS framework "
"function by var name str."
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:3
msgid "The DQAS framework function"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:5
msgid "Variables within the DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.get_weights:1
msgid ""
"This function works only when nnp has the same shape as stp, i.e. one "
"parameter for each op."
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_kernel:1
msgid "The kernel for multiprocess to run parallel in DQAS function/"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:1
msgid ""
"parallel variational parameter training and search to avoid local minimum"
" not limited to qaoa setup as the function name indicates, as long as you"
" provided suitable `vag_func`"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:6
msgid "data input generator for vag_func"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:7
msgid "vag_kernel"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:10
msgid "number of tries"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:11
msgid "for optimization problem the input is in general fixed so batch is often 1"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:12
msgid "number of parallel jobs"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:13
msgid "mean value of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:14
msgid "std deviation of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.verbose_output:1
msgid "Doesn't support prob model DQAS search."
msgstr ""

#: ../../source/api/applications/graphdata.rst:2
msgid "tensorcircuit.applications.graphdata"
msgstr ""

#: of tensorcircuit.applications.graphdata:1
msgid "Modules for graph instance data and more"
msgstr ""

#: of tensorcircuit.applications.graphdata.dict2graph:1
msgid "```python d = nx.to_dict_of_dicts(g) ```"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:1
msgid "1D PBC chain with n sites."
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:3
msgid "The number of nodes"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:5
msgid "The resulted graph g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduce_edges:3
msgid "all graphs with m edge out from g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:1
msgid ""
"Generate a reduced graph with given ratio of edges compared to the "
"original graph g."
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:3
msgid "The base graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:5
msgid "number of edges kept, default half of the edges"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:6
msgid "The resulted reduced graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:1
msgid "Split the graph in exactly ``split`` piece evenly."
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:3
msgid "The mother graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:5
msgid "The number of the graph we want to divide into, defaults to 2"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:7
msgid "List of graph instance of size ``split``"
msgstr ""

#: ../../source/api/applications/layers.rst:2
msgid "tensorcircuit.applications.layers"
msgstr ""

#: of tensorcircuit.applications.layers:1
msgid "Module for functions adding layers of circuits"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "Hlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyswaplayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "cnotlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "swaplayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_any_gate_layer:1
msgid "$$e^{-i\\theta_i \\sigma}$$"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer:1
msgid ""
"The following function should be used to generate layers with special "
"case. As its soundness depends on the nature of the task or problem, it "
"doesn't always make sense."
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_gate_layer:1
#: tensorcircuit.applications.layers.generate_cirq_gate_layer:1
#: tensorcircuit.applications.layers.generate_gate_layer:1
msgid "$$e^{-i\\theta \\sigma}$$"
msgstr ""

#: ../../source/api/applications/utils.rst:2
msgid "tensorcircuit.applications.utils"
msgstr ""

#: of tensorcircuit.applications.utils:1
msgid ""
"A collection of useful function snippets that irrelevant with the main "
"modules or await for further refactor"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:1
msgid "color cirq circuit SVG for given gates, a small tool to hack the cirq SVG"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:5
msgid "integer coordinate which gate is colored"
msgstr ""

#: of tensorcircuit.applications.utils.repr2array:1
msgid "transform repr form of an array to real numpy array"
msgstr ""

#: ../../source/api/applications/vags.rst:2
msgid "tensorcircuit.applications.vags"
msgstr ""

#: of tensorcircuit.applications.vags:1
msgid "DQAS application kernels as vag functions"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:1
msgid "1D array for full wavefunction, the basis is in lexcical order"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:2
#: tensorcircuit.applications.vags.energy:5
msgid "nx.Graph"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:3
msgid "transformation functions before averaged"
msgstr ""

#: of tensorcircuit.applications.vags.cvar:1
msgid "as f3"
msgstr ""

#: of tensorcircuit.applications.vags.energy:1
msgid "maxcut energy for n qubit wavefunction i-th basis"
msgstr ""

#: of tensorcircuit.applications.vags.energy:3
msgid "ranged from 0 to 2**n-1"
msgstr ""

#: of tensorcircuit.applications.vags.energy:4
#: tensorcircuit.applications.vags.unitary_design:4
#: tensorcircuit.quantum.correlation_from_samples:8
#: tensorcircuit.quantum.count_s2d:6 tensorcircuit.quantum.count_tuple2dict:5
#: tensorcircuit.quantum.count_vector2dict:5 tensorcircuit.quantum.sample2all:5
#: tensorcircuit.quantum.sample_bin2int:5
#: tensorcircuit.quantum.sample_int2bin:5
msgid "number of qubits"
msgstr ""

#: of tensorcircuit.applications.vags.entanglement_entropy:1
msgid ""
"deprecated as non tf and non flexible, use the combination of "
"``reduced_density_matrix`` and ``entropy`` instead."
msgstr ""

#: of tensorcircuit.applications.vags.entropy:1
#: tensorcircuit.applications.vags.reduced_density_matrix:1
msgid "deprecated, current version in tc.quantum"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:1
msgid ""
"value and gradient, currently only tensorflow backend is supported jax "
"and numpy seems to be slow in circuit simulation anyhow. *deprecated*"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:8
msgid "if lbd=0, take energy as objective"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:9
msgid "if as default 0, overlap will not compute in the process"
msgstr ""

#: of tensorcircuit.applications.vags.gapfilling:1
msgid "Fill single qubit gates according to placeholder on circuit"
msgstr ""

#: of tensorcircuit.applications.vags.heisenberg_measurements:1
msgid "Hamiltonian measurements for Heisenberg model on graph lattice g"
msgstr ""

#: of tensorcircuit.applications.vags.q:1
msgid "short cut for ``cirq.LineQubit(i)``"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_block_vag:1
#: tensorcircuit.applications.vags.qaoa_block_vag_energy:1
msgid "QAOA block encoding kernel, support 2 params in one op"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_train:1
msgid ""
"training QAOA with only optimizing circuit parameters, can be well "
"replaced with more general function `DQAS_search`"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:1
msgid "multi parameter for one layer"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:7
#: tensorcircuit.applications.vags.quantum_qaoa_vag:7
msgid "kw arguments for measurements_func"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:8
msgid "loss function, gradient of nnp"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_qaoa_vag:1
msgid ""
"tensorflow quantum backend compare to qaoa_vag which is tensorcircuit "
"backend"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:1
msgid "Hamiltonian for tfim on lattice defined by graph g"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:8
msgid "cirq.PauliSum as operators for tfq expectation layer"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:1
msgid ""
"generate random wavefunction from approximately Haar measure, reference:"
"  https://doi.org/10.1063/1.4983266"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:5
msgid "repetition of the blocks"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:1
msgid "random Haar measure approximation"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:3
msgid "cirq.Circuit, empty circuit"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:4
msgid "# of qubit"
msgstr ""

#: ../../source/api/applications/van.rst:2
msgid "tensorcircuit.applications.van"
msgstr ""

#: of tensorcircuit.applications.van:1
msgid ""
"One-hot variational autoregressive models for multiple categorical "
"choices beyond binary"
msgstr ""

#: of tensorcircuit.applications.van.MADE:1
#: tensorcircuit.applications.van.NMF:1
#: tensorcircuit.applications.van.PixelCNN:1
msgid "Bases: :py:class:`keras.engine.training.Model`"
msgstr ""

#: of tensorcircuit.applications.van.MADE.activity_regularizer:1
#: tensorcircuit.applications.van.MaskedConv2D.activity_regularizer:1
#: tensorcircuit.applications.van.MaskedLinear.activity_regularizer:1
#: tensorcircuit.applications.van.NMF.activity_regularizer:1
#: tensorcircuit.applications.van.PixelCNN.activity_regularizer:1
#: tensorcircuit.applications.van.ResidualBlock.activity_regularizer:1
#: tensorcircuit.applications.vqes.Linear.activity_regularizer:1
#: tensorcircuit.keras.QuantumLayer.activity_regularizer:1
msgid "Optional regularizer function for the output of this layer."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:1 of
msgid "Add loss tensor(s), potentially dependent on layer inputs."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:3 of
msgid ""
"Some losses (for instance, activity regularization losses) may be "
"dependent on the inputs passed when calling a layer. Hence, when reusing "
"the same layer on different inputs `a` and `b`, some entries in "
"`layer.losses` may be dependent on `a` and some on `b`. This method "
"automatically keeps track of dependencies."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:9 of
msgid ""
"This method can be used inside a subclassed layer or model's `call` "
"function, in which case `losses` should be a Tensor or list of Tensors."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:12
#: keras.engine.base_layer.Layer.add_loss:26
#: keras.engine.base_layer.Layer.add_loss:42
#: keras.engine.training.Model.compile:3 keras.engine.training.Model.save:28 of
#: tensorcircuit.applications.van.MaskedConv2D.metrics:3
#: tensorcircuit.applications.van.MaskedLinear.metrics:3
#: tensorcircuit.applications.van.ResidualBlock.metrics:3
#: tensorcircuit.applications.vqes.Linear.metrics:3
#: tensorcircuit.keras.QuantumLayer.metrics:3
msgid "Example:"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:14 of
msgid "```python class MyLayer(tf.keras.layers.Layer):"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:17
#: keras.engine.base_layer.Layer.add_metric:14 of
msgid "def call(self, inputs):"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:17 of
msgid "self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:19
#: keras.engine.base_layer.Layer.add_metric:16
#: keras.engine.training.Model.compile:10 of
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:21
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:35
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:21
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:35
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:19
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:33
msgid "```"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:21 of
msgid ""
"This method can also be called directly on a Functional Model during "
"construction. In this case, any loss Tensors passed to this Model must be"
" symbolic and be able to be traced back to the model's `Input`s. These "
"losses become part of the model's topology and are tracked in "
"`get_config`."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:28 of
msgid ""
"```python inputs = tf.keras.Input(shape=(10,)) x = "
"tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) "
"model = tf.keras.Model(inputs, outputs) # Activity regularization. "
"model.add_loss(tf.abs(tf.reduce_mean(x))) ```"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:37 of
msgid ""
"If this is not the case for your loss (if, for example, your loss "
"references a `Variable` of one of the model's layers), you can wrap your "
"loss in a zero-argument lambda. These losses are not tracked as part of "
"the model's topology since they can't be serialized."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:44 of
msgid ""
"```python inputs = tf.keras.Input(shape=(10,)) d = "
"tf.keras.layers.Dense(10) x = d(inputs) outputs = "
"tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # "
"Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) "
"```"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:54 of
msgid ""
"Loss tensor, or list/tuple of tensors. Rather than tensors, losses may "
"also be zero-argument callables which create a loss tensor."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:56 of
msgid ""
"Additional keyword arguments for backward compatibility. Accepted values:"
"   inputs - Deprecated, will be automatically inferred."
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:56 of
msgid "Additional keyword arguments for backward compatibility. Accepted values:"
msgstr ""

#: keras.engine.base_layer.Layer.add_loss:58 of
msgid "inputs - Deprecated, will be automatically inferred."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:1 of
msgid "Adds metric tensor to the layer."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:3 of
msgid ""
"This method can be used inside the `call()` method of a subclassed layer "
"or model."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:6 of
msgid "```python class MyMetricLayer(tf.keras.layers.Layer):"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:10 of
msgid "def __init__(self):"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:9 of
msgid ""
"super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = "
"tf.keras.metrics.Mean(name='metric_1')"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:13 of
msgid ""
"self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs),"
" name='metric_2') return inputs"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:18 of
msgid ""
"This method can also be called directly on a Functional Model during "
"construction. In this case, any tensor passed to this Model must be "
"symbolic and be able to be traced back to the model's `Input`s. These "
"metrics become part of the model's topology and are tracked when you save"
" the model via `save()`."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:24 of
msgid ""
"```python inputs = tf.keras.Input(shape=(10,)) x = "
"tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) "
"model = tf.keras.Model(inputs, outputs) "
"model.add_metric(math_ops.reduce_sum(x), name='metric_1') ```"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:32 of
msgid ""
"Note: Calling `add_metric()` with the result of a metric object on a "
"Functional Model, as shown in the example below, is not supported. This "
"is because we cannot trace the metric result tensor back to the model's "
"inputs."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:36 of
msgid ""
"```python inputs = tf.keras.Input(shape=(10,)) x = "
"tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) "
"model = tf.keras.Model(inputs, outputs) "
"model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ```"
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:44 of
msgid "Metric tensor."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:45 of
msgid "String metric name."
msgstr ""

#: keras.engine.base_layer.Layer.add_metric:46 of
msgid ""
"Additional keyword arguments for backward compatibility. Accepted values:"
" `aggregation` - When the `value` tensor provided is not the result of "
"calling a `keras.Metric` instance, it will be aggregated by default using"
" a `keras.Metric.Mean`."
msgstr ""

#: keras.engine.base_layer.Layer.add_update:1 of
msgid "Add update op(s), potentially dependent on layer inputs."
msgstr ""

#: keras.engine.base_layer.Layer.add_update:3 of
msgid ""
"Weight updates (for instance, the updates of the moving mean and variance"
" in a BatchNormalization layer) may be dependent on the inputs passed "
"when calling a layer. Hence, when reusing the same layer on different "
"inputs `a` and `b`, some entries in `layer.updates` may be dependent on "
"`a` and some on `b`. This method automatically keeps track of "
"dependencies."
msgstr ""

#: keras.engine.base_layer.Layer.add_update:10 of
msgid ""
"This call is ignored when eager execution is enabled (in that case, "
"variable updates are run on the fly and thus do not need to be tracked "
"for later execution)."
msgstr ""

#: keras.engine.base_layer.Layer.add_update:14 of
msgid ""
"Update op, or list/tuple of update ops, or zero-arg callable that returns"
" an update op. A zero-arg callable should be passed in order to disable "
"running the updates by setting `trainable=False` on this Layer, when "
"executing in Eager mode."
msgstr ""

#: keras.engine.base_layer.Layer.add_update:18 of
msgid "Deprecated, will be automatically inferred."
msgstr ""

#: keras.engine.base_layer.Layer.add_variable:1 of
msgid "Deprecated, do NOT use! Alias for `add_weight`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:1 of
msgid "Adds a new variable to the layer."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:3 of
msgid "Variable name."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:4 of
msgid "Variable shape. Defaults to scalar if unspecified."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:5 of
msgid "The type of the variable. Defaults to `self.dtype`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:6 of
msgid "Initializer instance (callable)."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:7 of
msgid "Regularizer instance (callable)."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:8 of
msgid ""
"Boolean, whether the variable should be part of the layer's "
"\"trainable_variables\" (e.g. variables, biases) or "
"\"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that"
" `trainable` cannot be `True` if `synchronization` is set to `ON_READ`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:13 of
msgid "Constraint instance (callable)."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:14 of
msgid "Whether to use `ResourceVariable`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:15 of
msgid ""
"Indicates when a distributed a variable will be aggregated. Accepted "
"values are constants defined in the class `tf.VariableSynchronization`. "
"By default the synchronization is set to `AUTO` and the current "
"`DistributionStrategy` chooses when to synchronize. If `synchronization` "
"is set to `ON_READ`, `trainable` must not be set to `True`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:21 of
msgid ""
"Indicates how a distributed variable will be aggregated. Accepted values "
"are constants defined in the class `tf.VariableAggregation`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:24 of
msgid ""
"Additional keyword arguments. Accepted values are `getter`, "
"`collections`, `experimental_autocast` and `caching_device`."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:27 of
msgid "The variable created."
msgstr ""

#: keras.engine.base_layer.Layer.add_weight
#: keras.engine.base_layer.Layer.compute_output_signature
#: keras.engine.base_layer.Layer.count_params
#: keras.engine.base_layer.Layer.get_input_at
#: keras.engine.base_layer.Layer.get_input_shape_at
#: keras.engine.base_layer.Layer.get_output_at
#: keras.engine.base_layer.Layer.get_output_shape_at
#: keras.engine.base_layer.Layer.set_weights keras.engine.training.Model.build
#: keras.engine.training.Model.evaluate keras.engine.training.Model.fit
#: keras.engine.training.Model.load_weights keras.engine.training.Model.predict
#: keras.engine.training.Model.predict_on_batch
#: keras.engine.training.Model.save_weights keras.engine.training.Model.summary
#: keras.engine.training.Model.test_on_batch
#: keras.engine.training.Model.to_yaml
#: keras.engine.training.Model.train_on_batch of
#: tensorcircuit.applications.van.MADE.input
#: tensorcircuit.applications.van.MADE.input_mask
#: tensorcircuit.applications.van.MADE.input_shape
#: tensorcircuit.applications.van.MADE.output
#: tensorcircuit.applications.van.MADE.output_mask
#: tensorcircuit.applications.van.MADE.output_shape
#: tensorcircuit.applications.van.MaskedConv2D.input
#: tensorcircuit.applications.van.MaskedConv2D.input_mask
#: tensorcircuit.applications.van.MaskedConv2D.input_shape
#: tensorcircuit.applications.van.MaskedConv2D.output
#: tensorcircuit.applications.van.MaskedConv2D.output_mask
#: tensorcircuit.applications.van.MaskedConv2D.output_shape
#: tensorcircuit.applications.van.MaskedLinear.input
#: tensorcircuit.applications.van.MaskedLinear.input_mask
#: tensorcircuit.applications.van.MaskedLinear.input_shape
#: tensorcircuit.applications.van.MaskedLinear.output
#: tensorcircuit.applications.van.MaskedLinear.output_mask
#: tensorcircuit.applications.van.MaskedLinear.output_shape
#: tensorcircuit.applications.van.NMF.input
#: tensorcircuit.applications.van.NMF.input_mask
#: tensorcircuit.applications.van.NMF.input_shape
#: tensorcircuit.applications.van.NMF.output
#: tensorcircuit.applications.van.NMF.output_mask
#: tensorcircuit.applications.van.NMF.output_shape
#: tensorcircuit.applications.van.PixelCNN.input
#: tensorcircuit.applications.van.PixelCNN.input_mask
#: tensorcircuit.applications.van.PixelCNN.input_shape
#: tensorcircuit.applications.van.PixelCNN.output
#: tensorcircuit.applications.van.PixelCNN.output_mask
#: tensorcircuit.applications.van.PixelCNN.output_shape
#: tensorcircuit.applications.van.ResidualBlock.input
#: tensorcircuit.applications.van.ResidualBlock.input_mask
#: tensorcircuit.applications.van.ResidualBlock.input_shape
#: tensorcircuit.applications.van.ResidualBlock.output
#: tensorcircuit.applications.van.ResidualBlock.output_mask
#: tensorcircuit.applications.van.ResidualBlock.output_shape
#: tensorcircuit.applications.vqes.Linear.input
#: tensorcircuit.applications.vqes.Linear.input_mask
#: tensorcircuit.applications.vqes.Linear.input_shape
#: tensorcircuit.applications.vqes.Linear.output
#: tensorcircuit.applications.vqes.Linear.output_mask
#: tensorcircuit.applications.vqes.Linear.output_shape
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map
#: tensorcircuit.basecircuit.BaseCircuit.expectation_before
#: tensorcircuit.circuit.Circuit.expectation tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.gates.bmatrix tensorcircuit.keras.QuantumLayer.input
#: tensorcircuit.keras.QuantumLayer.input_mask
#: tensorcircuit.keras.QuantumLayer.input_shape
#: tensorcircuit.keras.QuantumLayer.output
#: tensorcircuit.keras.QuantumLayer.output_mask
#: tensorcircuit.keras.QuantumLayer.output_shape tensorcircuit.keras.load_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.check_spaces
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.position
#: tensornetwork.network_components.AbstractNode.add_axis_names
#: tensornetwork.network_components.AbstractNode.add_edge
#: tensornetwork.network_components.AbstractNode.get_dimension
#: tensornetwork.network_components.AbstractNode.reorder_axes
#: tensornetwork.network_components.AbstractNode.reorder_edges
#: tensornetwork.network_components.Node.__init__
#: torch.nn.modules.module.Module.get_buffer
#: torch.nn.modules.module.Module.get_parameter
#: torch.nn.modules.module.Module.get_submodule
msgid "Raises"
msgstr ""

#: keras.engine.base_layer.Layer.add_weight:29 of
msgid ""
"When giving unsupported dtype and no initializer or when     trainable "
"has been set to True with synchronization set as `ON_READ`."
msgstr ""

#: keras.engine.base_layer.Layer.apply:1
#: keras.engine.base_layer.Layer.get_losses_for:1
#: keras.engine.base_layer.Layer.get_updates_for:1 of
#: tensorcircuit.applications.van.MADE.state_updates:1
#: tensorcircuit.applications.van.NMF.state_updates:1
#: tensorcircuit.applications.van.PixelCNN.state_updates:1
msgid "Deprecated, do NOT use!"
msgstr ""

#: keras.engine.base_layer.Layer.apply:3 of
msgid "This is an alias of `self.__call__`."
msgstr ""

#: keras.engine.base_layer.Layer.apply:5 of
msgid "Input tensor(s)."
msgstr ""

#: keras.engine.base_layer.Layer.apply:6 of
msgid "additional positional arguments to be passed to `self.call`."
msgstr ""

#: keras.engine.base_layer.Layer.apply:7 of
msgid "additional keyword arguments to be passed to `self.call`."
msgstr ""

#: keras.engine.base_layer.Layer.apply:9 of
msgid "Output tensor(s)."
msgstr ""

#: keras.engine.training.Model.build:1 of
msgid "Builds the model based on input shapes received."
msgstr ""

#: keras.engine.training.Model.build:3 of
msgid ""
"This is to be used for subclassed models, which do not know at "
"instantiation time what their inputs look like."
msgstr ""

#: keras.engine.training.Model.build:6 of
msgid ""
"This method only exists for users who want to call `model.build()` in a "
"standalone way (as a substitute for calling the model on real data to "
"build it). It will never be called by the framework (and thus it will "
"never throw unexpected errors in an unrelated workflow)."
msgstr ""

#: keras.engine.training.Model.build:11 of
msgid ""
"Single tuple, `TensorShape` instance, or list/dict of shapes, where "
"shapes are tuples, integers, or `TensorShape` instances."
msgstr ""

#: keras.engine.training.Model.build:14 of
msgid ""
"1. In case of invalid user-provided data (not of type tuple,        list,"
" `TensorShape`, or dict).     2. If the model requires call arguments "
"that are agnostic        to the input shapes (positional or keyword arg "
"in call signature).     3. If not all layers were properly built.     4. "
"If float type inputs are not supported within the layers."
msgstr ""

#: keras.engine.training.Model.build:14 of
msgid ""
"In case of invalid user-provided data (not of type tuple,        list, "
"`TensorShape`, or dict).     2. If the model requires call arguments that"
" are agnostic        to the input shapes (positional or keyword arg in "
"call signature).     3. If not all layers were properly built.     4. If "
"float type inputs are not supported within the layers."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:1
#: tensorcircuit.applications.van.NMF.call:1
#: tensorcircuit.applications.van.PixelCNN.call:1
msgid "Calls the model on new inputs and returns the outputs as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:3
#: tensorcircuit.applications.van.NMF.call:3
#: tensorcircuit.applications.van.PixelCNN.call:3
msgid ""
"In this case `call()` just reapplies all ops in the graph to the new "
"inputs (e.g. build a new computational graph from the provided inputs)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:7
#: tensorcircuit.applications.van.NMF.call:7
#: tensorcircuit.applications.van.PixelCNN.call:7
msgid ""
"Note: This method should not be called directly. It is only meant to be "
"overridden when subclassing `tf.keras.Model`. To call a model on an "
"input, always use the `__call__()` method, i.e. `model(inputs)`, which "
"relies on the underlying `call()` method."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:12
#: tensorcircuit.applications.van.NMF.call:12
#: tensorcircuit.applications.van.PixelCNN.call:12
msgid "Input tensor, or dict/list/tuple of input tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:13
#: tensorcircuit.applications.van.NMF.call:13
#: tensorcircuit.applications.van.PixelCNN.call:13
msgid ""
"Boolean or boolean scalar tensor, indicating whether to run the `Network`"
" in training mode or inference mode."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:15
#: tensorcircuit.applications.van.NMF.call:15
#: tensorcircuit.applications.van.PixelCNN.call:15
msgid ""
"A mask or list of masks. A mask can be either a boolean tensor or None "
"(no mask). For more details, check the guide   "
"[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:15
#: tensorcircuit.applications.van.NMF.call:15
#: tensorcircuit.applications.van.PixelCNN.call:15
msgid ""
"A mask or list of masks. A mask can be either a boolean tensor or None "
"(no mask). For more details, check the guide"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:17
#: tensorcircuit.applications.van.NMF.call:17
#: tensorcircuit.applications.van.PixelCNN.call:17
msgid "[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:19
#: tensorcircuit.applications.van.NMF.call:19
#: tensorcircuit.applications.van.PixelCNN.call:19
msgid ""
"A tensor if there is a single output, or a list of tensors if there are "
"more than one outputs."
msgstr ""

#: keras.engine.training.Model.compile:1 of
msgid "Configures the model for training."
msgstr ""

#: keras.engine.training.Model.compile:5 of
msgid ""
"```python "
"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),"
msgstr ""

#: keras.engine.training.Model.compile:7 of
msgid ""
"loss=tf.keras.losses.BinaryCrossentropy(), "
"metrics=[tf.keras.metrics.BinaryAccuracy(),"
msgstr ""

#: keras.engine.training.Model.compile:9 of
msgid "tf.keras.metrics.FalseNegatives()])"
msgstr ""

#: keras.engine.training.Model.compile:12 of
msgid ""
"String (name of optimizer) or optimizer instance. See "
"`tf.keras.optimizers`."
msgstr ""

#: keras.engine.training.Model.compile:14 of
msgid ""
"Loss function. Maybe be a string (name of loss function), or a "
"`tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss function "
"is any callable with the signature `loss = fn(y_true, y_pred)`, where "
"`y_true` are the ground truth values, and `y_pred` are the model's "
"predictions. `y_true` should have shape `(batch_size, d0, .. dN)` (except"
" in the case of sparse loss functions such as sparse categorical "
"crossentropy which expects integer arrays of shape `(batch_size, d0, .. "
"dN-1)`). `y_pred` should have shape `(batch_size, d0, .. dN)`. The loss "
"function should return a float tensor. If a custom `Loss` instance is "
"used and reduction is set to `None`, return value has shape `(batch_size,"
" d0, .. dN-1)` i.e. per-sample or per-timestep loss values; otherwise, it"
" is a scalar. If the model has multiple outputs, you can use a different "
"loss on each output by passing a dictionary or a list of losses. The loss"
" value that will be minimized by the model will then be the sum of all "
"individual losses, unless `loss_weights` is specified."
msgstr ""

#: keras.engine.training.Model.compile:34 of
msgid ""
"List of metrics to be evaluated by the model during training and testing."
" Each of this can be a string (name of a built-in function), function or "
"a `tf.keras.metrics.Metric` instance. See `tf.keras.metrics`. Typically "
"you will use `metrics=['accuracy']`. A function is any callable with the "
"signature `result = fn(y_true, y_pred)`. To specify different metrics for"
" different outputs of a multi-output model, you could also pass a "
"dictionary, such as `metrics={'output_a': 'accuracy', 'output_b': "
"['accuracy', 'mse']}`. You can also pass a list to specify a metric or a "
"list of metrics for each output, such as `metrics=[['accuracy'], "
"['accuracy', 'mse']]` or `metrics=['accuracy', ['accuracy', 'mse']]`. "
"When you pass the strings 'accuracy' or 'acc', we convert this to one of "
"`tf.keras.metrics.BinaryAccuracy`, "
"`tf.keras.metrics.CategoricalAccuracy`, "
"`tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function "
"used and the model output shape. We do a similar conversion for the "
"strings 'crossentropy' and 'ce' as well."
msgstr ""

#: keras.engine.training.Model.compile:51 of
msgid ""
"Optional list or dictionary specifying scalar coefficients (Python "
"floats) to weight the loss contributions of different model outputs. The "
"loss value that will be minimized by the model will then be the *weighted"
" sum* of all individual losses, weighted by the `loss_weights` "
"coefficients.   If a list, it is expected to have a 1:1 mapping to the "
"model's     outputs. If a dict, it is expected to map output names "
"(strings)     to scalar coefficients."
msgstr ""

#: keras.engine.training.Model.compile:51 of
msgid ""
"Optional list or dictionary specifying scalar coefficients (Python "
"floats) to weight the loss contributions of different model outputs. The "
"loss value that will be minimized by the model will then be the *weighted"
" sum* of all individual losses, weighted by the `loss_weights` "
"coefficients."
msgstr ""

#: keras.engine.training.Model.compile:57 of
msgid "If a list, it is expected to have a 1:1 mapping to the model's"
msgstr ""

#: keras.engine.training.Model.compile:57 of
msgid ""
"outputs. If a dict, it is expected to map output names (strings) to "
"scalar coefficients."
msgstr ""

#: keras.engine.training.Model.compile:59 of
msgid ""
"List of metrics to be evaluated and weighted by `sample_weight` or "
"`class_weight` during training and testing."
msgstr ""

#: keras.engine.training.Model.compile:61 of
msgid ""
"Bool. Defaults to `False`. If `True`, this `Model`'s logic will not be "
"wrapped in a `tf.function`. Recommended to leave this as `None` unless "
"your `Model` cannot be run inside a `tf.function`. `run_eagerly=True` is "
"not supported when using "
"`tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.compile:66 of
msgid ""
"Int. Defaults to 1. The number of batches to run during each "
"`tf.function` call. Running multiple batches inside a single "
"`tf.function` call can greatly improve performance on TPUs or small "
"models with a large Python overhead. At most, one full epoch will be run "
"each execution. If a number larger than the size of the epoch is passed, "
"the execution will be truncated to the size of the epoch. Note that if "
"`steps_per_execution` is set to `N`, `Callback.on_batch_begin` and "
"`Callback.on_batch_end` methods will only be called every `N` batches "
"(i.e. before/after each `tf.function` execution)."
msgstr ""

#: keras.engine.training.Model.compile:77 of
msgid "Arguments supported for backwards compatibility only."
msgstr ""

#: of tensorcircuit.applications.van.MADE.compute_dtype:1
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype:1
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype:1
#: tensorcircuit.applications.van.NMF.compute_dtype:1
#: tensorcircuit.applications.van.PixelCNN.compute_dtype:1
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype:1
#: tensorcircuit.applications.vqes.Linear.compute_dtype:1
#: tensorcircuit.keras.QuantumLayer.compute_dtype:1
msgid "The dtype of the layer's computations."
msgstr ""

#: of tensorcircuit.applications.van.MADE.compute_dtype:3
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype:3
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype:3
#: tensorcircuit.applications.van.NMF.compute_dtype:3
#: tensorcircuit.applications.van.PixelCNN.compute_dtype:3
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype:3
#: tensorcircuit.applications.vqes.Linear.compute_dtype:3
#: tensorcircuit.keras.QuantumLayer.compute_dtype:3
msgid ""
"This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed "
"precision is used, this is the same as `Layer.dtype`, the dtype of the "
"weights."
msgstr ""

#: of tensorcircuit.applications.van.MADE.compute_dtype:7
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype:7
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype:7
#: tensorcircuit.applications.van.NMF.compute_dtype:7
#: tensorcircuit.applications.van.PixelCNN.compute_dtype:7
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype:7
#: tensorcircuit.applications.vqes.Linear.compute_dtype:7
#: tensorcircuit.keras.QuantumLayer.compute_dtype:7
msgid ""
"Layers automatically cast their inputs to the compute dtype, which causes"
" computations and the output to be in the compute dtype as well. This is "
"done by the base Layer class in `Layer.__call__`, so you do not have to "
"insert these casts if implementing your own layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.compute_dtype:12
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype:12
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype:12
#: tensorcircuit.applications.van.NMF.compute_dtype:12
#: tensorcircuit.applications.van.PixelCNN.compute_dtype:12
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype:12
#: tensorcircuit.applications.vqes.Linear.compute_dtype:12
#: tensorcircuit.keras.QuantumLayer.compute_dtype:12
msgid ""
"Layers often perform certain internal computations in higher precision "
"when `compute_dtype` is float16 or bfloat16 for numeric stability. The "
"output will still typically be float16 or bfloat16 in such cases."
msgstr ""

#: of tensorcircuit.applications.van.MADE.compute_dtype:16
#: tensorcircuit.applications.van.MaskedConv2D.compute_dtype:16
#: tensorcircuit.applications.van.MaskedLinear.compute_dtype:16
#: tensorcircuit.applications.van.NMF.compute_dtype:16
#: tensorcircuit.applications.van.PixelCNN.compute_dtype:16
#: tensorcircuit.applications.van.ResidualBlock.compute_dtype:16
#: tensorcircuit.applications.vqes.Linear.compute_dtype:16
#: tensorcircuit.keras.QuantumLayer.compute_dtype:16
msgid "The layer's compute dtype."
msgstr ""

#: keras.engine.base_layer.Layer.compute_mask:1 of
msgid "Computes an output mask tensor."
msgstr ""

#: keras.engine.base_layer.Layer.compute_mask:3
#: keras.engine.base_layer.Layer.compute_mask:4 of
msgid "Tensor or list of tensors."
msgstr ""

#: keras.engine.base_layer.Layer.compute_mask:6 of
msgid ""
"None or a tensor (or list of tensors,     one per output tensor of the "
"layer)."
msgstr ""

#: keras.engine.base_layer.Layer.compute_mask:8 of
msgid "None or a tensor (or list of tensors,"
msgstr ""

#: keras.engine.base_layer.Layer.compute_mask:9 of
msgid "one per output tensor of the layer)."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_shape:1 of
msgid "Computes the output shape of the layer."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_shape:3 of
msgid ""
"If the layer has not been built, this method will call `build` on the "
"layer. This assumes that the layer will later be used with inputs that "
"match the input shape provided here."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_shape:7 of
msgid ""
"Shape tuple (tuple of integers) or list of shape tuples (one per output "
"tensor of the layer). Shape tuples can include None for free dimensions, "
"instead of an integer."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_shape:12 of
msgid "An input shape tuple."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:1 of
msgid "Compute the output tensor signature of the layer based on the inputs."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:3 of
msgid ""
"Unlike a TensorShape object, a TensorSpec object contains both shape and "
"dtype information for a tensor. This method allows layers to provide "
"output dtype information if it is different from the input dtype. For any"
" layer that doesn't implement this function, the framework will fall back"
" to use `compute_output_shape`, and will assume that the output dtype "
"matches the input dtype."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:10 of
msgid ""
"Single TensorSpec or nested structure of TensorSpec objects, describing a"
" candidate input for the layer."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:13 of
msgid ""
"Single TensorSpec or nested structure of TensorSpec objects, describing"
"   how the layer would transform the provided input."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:16 of
msgid "Single TensorSpec or nested structure of TensorSpec objects, describing"
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:16 of
msgid "how the layer would transform the provided input."
msgstr ""

#: keras.engine.base_layer.Layer.compute_output_signature:18 of
msgid "If input_signature contains a non-TensorSpec object."
msgstr ""

#: keras.engine.base_layer.Layer.count_params:1 of
msgid "Count the total number of scalars composing the weights."
msgstr ""

#: keras.engine.base_layer.Layer.count_params:3 of
msgid "An integer count."
msgstr ""

#: keras.engine.base_layer.Layer.count_params:5 of
msgid ""
"if the layer isn't yet built     (in which case its weights aren't yet "
"defined)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.distribute_strategy:1
#: tensorcircuit.applications.van.NMF.distribute_strategy:1
#: tensorcircuit.applications.van.PixelCNN.distribute_strategy:1
msgid "The `tf.distribute.Strategy` this model was created under."
msgstr ""

#: of tensorcircuit.applications.van.MADE.dtype:1
#: tensorcircuit.applications.van.MaskedConv2D.dtype:1
#: tensorcircuit.applications.van.MaskedLinear.dtype:1
#: tensorcircuit.applications.van.NMF.dtype:1
#: tensorcircuit.applications.van.PixelCNN.dtype:1
#: tensorcircuit.applications.van.ResidualBlock.dtype:1
#: tensorcircuit.applications.vqes.Linear.dtype:1
#: tensorcircuit.keras.QuantumLayer.dtype:1
msgid "The dtype of the layer weights."
msgstr ""

#: of tensorcircuit.applications.van.MADE.dtype:3
#: tensorcircuit.applications.van.MaskedConv2D.dtype:3
#: tensorcircuit.applications.van.MaskedLinear.dtype:3
#: tensorcircuit.applications.van.NMF.dtype:3
#: tensorcircuit.applications.van.PixelCNN.dtype:3
#: tensorcircuit.applications.van.ResidualBlock.dtype:3
#: tensorcircuit.applications.vqes.Linear.dtype:3
#: tensorcircuit.keras.QuantumLayer.dtype:3
msgid ""
"This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed "
"precision is used, this is the same as `Layer.compute_dtype`, the dtype "
"of the layer's computations."
msgstr ""

#: of tensorcircuit.applications.van.MADE.dtype_policy:1
#: tensorcircuit.applications.van.MaskedConv2D.dtype_policy:1
#: tensorcircuit.applications.van.MaskedLinear.dtype_policy:1
#: tensorcircuit.applications.van.NMF.dtype_policy:1
#: tensorcircuit.applications.van.PixelCNN.dtype_policy:1
#: tensorcircuit.applications.van.ResidualBlock.dtype_policy:1
#: tensorcircuit.applications.vqes.Linear.dtype_policy:1
#: tensorcircuit.keras.QuantumLayer.dtype_policy:1
msgid "The dtype policy associated with this layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.dtype_policy:3
#: tensorcircuit.applications.van.MaskedConv2D.dtype_policy:3
#: tensorcircuit.applications.van.MaskedLinear.dtype_policy:3
#: tensorcircuit.applications.van.NMF.dtype_policy:3
#: tensorcircuit.applications.van.PixelCNN.dtype_policy:3
#: tensorcircuit.applications.van.ResidualBlock.dtype_policy:3
#: tensorcircuit.applications.vqes.Linear.dtype_policy:3
#: tensorcircuit.keras.QuantumLayer.dtype_policy:3
msgid "This is an instance of a `tf.keras.mixed_precision.Policy`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.dynamic:1
#: tensorcircuit.applications.van.MaskedConv2D.dynamic:1
#: tensorcircuit.applications.van.MaskedLinear.dynamic:1
#: tensorcircuit.applications.van.NMF.dynamic:1
#: tensorcircuit.applications.van.PixelCNN.dynamic:1
#: tensorcircuit.applications.van.ResidualBlock.dynamic:1
#: tensorcircuit.applications.vqes.Linear.dynamic:1
#: tensorcircuit.keras.QuantumLayer.dynamic:1
msgid "Whether the layer is dynamic (eager-only); set in the constructor."
msgstr ""

#: keras.engine.training.Model.evaluate:1 of
msgid "Returns the loss value & metrics values for the model in test mode."
msgstr ""

#: keras.engine.training.Model.evaluate:3 of
msgid "Computation is done in batches (see the `batch_size` arg.)"
msgstr ""

#: keras.engine.training.Model.evaluate:5 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays   (in case the model has multiple inputs). - A TensorFlow tensor, "
"or a list of tensors   (in case the model has multiple inputs). - A dict "
"mapping input names to the corresponding array/tensors,   if the model "
"has named inputs. - A `tf.data` dataset. Should return a tuple   of "
"either `(inputs, targets)` or   `(inputs, targets, sample_weights)`. - A "
"generator or `keras.utils.Sequence` returning `(inputs, targets)`   or "
"`(inputs, targets, sample_weights)`. A more detailed description of "
"unpacking behavior for iterator types (Dataset, generator, Sequence) is "
"given in the `Unpacking behavior for iterator-like inputs` section of "
"`Model.fit`."
msgstr ""

#: keras.engine.training.Model.evaluate:5 keras.engine.training.Model.fit:3
#: keras.engine.training.Model.train_on_batch:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays"
msgstr ""

#: keras.engine.training.Model.evaluate:7 keras.engine.training.Model.fit:5
#: keras.engine.training.Model.predict:13
#: keras.engine.training.Model.train_on_batch:5
#: keras.engine.training.Model.train_on_batch:7 of
msgid "(in case the model has multiple inputs)."
msgstr ""

#: keras.engine.training.Model.evaluate:8 keras.engine.training.Model.fit:6
#: keras.engine.training.Model.predict:14 of
msgid ""
"A TensorFlow tensor, or a list of tensors (in case the model has multiple"
" inputs)."
msgstr ""

#: keras.engine.training.Model.evaluate:10 keras.engine.training.Model.fit:8 of
msgid ""
"A dict mapping input names to the corresponding array/tensors, if the "
"model has named inputs."
msgstr ""

#: keras.engine.training.Model.evaluate:12 keras.engine.training.Model.fit:10
#: of
msgid ""
"A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` "
"or `(inputs, targets, sample_weights)`."
msgstr ""

#: keras.engine.training.Model.evaluate:15 keras.engine.training.Model.fit:13
#: of
msgid ""
"A generator or `keras.utils.Sequence` returning `(inputs, targets)` or "
"`(inputs, targets, sample_weights)`."
msgstr ""

#: keras.engine.training.Model.evaluate:17
#: keras.engine.training.Model.predict:18 of
msgid ""
"A more detailed description of unpacking behavior for iterator types "
"(Dataset, generator, Sequence) is given in the `Unpacking behavior for "
"iterator-like inputs` section of `Model.fit`."
msgstr ""

#: keras.engine.training.Model.evaluate:20 of
msgid ""
"Target data. Like the input data `x`, it could be either Numpy array(s) "
"or TensorFlow tensor(s). It should be consistent with `x` (you cannot "
"have Numpy inputs and tensor targets, or inversely). If `x` is a dataset,"
" generator or `keras.utils.Sequence` instance, `y` should not be "
"specified (since targets will be obtained from the iterator/dataset)."
msgstr ""

#: keras.engine.training.Model.evaluate:26 of
msgid ""
"Integer or `None`. Number of samples per batch of computation. If "
"unspecified, `batch_size` will default to 32. Do not specify the "
"`batch_size` if your data is in the form of a dataset, generators, or "
"`keras.utils.Sequence` instances (since they generate batches)."
msgstr ""

#: keras.engine.training.Model.evaluate:31 of
msgid "0 or 1. Verbosity mode. 0 = silent, 1 = progress bar."
msgstr ""

#: keras.engine.training.Model.evaluate:32 of
msgid ""
"Optional Numpy array of weights for the test samples, used for weighting "
"the loss function. You can either pass a flat (1D) Numpy array with the "
"same length as the input samples   (1:1 mapping between weights and "
"samples), or in the case of     temporal data, you can pass a 2D array "
"with shape `(samples,     sequence_length)`, to apply a different weight "
"to every timestep     of every sample. This argument is not supported "
"when `x` is a     dataset, instead pass sample weights as the third "
"element of `x`."
msgstr ""

#: keras.engine.training.Model.evaluate:32 of
msgid ""
"Optional Numpy array of weights for the test samples, used for weighting "
"the loss function. You can either pass a flat (1D) Numpy array with the "
"same length as the input samples"
msgstr ""

#: keras.engine.training.Model.evaluate:38 of
msgid "(1:1 mapping between weights and samples), or in the case of"
msgstr ""

#: keras.engine.training.Model.evaluate:36 of
msgid ""
"temporal data, you can pass a 2D array with shape `(samples, "
"sequence_length)`, to apply a different weight to every timestep of every"
" sample. This argument is not supported when `x` is a dataset, instead "
"pass sample weights as the third element of `x`."
msgstr ""

#: keras.engine.training.Model.evaluate:40 of
msgid ""
"Integer or `None`. Total number of steps (batches of samples) before "
"declaring the evaluation round finished. Ignored with the default value "
"of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' "
"will run until the dataset is exhausted. This argument is not supported "
"with array inputs."
msgstr ""

#: keras.engine.training.Model.evaluate:45 of
msgid ""
"List of `keras.callbacks.Callback` instances. List of callbacks to apply "
"during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks)."
msgstr ""

#: keras.engine.training.Model.evaluate:48 keras.engine.training.Model.fit:160
#: keras.engine.training.Model.predict:36 of
msgid ""
"Integer. Used for generator or `keras.utils.Sequence` input only. Maximum"
" size for the generator queue. If unspecified, `max_queue_size` will "
"default to 10."
msgstr ""

#: keras.engine.training.Model.evaluate:51 keras.engine.training.Model.fit:163
#: keras.engine.training.Model.predict:39 of
msgid ""
"Integer. Used for generator or `keras.utils.Sequence` input only. Maximum"
" number of processes to spin up when using process-based threading. If "
"unspecified, `workers` will default to 1."
msgstr ""

#: keras.engine.training.Model.evaluate:54 keras.engine.training.Model.fit:167
#: keras.engine.training.Model.predict:43 of
msgid ""
"Boolean. Used for generator or `keras.utils.Sequence` input only. If "
"`True`, use process-based threading. If unspecified, "
"`use_multiprocessing` will default to `False`. Note that because this "
"implementation relies on multiprocessing, you should not pass non-"
"picklable arguments to the generator as they can't be passed easily to "
"children processes."
msgstr ""

#: keras.engine.training.Model.evaluate:60
#: keras.engine.training.Model.test_on_batch:21
#: keras.engine.training.Model.train_on_batch:25 of
msgid ""
"If `True`, loss and metric results are returned as a dict, with each key "
"being the name of the metric. If `False`, they are returned as a list."
msgstr ""

#: keras.engine.training.Model.evaluate:63 of
msgid "Unused at this time."
msgstr ""

#: keras.engine.training.Model.evaluate:65 of
msgid ""
"See the discussion of `Unpacking behavior for iterator-like inputs` for "
"`Model.fit`."
msgstr ""

#: keras.engine.training.Model.evaluate:68 of
msgid ""
"`Model.evaluate` is not yet supported with "
"`tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.evaluate:71
#: keras.engine.training.Model.test_on_batch:25 of
msgid ""
"Scalar test loss (if the model has a single output and no metrics) or "
"list of scalars (if the model has multiple outputs and/or metrics). The "
"attribute `model.metrics_names` will give you the display labels for the "
"scalar outputs."
msgstr ""

#: keras.engine.training.Model.evaluate:76 of
msgid "If `model.evaluate` is wrapped in a `tf.function`."
msgstr ""

#: keras.engine.training.Model.evaluate_generator:1 of
msgid "Evaluates the model on a data generator."
msgstr ""

#: keras.engine.training.Model.evaluate_generator:4
#: keras.engine.training.Model.fit_generator:4
#: keras.engine.training.Model.predict_generator:4 of
msgid "DEPRECATED:"
msgstr ""

#: keras.engine.training.Model.evaluate_generator:4 of
msgid ""
"`Model.evaluate` now supports generators, so there is no longer any need "
"to use this endpoint."
msgstr ""

#: keras.engine.base_layer.Layer.finalize_state:1 of
msgid "Finalizes the layers state after updating layer weights."
msgstr ""

#: keras.engine.base_layer.Layer.finalize_state:3 of
msgid ""
"This function can be subclassed in a layer and will be called after "
"updating a layer weights. It can be overridden to finalize any additional"
" layer state after a weight update."
msgstr ""

#: keras.engine.training.Model.fit:1 of
msgid "Trains the model for a fixed number of epochs (iterations on a dataset)."
msgstr ""

#: keras.engine.training.Model.fit:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays   (in case the model has multiple inputs). - A TensorFlow tensor, "
"or a list of tensors   (in case the model has multiple inputs). - A dict "
"mapping input names to the corresponding array/tensors,   if the model "
"has named inputs. - A `tf.data` dataset. Should return a tuple   of "
"either `(inputs, targets)` or   `(inputs, targets, sample_weights)`. - A "
"generator or `keras.utils.Sequence` returning `(inputs, targets)`   or "
"`(inputs, targets, sample_weights)`. - A "
"`tf.keras.utils.experimental.DatasetCreator`, which wraps a   callable "
"that takes a single argument of type   `tf.distribute.InputContext`, and "
"returns a `tf.data.Dataset`.   `DatasetCreator` should be used when users"
" prefer to specify the   per-replica batching and sharding logic for the "
"`Dataset`.   See `tf.keras.utils.experimental.DatasetCreator` doc for "
"more   information. A more detailed description of unpacking behavior for"
" iterator types (Dataset, generator, Sequence) is given below. If using "
"`tf.distribute.experimental.ParameterServerStrategy`, only "
"`DatasetCreator` type is supported for `x`."
msgstr ""

#: keras.engine.training.Model.fit:15 of
msgid ""
"A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable "
"that takes a single argument of type `tf.distribute.InputContext`, and "
"returns a `tf.data.Dataset`. `DatasetCreator` should be used when users "
"prefer to specify the per-replica batching and sharding logic for the "
"`Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more "
"information."
msgstr ""

#: keras.engine.training.Model.fit:22 of
msgid ""
"A more detailed description of unpacking behavior for iterator types "
"(Dataset, generator, Sequence) is given below. If using "
"`tf.distribute.experimental.ParameterServerStrategy`, only "
"`DatasetCreator` type is supported for `x`."
msgstr ""

#: keras.engine.training.Model.fit:26 of
msgid ""
"Target data. Like the input data `x`, it could be either Numpy array(s) "
"or TensorFlow tensor(s). It should be consistent with `x` (you cannot "
"have Numpy inputs and tensor targets, or inversely). If `x` is a dataset,"
" generator, or `keras.utils.Sequence` instance, `y` should not be "
"specified (since targets will be obtained from `x`)."
msgstr ""

#: keras.engine.training.Model.fit:32 of
msgid ""
"Integer or `None`. Number of samples per gradient update. If unspecified,"
" `batch_size` will default to 32. Do not specify the `batch_size` if your"
" data is in the form of datasets, generators, or `keras.utils.Sequence` "
"instances (since they generate batches)."
msgstr ""

#: keras.engine.training.Model.fit:38 of
msgid ""
"Integer. Number of epochs to train the model. An epoch is an iteration "
"over the entire `x` and `y` data provided (unless the `steps_per_epoch` "
"flag is set to something other than None). Note that in conjunction with "
"`initial_epoch`, `epochs` is to be understood as \"final epoch\". The "
"model is not trained for a number of iterations given by `epochs`, but "
"merely until the epoch of index `epochs` is reached."
msgstr ""

#: keras.engine.training.Model.fit:48 of
msgid ""
"'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one"
" line per epoch. 'auto' defaults to 1 for most cases, but 2 when used "
"with `ParameterServerStrategy`. Note that the progress bar is not "
"particularly useful when logged to a file, so verbose=2 is recommended "
"when not running interactively (eg, in a production environment)."
msgstr ""

#: keras.engine.training.Model.fit:55 of
msgid ""
"List of `keras.callbacks.Callback` instances. List of callbacks to apply "
"during training. See `tf.keras.callbacks`. Note "
"`tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` "
"callbacks are created automatically and need not be passed into "
"`model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based "
"on `verbose` argument to `model.fit`. Callbacks with batch-level calls "
"are currently unsupported with "
"`tf.distribute.experimental.ParameterServerStrategy`, and users are "
"advised to implement epoch-level calls instead with an appropriate "
"`steps_per_epoch` value."
msgstr ""

#: keras.engine.training.Model.fit:66 of
msgid ""
"Float between 0 and 1.  Fraction of the training data to be used as "
"validation data.  The model will set apart this fraction of the training "
"data,  will not train on it, and will evaluate  the loss and any model "
"metrics  on this data at the end of each epoch.  The validation data is "
"selected from the last samples  in the `x` and `y` data provided, before "
"shuffling. This argument is  not supported when `x` is a dataset, "
"generator or `keras.utils.Sequence` instance.  `validation_split` is not "
"yet supported with  `tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.fit:74 of
msgid "Float between 0 and 1."
msgstr ""

#: keras.engine.training.Model.fit:68 of
msgid ""
"Fraction of the training data to be used as validation data. The model "
"will set apart this fraction of the training data, will not train on it, "
"and will evaluate the loss and any model metrics on this data at the end "
"of each epoch. The validation data is selected from the last samples in "
"the `x` and `y` data provided, before shuffling. This argument is not "
"supported when `x` is a dataset, generator or"
msgstr ""

#: keras.engine.training.Model.fit:77 of
msgid "`keras.utils.Sequence` instance."
msgstr ""

#: keras.engine.training.Model.fit:77 of
msgid ""
"`validation_split` is not yet supported with "
"`tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.fit:79 of
msgid ""
"Data on which to evaluate the loss and any model metrics at the end of "
"each epoch. The model will not be trained on this data. Thus, note the "
"fact that the validation loss of data provided using `validation_split` "
"or `validation_data` is not affected by regularization layers like noise "
"and dropout. `validation_data` will override `validation_split`. "
"`validation_data` could be:   - A tuple `(x_val, y_val)` of Numpy arrays "
"or tensors.   - A tuple `(x_val, y_val, val_sample_weights)` of NumPy "
"arrays.   - A `tf.data.Dataset`.   - A Python generator or "
"`keras.utils.Sequence` returning   `(inputs, targets)` or `(inputs, "
"targets, sample_weights)`. `validation_data` is not yet supported with "
"`tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.fit:79 of
msgid ""
"Data on which to evaluate the loss and any model metrics at the end of "
"each epoch. The model will not be trained on this data. Thus, note the "
"fact that the validation loss of data provided using `validation_split` "
"or `validation_data` is not affected by regularization layers like noise "
"and dropout. `validation_data` will override `validation_split`. "
"`validation_data` could be:"
msgstr ""

#: keras.engine.training.Model.fit:87 of
msgid "A tuple `(x_val, y_val)` of Numpy arrays or tensors."
msgstr ""

#: keras.engine.training.Model.fit:88 of
msgid "A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays."
msgstr ""

#: keras.engine.training.Model.fit:89 of
msgid "A `tf.data.Dataset`."
msgstr ""

#: keras.engine.training.Model.fit:90 of
msgid "A Python generator or `keras.utils.Sequence` returning"
msgstr ""

#: keras.engine.training.Model.fit:91 of
msgid "`(inputs, targets)` or `(inputs, targets, sample_weights)`."
msgstr ""

#: keras.engine.training.Model.fit:92 of
msgid ""
"`validation_data` is not yet supported with "
"`tf.distribute.experimental.ParameterServerStrategy`."
msgstr ""

#: keras.engine.training.Model.fit:94 of
msgid ""
"Boolean (whether to shuffle the training data before each epoch) or str "
"(for 'batch'). This argument is ignored when `x` is a generator or an "
"object of tf.data.Dataset. 'batch' is a special option for dealing with "
"the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no "
"effect when `steps_per_epoch` is not `None`."
msgstr ""

#: keras.engine.training.Model.fit:100 of
msgid ""
"Optional dictionary mapping class indices (integers) to a weight (float) "
"value, used for weighting the loss function (during training only). This "
"can be useful to tell the model to \"pay more attention\" to samples from"
" an under-represented class."
msgstr ""

#: keras.engine.training.Model.fit:106 of
msgid ""
"Optional Numpy array of weights for  the training samples, used for "
"weighting the loss function  (during training only). You can either pass "
"a flat (1D)  Numpy array with the same length as the input samples  (1:1 "
"mapping between weights and samples),  or in the case of temporal data,  "
"you can pass a 2D array with shape  `(samples, sequence_length)`,  to "
"apply a different weight to every timestep of every sample. This  "
"argument is not supported when `x` is a dataset, generator, or "
"`keras.utils.Sequence` instance, instead provide the sample_weights  as "
"the third element of `x`."
msgstr ""

#: keras.engine.training.Model.fit:115 of
msgid "Optional Numpy array of weights for"
msgstr ""

#: keras.engine.training.Model.fit:108 of
msgid ""
"the training samples, used for weighting the loss function (during "
"training only). You can either pass a flat (1D) Numpy array with the same"
" length as the input samples (1:1 mapping between weights and samples), "
"or in the case of temporal data, you can pass a 2D array with shape "
"`(samples, sequence_length)`, to apply a different weight to every "
"timestep of every sample. This argument is not supported when `x` is a "
"dataset, generator, or"
msgstr ""

#: keras.engine.training.Model.fit:117 of
msgid "`keras.utils.Sequence` instance, instead provide the sample_weights"
msgstr ""

#: keras.engine.training.Model.fit:118 of
msgid "as the third element of `x`."
msgstr ""

#: keras.engine.training.Model.fit:119 of
msgid ""
"Integer. Epoch at which to start training (useful for resuming a previous"
" training run)."
msgstr ""

#: keras.engine.training.Model.fit:122 of
msgid ""
"Integer or `None`. Total number of steps (batches of samples) before "
"declaring one epoch finished and starting the next epoch. When training "
"with input tensors such as TensorFlow data tensors, the default `None` is"
" equal to the number of samples in your dataset divided by the batch "
"size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and"
" 'steps_per_epoch' is None, the epoch will run until the input dataset is"
" exhausted. When passing an infinitely repeating dataset, you must "
"specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the "
"training will run indefinitely with an infinitely repeating dataset. This"
" argument is not supported with array inputs. When using "
"`tf.distribute.experimental.ParameterServerStrategy`:   * "
"`steps_per_epoch=None` is not supported."
msgstr ""

#: keras.engine.training.Model.fit:122 of
msgid ""
"Integer or `None`. Total number of steps (batches of samples) before "
"declaring one epoch finished and starting the next epoch. When training "
"with input tensors such as TensorFlow data tensors, the default `None` is"
" equal to the number of samples in your dataset divided by the batch "
"size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and"
" 'steps_per_epoch' is None, the epoch will run until the input dataset is"
" exhausted. When passing an infinitely repeating dataset, you must "
"specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the "
"training will run indefinitely with an infinitely repeating dataset. This"
" argument is not supported with array inputs. When using "
"`tf.distribute.experimental.ParameterServerStrategy`:"
msgstr ""

#: keras.engine.training.Model.fit:136 of
msgid "`steps_per_epoch=None` is not supported."
msgstr ""

#: keras.engine.training.Model.fit:137 of
msgid ""
"Only relevant if `validation_data` is provided and is a `tf.data` "
"dataset. Total number of steps (batches of samples) to draw before "
"stopping when performing validation at the end of every epoch. If "
"'validation_steps' is None, validation will run until the "
"`validation_data` dataset is exhausted. In the case of an infinitely "
"repeated dataset, it will run into an infinite loop. If "
"'validation_steps' is specified and only part of the dataset will be "
"consumed, the evaluation will start from the beginning of the dataset at "
"each epoch. This ensures that the same validation samples are used every "
"time."
msgstr ""

#: keras.engine.training.Model.fit:147 of
msgid ""
"Integer or `None`. Number of samples per validation batch. If "
"unspecified, will default to `batch_size`. Do not specify the "
"`validation_batch_size` if your data is in the form of datasets, "
"generators, or `keras.utils.Sequence` instances (since they generate "
"batches)."
msgstr ""

#: keras.engine.training.Model.fit:153 of
msgid ""
"Only relevant if validation data is provided. Integer or "
"`collections.abc.Container` instance (e.g. list, tuple, etc.). If an "
"integer, specifies how many training epochs to run before a new "
"validation run is performed, e.g. `validation_freq=2` runs validation "
"every 2 epochs. If a Container, specifies the epochs on which to run "
"validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end "
"of the 1st, 2nd, and 10th epochs."
msgstr ""

#: keras.engine.training.Model.fit:196 of
msgid "Unpacking behavior for iterator-like inputs:"
msgstr ""

#: keras.engine.training.Model.fit:175 of
msgid "A common pattern is to pass a tf.data.Dataset, generator, or"
msgstr ""

#: keras.engine.training.Model.fit:176 of
msgid ""
"tf.keras.utils.Sequence to the `x` argument of fit, which will in fact "
"yield not only features (x) but optionally targets (y) and sample "
"weights. Keras requires that the output of such iterator-likes be "
"unambiguous. The iterator should return a tuple of length 1, 2, or 3, "
"where the optional second and third elements will be used for y and "
"sample_weight respectively. Any other type provided will be wrapped in a "
"length one tuple, effectively treating everything as 'x'. When yielding "
"dicts, they should still adhere to the top-level tuple structure. e.g. "
"`({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate "
"features, targets, and weights from the keys of a single dict."
msgstr ""

#: keras.engine.training.Model.fit:186 of
msgid "A notable unsupported data type is the namedtuple. The reason is that"
msgstr ""

#: keras.engine.training.Model.fit:187 of
msgid ""
"it behaves like both an ordered datatype (tuple) and a mapping datatype "
"(dict). So given a namedtuple of the form:"
msgstr ""

#: keras.engine.training.Model.fit:189 of
msgid "`namedtuple(\"example_tuple\", [\"y\", \"x\"])`"
msgstr ""

#: keras.engine.training.Model.fit:190 of
msgid ""
"it is ambiguous whether to reverse the order of the elements when "
"interpreting the value. Even worse is a tuple of the form:"
msgstr ""

#: keras.engine.training.Model.fit:192 of
msgid "`namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`"
msgstr ""

#: keras.engine.training.Model.fit:193 of
msgid ""
"where it is unclear if the tuple was intended to be unpacked into x, y, "
"and sample_weight or passed through as a single element to `x`. As a "
"result the data processing code will simply raise a ValueError if it "
"encounters a namedtuple. (Along with instructions to remedy the issue.)"
msgstr ""

#: keras.engine.training.Model.fit:198 of
msgid ""
"A `History` object. Its `History.history` attribute is a record of "
"training loss values and metrics values at successive epochs, as well as "
"validation loss values and validation metrics values (if applicable)."
msgstr ""

#: keras.engine.training.Model.fit:203 of
msgid "1. If the model was never compiled or,"
msgstr ""

#: keras.engine.training.Model.fit:203 of
msgid "If the model was never compiled or,"
msgstr ""

#: keras.engine.training.Model.fit:205 of
msgid ""
"In case of mismatch between the provided input data     and what the "
"model expects or when the input data is empty."
msgstr ""

#: keras.engine.training.Model.fit_generator:1 of
msgid "Fits the model on data yielded batch-by-batch by a Python generator."
msgstr ""

#: keras.engine.training.Model.fit_generator:4 of
msgid ""
"`Model.fit` now supports generators, so there is no longer any need to "
"use this endpoint."
msgstr ""

#: keras.engine.base_layer.Layer.from_config:1
#: keras.engine.training.Model.from_config:1 of
msgid "Creates a layer from its config."
msgstr ""

#: keras.engine.base_layer.Layer.from_config:3
#: keras.engine.training.Model.from_config:3 of
msgid ""
"This method is the reverse of `get_config`, capable of instantiating the "
"same layer from the config dictionary. It does not handle layer "
"connectivity (handled by Network), nor weights (handled by "
"`set_weights`)."
msgstr ""

#: keras.engine.base_layer.Layer.from_config:8
#: keras.engine.training.Model.from_config:8 of
msgid "A Python dictionary, typically the output of get_config."
msgstr ""

#: keras.engine.base_layer.Layer.from_config:11
#: keras.engine.training.Model.from_config:11
#: keras.engine.training.Model.get_layer:9 of
msgid "A layer instance."
msgstr ""

#: keras.engine.base_layer.Layer.get_config:1
#: keras.engine.training.Model.get_config:1 of
msgid "Returns the config of the layer."
msgstr ""

#: keras.engine.base_layer.Layer.get_config:3
#: keras.engine.training.Model.get_config:3 of
msgid ""
"A layer config is a Python dictionary (serializable) containing the "
"configuration of a layer. The same layer can be reinstantiated later "
"(without its trained weights) from this configuration."
msgstr ""

#: keras.engine.base_layer.Layer.get_config:8
#: keras.engine.training.Model.get_config:8 of
msgid ""
"The config of a layer does not include connectivity information, nor the "
"layer class name. These are handled by `Network` (one layer of "
"abstraction above)."
msgstr ""

#: keras.engine.base_layer.Layer.get_config:12
#: keras.engine.training.Model.get_config:12 of
msgid ""
"Note that `get_config()` does not guarantee to return a fresh copy of "
"dict every time it is called. The callers should make a copy of the "
"returned dict if they want to modify it."
msgstr ""

#: keras.engine.base_layer.Layer.get_config:16
#: keras.engine.training.Model.get_config:16 of
msgid "Python dictionary."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_at:1 of
msgid "Retrieves the input tensor(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_at:3 of
msgid ""
"Integer, index of the node from which to retrieve the attribute. E.g. "
"`node_index=0` will correspond to the first input node of the layer."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_at:8 of
msgid "A tensor (or list of tensors if the layer has multiple inputs)."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_at:10
#: keras.engine.base_layer.Layer.get_input_shape_at:11
#: keras.engine.base_layer.Layer.get_output_at:10
#: keras.engine.base_layer.Layer.get_output_shape_at:11 of
#: tensorcircuit.applications.van.MADE.input:8
#: tensorcircuit.applications.van.MaskedConv2D.input:8
#: tensorcircuit.applications.van.MaskedLinear.input:8
#: tensorcircuit.applications.van.NMF.input:8
#: tensorcircuit.applications.van.PixelCNN.input:8
#: tensorcircuit.applications.van.ResidualBlock.input:8
#: tensorcircuit.applications.vqes.Linear.input:8
#: tensorcircuit.keras.QuantumLayer.input:8
msgid "If called in Eager mode."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_mask_at:1 of
msgid "Retrieves the input mask tensor(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_mask_at:3
#: keras.engine.base_layer.Layer.get_input_shape_at:3
#: keras.engine.base_layer.Layer.get_output_mask_at:3
#: keras.engine.base_layer.Layer.get_output_shape_at:3 of
msgid ""
"Integer, index of the node from which to retrieve the attribute. E.g. "
"`node_index=0` will correspond to the first time the layer was called."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_mask_at:8 of
msgid "A mask tensor (or list of tensors if the layer has multiple inputs)."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_shape_at:1 of
msgid "Retrieves the input shape(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_input_shape_at:8 of
msgid "A shape tuple (or list of shape tuples if the layer has multiple inputs)."
msgstr ""

#: keras.engine.training.Model.get_layer:1 of
msgid "Retrieves a layer based on either its name (unique) or index."
msgstr ""

#: keras.engine.training.Model.get_layer:3 of
msgid ""
"If `name` and `index` are both provided, `index` will take precedence. "
"Indices are based on order of horizontal graph traversal (bottom-up)."
msgstr ""

#: keras.engine.training.Model.get_layer:6 of
msgid "String, name of layer."
msgstr ""

#: keras.engine.training.Model.get_layer:7 of
msgid "Integer, index of layer."
msgstr ""

#: keras.engine.base_layer.Layer.get_losses_for:3 of
msgid "Retrieves losses relevant to a specific set of inputs."
msgstr ""

#: keras.engine.base_layer.Layer.get_losses_for:5
#: keras.engine.base_layer.Layer.get_updates_for:5 of
msgid "Input tensor or list/tuple of input tensors."
msgstr ""

#: keras.engine.base_layer.Layer.get_losses_for:7 of
msgid "List of loss tensors of the layer that depend on `inputs`."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_at:1 of
msgid "Retrieves the output tensor(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_at:3 of
msgid ""
"Integer, index of the node from which to retrieve the attribute. E.g. "
"`node_index=0` will correspond to the first output node of the layer."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_at:8 of
msgid "A tensor (or list of tensors if the layer has multiple outputs)."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_mask_at:1 of
msgid "Retrieves the output mask tensor(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_mask_at:8 of
msgid "A mask tensor (or list of tensors if the layer has multiple outputs)."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_shape_at:1 of
msgid "Retrieves the output shape(s) of a layer at a given node."
msgstr ""

#: keras.engine.base_layer.Layer.get_output_shape_at:8 of
msgid "A shape tuple (or list of shape tuples if the layer has multiple outputs)."
msgstr ""

#: keras.engine.base_layer.Layer.get_updates_for:3 of
msgid "Retrieves updates relevant to a specific set of inputs."
msgstr ""

#: keras.engine.base_layer.Layer.get_updates_for:7 of
msgid "List of update ops of the layer that depend on `inputs`."
msgstr ""

#: keras.engine.training.Model.get_weights:1 of
msgid "Retrieves the weights of the model."
msgstr ""

#: keras.engine.training.Model.get_weights:3 of
msgid "A flat list of Numpy arrays."
msgstr ""

#: of tensorcircuit.applications.van.MADE.inbound_nodes:1
#: tensorcircuit.applications.van.MADE.outbound_nodes:1
#: tensorcircuit.applications.van.MaskedConv2D.inbound_nodes:1
#: tensorcircuit.applications.van.MaskedConv2D.outbound_nodes:1
#: tensorcircuit.applications.van.MaskedLinear.inbound_nodes:1
#: tensorcircuit.applications.van.MaskedLinear.outbound_nodes:1
#: tensorcircuit.applications.van.NMF.inbound_nodes:1
#: tensorcircuit.applications.van.NMF.outbound_nodes:1
#: tensorcircuit.applications.van.PixelCNN.inbound_nodes:1
#: tensorcircuit.applications.van.PixelCNN.outbound_nodes:1
#: tensorcircuit.applications.van.ResidualBlock.inbound_nodes:1
#: tensorcircuit.applications.van.ResidualBlock.outbound_nodes:1
#: tensorcircuit.applications.vqes.Linear.inbound_nodes:1
#: tensorcircuit.applications.vqes.Linear.outbound_nodes:1
#: tensorcircuit.keras.QuantumLayer.inbound_nodes:1
#: tensorcircuit.keras.QuantumLayer.outbound_nodes:1
msgid "Deprecated, do NOT use! Only for compatibility with external Keras."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input:1
#: tensorcircuit.applications.van.MaskedConv2D.input:1
#: tensorcircuit.applications.van.MaskedLinear.input:1
#: tensorcircuit.applications.van.NMF.input:1
#: tensorcircuit.applications.van.PixelCNN.input:1
#: tensorcircuit.applications.van.ResidualBlock.input:1
#: tensorcircuit.applications.vqes.Linear.input:1
#: tensorcircuit.keras.QuantumLayer.input:1
msgid "Retrieves the input tensor(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input:3
#: tensorcircuit.applications.van.MaskedConv2D.input:3
#: tensorcircuit.applications.van.MaskedLinear.input:3
#: tensorcircuit.applications.van.NMF.input:3
#: tensorcircuit.applications.van.PixelCNN.input:3
#: tensorcircuit.applications.van.ResidualBlock.input:3
#: tensorcircuit.applications.vqes.Linear.input:3
#: tensorcircuit.keras.QuantumLayer.input:3
msgid ""
"Only applicable if the layer has exactly one input, i.e. if it is "
"connected to one incoming layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input:6
#: tensorcircuit.applications.van.MaskedConv2D.input:6
#: tensorcircuit.applications.van.MaskedLinear.input:6
#: tensorcircuit.applications.van.NMF.input:6
#: tensorcircuit.applications.van.PixelCNN.input:6
#: tensorcircuit.applications.van.ResidualBlock.input:6
#: tensorcircuit.applications.vqes.Linear.input:6
#: tensorcircuit.keras.QuantumLayer.input:6
msgid "Input tensor or list of input tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input:9
#: tensorcircuit.applications.van.MaskedConv2D.input:9
#: tensorcircuit.applications.van.MaskedLinear.input:9
#: tensorcircuit.applications.van.NMF.input:9
#: tensorcircuit.applications.van.PixelCNN.input:9
#: tensorcircuit.applications.van.ResidualBlock.input:9
#: tensorcircuit.applications.vqes.Linear.input:9
#: tensorcircuit.keras.QuantumLayer.input:9
msgid "If no inbound nodes are found."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_mask:1
#: tensorcircuit.applications.van.MaskedConv2D.input_mask:1
#: tensorcircuit.applications.van.MaskedLinear.input_mask:1
#: tensorcircuit.applications.van.NMF.input_mask:1
#: tensorcircuit.applications.van.PixelCNN.input_mask:1
#: tensorcircuit.applications.van.ResidualBlock.input_mask:1
#: tensorcircuit.applications.vqes.Linear.input_mask:1
#: tensorcircuit.keras.QuantumLayer.input_mask:1
msgid "Retrieves the input mask tensor(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_mask:3
#: tensorcircuit.applications.van.MADE.output_mask:3
#: tensorcircuit.applications.van.MaskedConv2D.input_mask:3
#: tensorcircuit.applications.van.MaskedConv2D.output_mask:3
#: tensorcircuit.applications.van.MaskedLinear.input_mask:3
#: tensorcircuit.applications.van.MaskedLinear.output_mask:3
#: tensorcircuit.applications.van.NMF.input_mask:3
#: tensorcircuit.applications.van.NMF.output_mask:3
#: tensorcircuit.applications.van.PixelCNN.input_mask:3
#: tensorcircuit.applications.van.PixelCNN.output_mask:3
#: tensorcircuit.applications.van.ResidualBlock.input_mask:3
#: tensorcircuit.applications.van.ResidualBlock.output_mask:3
#: tensorcircuit.applications.vqes.Linear.input_mask:3
#: tensorcircuit.applications.vqes.Linear.output_mask:3
#: tensorcircuit.keras.QuantumLayer.input_mask:3
#: tensorcircuit.keras.QuantumLayer.output_mask:3
msgid ""
"Only applicable if the layer has exactly one inbound node, i.e. if it is "
"connected to one incoming layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_mask:6
#: tensorcircuit.applications.van.MaskedConv2D.input_mask:6
#: tensorcircuit.applications.van.MaskedLinear.input_mask:6
#: tensorcircuit.applications.van.NMF.input_mask:6
#: tensorcircuit.applications.van.PixelCNN.input_mask:6
#: tensorcircuit.applications.van.ResidualBlock.input_mask:6
#: tensorcircuit.applications.vqes.Linear.input_mask:6
#: tensorcircuit.keras.QuantumLayer.input_mask:6
msgid "Input mask tensor (potentially None) or list of input mask tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_mask:9
#: tensorcircuit.applications.van.MADE.output_mask:9
#: tensorcircuit.applications.van.MaskedConv2D.input_mask:9
#: tensorcircuit.applications.van.MaskedConv2D.output_mask:9
#: tensorcircuit.applications.van.MaskedLinear.input_mask:9
#: tensorcircuit.applications.van.MaskedLinear.output_mask:9
#: tensorcircuit.applications.van.NMF.input_mask:9
#: tensorcircuit.applications.van.NMF.output_mask:9
#: tensorcircuit.applications.van.PixelCNN.input_mask:9
#: tensorcircuit.applications.van.PixelCNN.output_mask:9
#: tensorcircuit.applications.van.ResidualBlock.input_mask:9
#: tensorcircuit.applications.van.ResidualBlock.output_mask:9
#: tensorcircuit.applications.vqes.Linear.input_mask:9
#: tensorcircuit.applications.vqes.Linear.output_mask:9
#: tensorcircuit.keras.QuantumLayer.input_mask:9
#: tensorcircuit.keras.QuantumLayer.output_mask:9
msgid "if the layer is connected to"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_shape:1
#: tensorcircuit.applications.van.MaskedConv2D.input_shape:1
#: tensorcircuit.applications.van.MaskedLinear.input_shape:1
#: tensorcircuit.applications.van.NMF.input_shape:1
#: tensorcircuit.applications.van.PixelCNN.input_shape:1
#: tensorcircuit.applications.van.ResidualBlock.input_shape:1
#: tensorcircuit.applications.vqes.Linear.input_shape:1
#: tensorcircuit.keras.QuantumLayer.input_shape:1
msgid "Retrieves the input shape(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_shape:3
#: tensorcircuit.applications.van.MaskedConv2D.input_shape:3
#: tensorcircuit.applications.van.MaskedLinear.input_shape:3
#: tensorcircuit.applications.van.NMF.input_shape:3
#: tensorcircuit.applications.van.PixelCNN.input_shape:3
#: tensorcircuit.applications.van.ResidualBlock.input_shape:3
#: tensorcircuit.applications.vqes.Linear.input_shape:3
#: tensorcircuit.keras.QuantumLayer.input_shape:3
msgid ""
"Only applicable if the layer has exactly one input, i.e. if it is "
"connected to one incoming layer, or if all inputs have the same shape."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_shape:7
#: tensorcircuit.applications.van.MaskedConv2D.input_shape:7
#: tensorcircuit.applications.van.MaskedLinear.input_shape:7
#: tensorcircuit.applications.van.NMF.input_shape:7
#: tensorcircuit.applications.van.PixelCNN.input_shape:7
#: tensorcircuit.applications.van.ResidualBlock.input_shape:7
#: tensorcircuit.applications.vqes.Linear.input_shape:7
#: tensorcircuit.keras.QuantumLayer.input_shape:7
msgid ""
"Input shape, as an integer shape tuple (or list of shape tuples, one "
"tuple per input tensor)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_shape:10
#: tensorcircuit.applications.van.MaskedConv2D.input_shape:10
#: tensorcircuit.applications.van.MaskedLinear.input_shape:10
#: tensorcircuit.applications.van.NMF.input_shape:10
#: tensorcircuit.applications.van.PixelCNN.input_shape:10
#: tensorcircuit.applications.van.ResidualBlock.input_shape:10
#: tensorcircuit.applications.vqes.Linear.input_shape:10
#: tensorcircuit.keras.QuantumLayer.input_shape:10
msgid "if the layer has no defined input_shape."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_shape:11
#: tensorcircuit.applications.van.MADE.output:9
#: tensorcircuit.applications.van.MADE.output_shape:10
#: tensorcircuit.applications.van.MaskedConv2D.input_shape:11
#: tensorcircuit.applications.van.MaskedConv2D.output:9
#: tensorcircuit.applications.van.MaskedConv2D.output_shape:10
#: tensorcircuit.applications.van.MaskedLinear.input_shape:11
#: tensorcircuit.applications.van.MaskedLinear.output:9
#: tensorcircuit.applications.van.MaskedLinear.output_shape:10
#: tensorcircuit.applications.van.NMF.input_shape:11
#: tensorcircuit.applications.van.NMF.output:9
#: tensorcircuit.applications.van.NMF.output_shape:10
#: tensorcircuit.applications.van.PixelCNN.input_shape:11
#: tensorcircuit.applications.van.PixelCNN.output:9
#: tensorcircuit.applications.van.PixelCNN.output_shape:10
#: tensorcircuit.applications.van.ResidualBlock.input_shape:11
#: tensorcircuit.applications.van.ResidualBlock.output:9
#: tensorcircuit.applications.van.ResidualBlock.output_shape:10
#: tensorcircuit.applications.vqes.Linear.input_shape:11
#: tensorcircuit.applications.vqes.Linear.output:9
#: tensorcircuit.applications.vqes.Linear.output_shape:10
#: tensorcircuit.keras.QuantumLayer.input_shape:11
#: tensorcircuit.keras.QuantumLayer.output:9
#: tensorcircuit.keras.QuantumLayer.output_shape:10
msgid "if called in Eager mode."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:1
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:1
#: tensorcircuit.applications.van.MaskedLinear.input_spec:1
#: tensorcircuit.applications.van.NMF.input_spec:1
#: tensorcircuit.applications.van.PixelCNN.input_spec:1
#: tensorcircuit.applications.van.ResidualBlock.input_spec:1
#: tensorcircuit.applications.vqes.Linear.input_spec:1
#: tensorcircuit.keras.QuantumLayer.input_spec:1
msgid "`InputSpec` instance(s) describing the input format for this layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:3
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:3
#: tensorcircuit.applications.van.MaskedLinear.input_spec:3
#: tensorcircuit.applications.van.NMF.input_spec:3
#: tensorcircuit.applications.van.PixelCNN.input_spec:3
#: tensorcircuit.applications.van.ResidualBlock.input_spec:3
#: tensorcircuit.applications.vqes.Linear.input_spec:3
#: tensorcircuit.keras.QuantumLayer.input_spec:3
msgid ""
"When you create a layer subclass, you can set `self.input_spec` to enable"
" the layer to run input compatibility checks when it is called. Consider "
"a `Conv2D` layer: it can only be called on a single input tensor of rank "
"4. As such, you can set, in `__init__()`:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:8
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:8
#: tensorcircuit.applications.van.MaskedLinear.input_spec:8
#: tensorcircuit.applications.van.NMF.input_spec:8
#: tensorcircuit.applications.van.PixelCNN.input_spec:8
#: tensorcircuit.applications.van.ResidualBlock.input_spec:8
#: tensorcircuit.applications.vqes.Linear.input_spec:8
#: tensorcircuit.keras.QuantumLayer.input_spec:8
msgid "```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ```"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:12
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:12
#: tensorcircuit.applications.van.MaskedLinear.input_spec:12
#: tensorcircuit.applications.van.NMF.input_spec:12
#: tensorcircuit.applications.van.PixelCNN.input_spec:12
#: tensorcircuit.applications.van.ResidualBlock.input_spec:12
#: tensorcircuit.applications.vqes.Linear.input_spec:12
#: tensorcircuit.keras.QuantumLayer.input_spec:12
msgid ""
"Now, if you try to call the layer on an input that isn't rank 4 (for "
"instance, an input of shape `(2,)`, it will raise a nicely-formatted "
"error:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:16
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:16
#: tensorcircuit.applications.van.MaskedLinear.input_spec:16
#: tensorcircuit.applications.van.NMF.input_spec:16
#: tensorcircuit.applications.van.PixelCNN.input_spec:16
#: tensorcircuit.applications.van.ResidualBlock.input_spec:16
#: tensorcircuit.applications.vqes.Linear.input_spec:16
#: tensorcircuit.keras.QuantumLayer.input_spec:16
msgid ""
"``` ValueError: Input 0 of layer conv2d is incompatible with the layer: "
"expected ndim=4, found ndim=1. Full shape received: [2] ```"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:21
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:21
#: tensorcircuit.applications.van.MaskedLinear.input_spec:21
#: tensorcircuit.applications.van.NMF.input_spec:21
#: tensorcircuit.applications.van.PixelCNN.input_spec:21
#: tensorcircuit.applications.van.ResidualBlock.input_spec:21
#: tensorcircuit.applications.vqes.Linear.input_spec:21
#: tensorcircuit.keras.QuantumLayer.input_spec:21
msgid ""
"Input checks that can be specified via `input_spec` include: - Structure "
"(e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - "
"Dtype"
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:27
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:27
#: tensorcircuit.applications.van.MaskedLinear.input_spec:27
#: tensorcircuit.applications.van.NMF.input_spec:27
#: tensorcircuit.applications.van.PixelCNN.input_spec:27
#: tensorcircuit.applications.van.ResidualBlock.input_spec:27
#: tensorcircuit.applications.vqes.Linear.input_spec:27
#: tensorcircuit.keras.QuantumLayer.input_spec:27
msgid "For more information, see `tf.keras.layers.InputSpec`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.input_spec:29
#: tensorcircuit.applications.van.MaskedConv2D.input_spec:29
#: tensorcircuit.applications.van.MaskedLinear.input_spec:29
#: tensorcircuit.applications.van.NMF.input_spec:29
#: tensorcircuit.applications.van.PixelCNN.input_spec:29
#: tensorcircuit.applications.van.ResidualBlock.input_spec:29
#: tensorcircuit.applications.vqes.Linear.input_spec:29
#: tensorcircuit.keras.QuantumLayer.input_spec:29
msgid "A `tf.keras.layers.InputSpec` instance, or nested structure thereof."
msgstr ""

#: keras.engine.training.Model.load_weights:1 of
msgid "Loads all layer weights, either from a TensorFlow or an HDF5 weight file."
msgstr ""

#: keras.engine.training.Model.load_weights:3 of
msgid ""
"If `by_name` is False weights are loaded based on the network's topology."
" This means the architecture should be the same as when the weights were "
"saved.  Note that layers that don't have weights are not taken into "
"account in the topological ordering, so adding or removing layers is fine"
" as long as they don't have weights."
msgstr ""

#: keras.engine.training.Model.load_weights:9 of
msgid ""
"If `by_name` is True, weights are loaded into layers only if they share "
"the same name. This is useful for fine-tuning or transfer-learning models"
" where some of the layers have changed."
msgstr ""

#: keras.engine.training.Model.load_weights:13 of
msgid ""
"Only topological loading (`by_name=False`) is supported when loading "
"weights from the TensorFlow format. Note that topological loading differs"
" slightly between TensorFlow and HDF5 formats for user-defined classes "
"inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of"
" weights, while the TensorFlow format loads based on the object-local "
"names of attributes to which layers are assigned in the `Model`'s "
"constructor."
msgstr ""

#: keras.engine.training.Model.load_weights:20 of
msgid ""
"String, path to the weights file to load. For weight files in TensorFlow "
"format, this is the file prefix (the same as was passed to "
"`save_weights`). This can also be a path to a SavedModel saved from "
"`model.save`."
msgstr ""

#: keras.engine.training.Model.load_weights:24 of
msgid ""
"Boolean, whether to load weights by name or by topological order. Only "
"topological loading is supported for weight files in TensorFlow format."
msgstr ""

#: keras.engine.training.Model.load_weights:27 of
msgid ""
"Boolean, whether to skip loading of layers where there is a mismatch in "
"the number of weights, or a mismatch in the shape of the weight (only "
"valid when `by_name=True`)."
msgstr ""

#: keras.engine.training.Model.load_weights:30 of
msgid ""
"Optional `tf.train.CheckpointOptions` object that specifies options for "
"loading weights."
msgstr ""

#: keras.engine.training.Model.load_weights:33 of
msgid ""
"When loading a weight file in TensorFlow format, returns the same status "
"object as `tf.train.Checkpoint.restore`. When graph building, restore ops"
" are run automatically as soon as the network is built (on first call for"
" user-defined classes inheriting from `Model`, immediately if it is "
"already built).  When loading weights in HDF5 format, returns `None`."
msgstr ""

#: keras.engine.training.Model.load_weights:33 of
msgid ""
"When loading a weight file in TensorFlow format, returns the same status "
"object as `tf.train.Checkpoint.restore`. When graph building, restore ops"
" are run automatically as soon as the network is built (on first call for"
" user-defined classes inheriting from `Model`, immediately if it is "
"already built)."
msgstr ""

#: keras.engine.training.Model.load_weights:39 of
msgid "When loading weights in HDF5 format, returns `None`."
msgstr ""

#: keras.engine.training.Model.load_weights:41 of
msgid "If `h5py` is not available and the weight file is in HDF5     format."
msgstr ""

#: keras.engine.training.Model.load_weights:42 of
msgid "If `skip_mismatch` is set to `True` when `by_name` is     `False`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.losses:1
#: tensorcircuit.applications.van.MaskedConv2D.losses:1
#: tensorcircuit.applications.van.MaskedLinear.losses:1
#: tensorcircuit.applications.van.NMF.losses:1
#: tensorcircuit.applications.van.PixelCNN.losses:1
#: tensorcircuit.applications.van.ResidualBlock.losses:1
#: tensorcircuit.applications.vqes.Linear.losses:1
#: tensorcircuit.keras.QuantumLayer.losses:1
msgid "List of losses added using the `add_loss()` API."
msgstr ""

#: of tensorcircuit.applications.van.MADE.losses:3
#: tensorcircuit.applications.van.MaskedConv2D.losses:3
#: tensorcircuit.applications.van.MaskedLinear.losses:3
#: tensorcircuit.applications.van.NMF.losses:3
#: tensorcircuit.applications.van.PixelCNN.losses:3
#: tensorcircuit.applications.van.ResidualBlock.losses:3
#: tensorcircuit.applications.vqes.Linear.losses:3
#: tensorcircuit.keras.QuantumLayer.losses:3
msgid ""
"Variable regularization tensors are created when this property is "
"accessed, so it is eager safe: accessing `losses` under a "
"`tf.GradientTape` will propagate gradients back to the corresponding "
"variables."
msgstr ""

#: keras.engine.training.Model.reset_metrics:3 of
#: tensorcircuit.applications.van.MADE.losses:7
#: tensorcircuit.applications.van.MADE.metrics:6
#: tensorcircuit.applications.van.MADE.metrics_names:6
#: tensorcircuit.applications.van.MaskedConv2D.losses:7
#: tensorcircuit.applications.van.MaskedLinear.losses:7
#: tensorcircuit.applications.van.NMF.losses:7
#: tensorcircuit.applications.van.NMF.metrics:6
#: tensorcircuit.applications.van.NMF.metrics_names:6
#: tensorcircuit.applications.van.PixelCNN.losses:7
#: tensorcircuit.applications.van.PixelCNN.metrics:6
#: tensorcircuit.applications.van.PixelCNN.metrics_names:6
#: tensorcircuit.applications.van.ResidualBlock.losses:7
#: tensorcircuit.applications.vqes.Linear.losses:7
#: tensorcircuit.keras.QuantumLayer.losses:7
msgid "Examples:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.losses:39
#: tensorcircuit.applications.van.MaskedConv2D.losses:39
#: tensorcircuit.applications.van.MaskedLinear.losses:39
#: tensorcircuit.applications.van.NMF.losses:39
#: tensorcircuit.applications.van.PixelCNN.losses:39
#: tensorcircuit.applications.van.ResidualBlock.losses:39
#: tensorcircuit.applications.vqes.Linear.losses:39
#: tensorcircuit.keras.QuantumLayer.losses:39
msgid "A list of tensors."
msgstr ""

#: keras.engine.training.Model.make_predict_function:1 of
msgid "Creates a function that executes one step of inference."
msgstr ""

#: keras.engine.training.Model.make_predict_function:3 of
msgid ""
"This method can be overridden to support custom inference logic. This "
"method is called by `Model.predict` and `Model.predict_on_batch`."
msgstr ""

#: keras.engine.training.Model.make_predict_function:6 of
msgid ""
"Typically, this method directly controls `tf.function` and "
"`tf.distribute.Strategy` settings, and delegates the actual evaluation "
"logic to `Model.predict_step`."
msgstr ""

#: keras.engine.training.Model.make_predict_function:10 of
msgid ""
"This function is cached the first time `Model.predict` or "
"`Model.predict_on_batch` is called. The cache is cleared whenever "
"`Model.compile` is called. You can skip the cache and generate again the "
"function with `force=True`."
msgstr ""

#: keras.engine.training.Model.make_predict_function:15 of
msgid ""
"Whether to regenerate the predict function and skip the cached function "
"if available."
msgstr ""

#: keras.engine.training.Model.make_predict_function:18 of
msgid ""
"Function. The function created by this method should accept a "
"`tf.data.Iterator`, and return the outputs of the `Model`."
msgstr ""

#: keras.engine.training.Model.make_test_function:1 of
msgid "Creates a function that executes one step of evaluation."
msgstr ""

#: keras.engine.training.Model.make_test_function:3 of
msgid ""
"This method can be overridden to support custom evaluation logic. This "
"method is called by `Model.evaluate` and `Model.test_on_batch`."
msgstr ""

#: keras.engine.training.Model.make_test_function:6 of
msgid ""
"Typically, this method directly controls `tf.function` and "
"`tf.distribute.Strategy` settings, and delegates the actual evaluation "
"logic to `Model.test_step`."
msgstr ""

#: keras.engine.training.Model.make_test_function:10 of
msgid ""
"This function is cached the first time `Model.evaluate` or "
"`Model.test_on_batch` is called. The cache is cleared whenever "
"`Model.compile` is called. You can skip the cache and generate again the "
"function with `force=True`."
msgstr ""

#: keras.engine.training.Model.make_test_function:15 of
msgid ""
"Whether to regenerate the test function and skip the cached function if "
"available."
msgstr ""

#: keras.engine.training.Model.make_test_function:18 of
msgid ""
"Function. The function created by this method should accept a "
"`tf.data.Iterator`, and return a `dict` containing values that will be "
"passed to `tf.keras.Callbacks.on_test_batch_end`."
msgstr ""

#: keras.engine.training.Model.make_train_function:1 of
msgid "Creates a function that executes one step of training."
msgstr ""

#: keras.engine.training.Model.make_train_function:3 of
msgid ""
"This method can be overridden to support custom training logic. This "
"method is called by `Model.fit` and `Model.train_on_batch`."
msgstr ""

#: keras.engine.training.Model.make_train_function:6 of
msgid ""
"Typically, this method directly controls `tf.function` and "
"`tf.distribute.Strategy` settings, and delegates the actual training "
"logic to `Model.train_step`."
msgstr ""

#: keras.engine.training.Model.make_train_function:10 of
msgid ""
"This function is cached the first time `Model.fit` or "
"`Model.train_on_batch` is called. The cache is cleared whenever "
"`Model.compile` is called. You can skip the cache and generate again the "
"function with `force=True`."
msgstr ""

#: keras.engine.training.Model.make_train_function:15 of
msgid ""
"Whether to regenerate the train function and skip the cached function if "
"available."
msgstr ""

#: keras.engine.training.Model.make_train_function:18 of
msgid ""
"Function. The function created by this method should accept a "
"`tf.data.Iterator`, and return a `dict` containing values that will be "
"passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2,"
" 'accuracy': 0.7}`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.metrics:1
#: tensorcircuit.applications.van.NMF.metrics:1
#: tensorcircuit.applications.van.PixelCNN.metrics:1
msgid "Returns the model's metrics added using `compile()`, `add_metric()` APIs."
msgstr ""

#: of tensorcircuit.applications.van.MADE.metrics:3
#: tensorcircuit.applications.van.NMF.metrics:3
#: tensorcircuit.applications.van.PixelCNN.metrics:3
msgid ""
"Note: Metrics passed to `compile()` are available only after a "
"`keras.Model` has been trained/evaluated on actual data."
msgstr ""

#: of tensorcircuit.applications.van.MADE.metrics_names:1
#: tensorcircuit.applications.van.NMF.metrics_names:1
#: tensorcircuit.applications.van.PixelCNN.metrics_names:1
msgid "Returns the model's display labels for all outputs."
msgstr ""

#: of tensorcircuit.applications.van.MADE.metrics_names:3
#: tensorcircuit.applications.van.NMF.metrics_names:3
#: tensorcircuit.applications.van.PixelCNN.metrics_names:3
msgid ""
"Note: `metrics_names` are available only after a `keras.Model` has been "
"trained/evaluated on actual data."
msgstr ""

#: of tensorcircuit.applications.van.MADE.name:1
#: tensorcircuit.applications.van.MaskedConv2D.name:1
#: tensorcircuit.applications.van.MaskedLinear.name:1
#: tensorcircuit.applications.van.NMF.name:1
#: tensorcircuit.applications.van.PixelCNN.name:1
#: tensorcircuit.applications.van.ResidualBlock.name:1
#: tensorcircuit.applications.vqes.Linear.name:1
#: tensorcircuit.keras.QuantumLayer.name:1
msgid "Name of the layer (string), set in the constructor."
msgstr ""

#: of tensorcircuit.applications.van.MADE.name_scope:1
#: tensorcircuit.applications.van.MaskedConv2D.name_scope:1
#: tensorcircuit.applications.van.MaskedLinear.name_scope:1
#: tensorcircuit.applications.van.NMF.name_scope:1
#: tensorcircuit.applications.van.PixelCNN.name_scope:1
#: tensorcircuit.applications.van.ResidualBlock.name_scope:1
#: tensorcircuit.applications.vqes.Linear.name_scope:1
#: tensorcircuit.keras.QuantumLayer.name_scope:1
msgid "Returns a `tf.name_scope` instance for this class."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_variables:1
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_variables:1
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_variables:1
#: tensorcircuit.applications.van.NMF.non_trainable_variables:1
#: tensorcircuit.applications.van.PixelCNN.non_trainable_variables:1
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_variables:1
#: tensorcircuit.applications.vqes.Linear.non_trainable_variables:1
#: tensorcircuit.keras.QuantumLayer.non_trainable_variables:1
msgid ""
"Sequence of non-trainable variables owned by this module and its "
"submodules."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_variables:3
#: tensorcircuit.applications.van.MADE.trainable_variables:3
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_variables:3
#: tensorcircuit.applications.van.MaskedConv2D.trainable_variables:3
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_variables:3
#: tensorcircuit.applications.van.MaskedLinear.trainable_variables:3
#: tensorcircuit.applications.van.NMF.non_trainable_variables:3
#: tensorcircuit.applications.van.NMF.trainable_variables:3
#: tensorcircuit.applications.van.PixelCNN.non_trainable_variables:3
#: tensorcircuit.applications.van.PixelCNN.trainable_variables:3
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_variables:3
#: tensorcircuit.applications.van.ResidualBlock.trainable_variables:3
#: tensorcircuit.applications.vqes.Linear.non_trainable_variables:3
#: tensorcircuit.applications.vqes.Linear.trainable_variables:3
#: tensorcircuit.keras.QuantumLayer.non_trainable_variables:3
#: tensorcircuit.keras.QuantumLayer.trainable_variables:3
msgid ""
"Note: this method uses reflection to find variables on the current "
"instance and submodules. For performance reasons you may wish to cache "
"the result of calling this method if you don't expect the return value to"
" change."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_variables:7
#: tensorcircuit.applications.van.MADE.trainable_variables:7
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_variables:7
#: tensorcircuit.applications.van.MaskedConv2D.trainable_variables:7
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_variables:7
#: tensorcircuit.applications.van.MaskedLinear.trainable_variables:7
#: tensorcircuit.applications.van.NMF.non_trainable_variables:7
#: tensorcircuit.applications.van.NMF.trainable_variables:7
#: tensorcircuit.applications.van.PixelCNN.non_trainable_variables:7
#: tensorcircuit.applications.van.PixelCNN.trainable_variables:7
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_variables:7
#: tensorcircuit.applications.van.ResidualBlock.trainable_variables:7
#: tensorcircuit.applications.vqes.Linear.non_trainable_variables:7
#: tensorcircuit.applications.vqes.Linear.trainable_variables:7
#: tensorcircuit.keras.QuantumLayer.non_trainable_variables:7
#: tensorcircuit.keras.QuantumLayer.trainable_variables:7
msgid ""
"A sequence of variables for the current module (sorted by attribute name)"
" followed by variables from all submodules recursively (breadth first)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_weights:1
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_weights:1
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_weights:1
#: tensorcircuit.applications.van.NMF.non_trainable_weights:1
#: tensorcircuit.applications.van.PixelCNN.non_trainable_weights:1
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_weights:1
#: tensorcircuit.applications.vqes.Linear.non_trainable_weights:1
#: tensorcircuit.keras.QuantumLayer.non_trainable_weights:1
msgid "List of all non-trainable weights tracked by this layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_weights:3
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_weights:3
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_weights:3
#: tensorcircuit.applications.van.NMF.non_trainable_weights:3
#: tensorcircuit.applications.van.PixelCNN.non_trainable_weights:3
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_weights:3
#: tensorcircuit.applications.vqes.Linear.non_trainable_weights:3
#: tensorcircuit.keras.QuantumLayer.non_trainable_weights:3
msgid ""
"Non-trainable weights are *not* updated during training. They are "
"expected to be updated manually in `call()`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.non_trainable_weights:6
#: tensorcircuit.applications.van.MaskedConv2D.non_trainable_weights:6
#: tensorcircuit.applications.van.MaskedLinear.non_trainable_weights:6
#: tensorcircuit.applications.van.NMF.non_trainable_weights:6
#: tensorcircuit.applications.van.PixelCNN.non_trainable_weights:6
#: tensorcircuit.applications.van.ResidualBlock.non_trainable_weights:6
#: tensorcircuit.applications.vqes.Linear.non_trainable_weights:6
#: tensorcircuit.keras.QuantumLayer.non_trainable_weights:6
msgid "A list of non-trainable variables."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output:1
#: tensorcircuit.applications.van.MaskedConv2D.output:1
#: tensorcircuit.applications.van.MaskedLinear.output:1
#: tensorcircuit.applications.van.NMF.output:1
#: tensorcircuit.applications.van.PixelCNN.output:1
#: tensorcircuit.applications.van.ResidualBlock.output:1
#: tensorcircuit.applications.vqes.Linear.output:1
#: tensorcircuit.keras.QuantumLayer.output:1
msgid "Retrieves the output tensor(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output:3
#: tensorcircuit.applications.van.MaskedConv2D.output:3
#: tensorcircuit.applications.van.MaskedLinear.output:3
#: tensorcircuit.applications.van.NMF.output:3
#: tensorcircuit.applications.van.PixelCNN.output:3
#: tensorcircuit.applications.van.ResidualBlock.output:3
#: tensorcircuit.applications.vqes.Linear.output:3
#: tensorcircuit.keras.QuantumLayer.output:3
msgid ""
"Only applicable if the layer has exactly one output, i.e. if it is "
"connected to one incoming layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output:6
#: tensorcircuit.applications.van.MaskedConv2D.output:6
#: tensorcircuit.applications.van.MaskedLinear.output:6
#: tensorcircuit.applications.van.NMF.output:6
#: tensorcircuit.applications.van.PixelCNN.output:6
#: tensorcircuit.applications.van.ResidualBlock.output:6
#: tensorcircuit.applications.vqes.Linear.output:6
#: tensorcircuit.keras.QuantumLayer.output:6
msgid "Output tensor or list of output tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output:8
#: tensorcircuit.applications.van.MaskedConv2D.output:8
#: tensorcircuit.applications.van.MaskedLinear.output:8
#: tensorcircuit.applications.van.NMF.output:8
#: tensorcircuit.applications.van.PixelCNN.output:8
#: tensorcircuit.applications.van.ResidualBlock.output:8
#: tensorcircuit.applications.vqes.Linear.output:8
#: tensorcircuit.keras.QuantumLayer.output:8
msgid "if the layer is connected to more than one incoming     layers."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_mask:1
#: tensorcircuit.applications.van.MaskedConv2D.output_mask:1
#: tensorcircuit.applications.van.MaskedLinear.output_mask:1
#: tensorcircuit.applications.van.NMF.output_mask:1
#: tensorcircuit.applications.van.PixelCNN.output_mask:1
#: tensorcircuit.applications.van.ResidualBlock.output_mask:1
#: tensorcircuit.applications.vqes.Linear.output_mask:1
#: tensorcircuit.keras.QuantumLayer.output_mask:1
msgid "Retrieves the output mask tensor(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_mask:6
#: tensorcircuit.applications.van.MaskedConv2D.output_mask:6
#: tensorcircuit.applications.van.MaskedLinear.output_mask:6
#: tensorcircuit.applications.van.NMF.output_mask:6
#: tensorcircuit.applications.van.PixelCNN.output_mask:6
#: tensorcircuit.applications.van.ResidualBlock.output_mask:6
#: tensorcircuit.applications.vqes.Linear.output_mask:6
#: tensorcircuit.keras.QuantumLayer.output_mask:6
msgid "Output mask tensor (potentially None) or list of output mask tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_shape:1
#: tensorcircuit.applications.van.MaskedConv2D.output_shape:1
#: tensorcircuit.applications.van.MaskedLinear.output_shape:1
#: tensorcircuit.applications.van.NMF.output_shape:1
#: tensorcircuit.applications.van.PixelCNN.output_shape:1
#: tensorcircuit.applications.van.ResidualBlock.output_shape:1
#: tensorcircuit.applications.vqes.Linear.output_shape:1
#: tensorcircuit.keras.QuantumLayer.output_shape:1
msgid "Retrieves the output shape(s) of a layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_shape:3
#: tensorcircuit.applications.van.MaskedConv2D.output_shape:3
#: tensorcircuit.applications.van.MaskedLinear.output_shape:3
#: tensorcircuit.applications.van.NMF.output_shape:3
#: tensorcircuit.applications.van.PixelCNN.output_shape:3
#: tensorcircuit.applications.van.ResidualBlock.output_shape:3
#: tensorcircuit.applications.vqes.Linear.output_shape:3
#: tensorcircuit.keras.QuantumLayer.output_shape:3
msgid ""
"Only applicable if the layer has one output, or if all outputs have the "
"same shape."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_shape:6
#: tensorcircuit.applications.van.MaskedConv2D.output_shape:6
#: tensorcircuit.applications.van.MaskedLinear.output_shape:6
#: tensorcircuit.applications.van.NMF.output_shape:6
#: tensorcircuit.applications.van.PixelCNN.output_shape:6
#: tensorcircuit.applications.van.ResidualBlock.output_shape:6
#: tensorcircuit.applications.vqes.Linear.output_shape:6
#: tensorcircuit.keras.QuantumLayer.output_shape:6
msgid ""
"Output shape, as an integer shape tuple (or list of shape tuples, one "
"tuple per output tensor)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.output_shape:9
#: tensorcircuit.applications.van.MaskedConv2D.output_shape:9
#: tensorcircuit.applications.van.MaskedLinear.output_shape:9
#: tensorcircuit.applications.van.NMF.output_shape:9
#: tensorcircuit.applications.van.PixelCNN.output_shape:9
#: tensorcircuit.applications.van.ResidualBlock.output_shape:9
#: tensorcircuit.applications.vqes.Linear.output_shape:9
#: tensorcircuit.keras.QuantumLayer.output_shape:9
msgid "if the layer has no defined output shape."
msgstr ""

#: keras.engine.training.Model.predict:1 of
msgid "Generates output predictions for the input samples."
msgstr ""

#: keras.engine.training.Model.predict:3 of
msgid ""
"Computation is done in batches. This method is designed for performance "
"in large scale inputs. For small amount of inputs that fit in one batch, "
"directly using `__call__()` is recommended for faster execution, e.g., "
"`model(x)`, or `model(x, training=False)` if you have layers such as "
"`tf.keras.layers.BatchNormalization` that behaves differently during "
"inference. Also, note the fact that test loss is not affected by "
"regularization layers like noise and dropout."
msgstr ""

#: keras.engine.training.Model.predict:11 of
msgid ""
"Input samples. It could be: - A Numpy array (or array-like), or a list of"
" arrays   (in case the model has multiple inputs). - A TensorFlow tensor,"
" or a list of tensors   (in case the model has multiple inputs). - A "
"`tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A "
"more detailed description of unpacking behavior for iterator types "
"(Dataset, generator, Sequence) is given in the `Unpacking behavior for "
"iterator-like inputs` section of `Model.fit`."
msgstr ""

#: keras.engine.training.Model.predict:11 of
msgid ""
"Input samples. It could be: - A Numpy array (or array-like), or a list of"
" arrays"
msgstr ""

#: keras.engine.training.Model.predict:16 of
msgid "A `tf.data` dataset."
msgstr ""

#: keras.engine.training.Model.predict:17 of
msgid "A generator or `keras.utils.Sequence` instance."
msgstr ""

#: keras.engine.training.Model.predict:21 of
msgid ""
"Integer or `None`. Number of samples per batch. If unspecified, "
"`batch_size` will default to 32. Do not specify the `batch_size` if your "
"data is in the form of dataset, generators, or `keras.utils.Sequence` "
"instances (since they generate batches)."
msgstr ""

#: keras.engine.training.Model.predict:27 of
msgid "Verbosity mode, 0 or 1."
msgstr ""

#: keras.engine.training.Model.predict:28 of
msgid ""
"Total number of steps (batches of samples) before declaring the "
"prediction round finished. Ignored with the default value of `None`. If x"
" is a `tf.data` dataset and `steps` is None, `predict()` will run until "
"the input dataset is exhausted."
msgstr ""

#: keras.engine.training.Model.predict:33 of
msgid ""
"List of `keras.callbacks.Callback` instances. List of callbacks to apply "
"during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks)."
msgstr ""

#: keras.engine.training.Model.predict:50 of
msgid ""
"See the discussion of `Unpacking behavior for iterator-like inputs` for "
"`Model.fit`. Note that Model.predict uses the same interpretation rules "
"as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for "
"all three methods."
msgstr ""

#: keras.engine.training.Model.predict:55
#: keras.engine.training.Model.predict_on_batch:9 of
msgid "Numpy array(s) of predictions."
msgstr ""

#: keras.engine.training.Model.predict:57 of
msgid "If `model.predict` is wrapped in a `tf.function`."
msgstr ""

#: keras.engine.training.Model.predict:58 of
msgid ""
"In case of mismatch between the provided     input data and the model's "
"expectations,     or in case a stateful model receives a number of "
"samples     that is not a multiple of the batch size."
msgstr ""

#: keras.engine.training.Model.predict_generator:1 of
msgid "Generates predictions for the input samples from a data generator."
msgstr ""

#: keras.engine.training.Model.predict_generator:4 of
msgid ""
"`Model.predict` now supports generators, so there is no longer any need "
"to use this endpoint."
msgstr ""

#: keras.engine.training.Model.predict_on_batch:1 of
msgid "Returns predictions for a single batch of samples."
msgstr ""

#: keras.engine.training.Model.predict_on_batch:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays (in case the     model has multiple inputs). - A TensorFlow "
"tensor, or a list of tensors (in case the model has     multiple inputs)."
msgstr ""

#: keras.engine.training.Model.predict_on_batch:3
#: keras.engine.training.Model.test_on_batch:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays (in case the"
msgstr ""

#: keras.engine.training.Model.predict_on_batch:5
#: keras.engine.training.Model.test_on_batch:5 of
msgid "model has multiple inputs)."
msgstr ""

#: keras.engine.training.Model.predict_on_batch:7
#: keras.engine.training.Model.test_on_batch:6 of
msgid "A TensorFlow tensor, or a list of tensors (in case the model has"
msgstr ""

#: keras.engine.training.Model.predict_on_batch:7
#: keras.engine.training.Model.test_on_batch:7 of
msgid "multiple inputs)."
msgstr ""

#: keras.engine.training.Model.predict_on_batch:11 of
msgid "If `model.predict_on_batch` is wrapped in a `tf.function`."
msgstr ""

#: keras.engine.training.Model.predict_step:1 of
msgid "The logic for one inference step."
msgstr ""

#: keras.engine.training.Model.predict_step:3 of
msgid ""
"This method can be overridden to support custom inference logic. This "
"method is called by `Model.make_predict_function`."
msgstr ""

#: keras.engine.training.Model.predict_step:6 of
msgid ""
"This method should contain the mathematical logic for one step of "
"inference. This typically includes the forward pass."
msgstr ""

#: keras.engine.training.Model.predict_step:9 of
msgid ""
"Configuration details for *how* this logic is run (e.g. `tf.function` and"
" `tf.distribute.Strategy` settings), should be left to "
"`Model.make_predict_function`, which can also be overridden."
msgstr ""

#: keras.engine.training.Model.predict_step:13
#: keras.engine.training.Model.test_step:15
#: keras.engine.training.Model.train_step:16 of
msgid "A nested structure of `Tensor`s."
msgstr ""

#: keras.engine.training.Model.predict_step:15 of
msgid ""
"The result of one inference step, typically the output of calling the "
"`Model` on data."
msgstr ""

#: keras.engine.training.Model.reset_metrics:1 of
msgid "Resets the state of all the metrics in the model."
msgstr ""

#: of tensorcircuit.applications.van.MADE.run_eagerly:1
#: tensorcircuit.applications.van.NMF.run_eagerly:1
#: tensorcircuit.applications.van.PixelCNN.run_eagerly:1
msgid "Settable attribute indicating whether the model should run eagerly."
msgstr ""

#: of tensorcircuit.applications.van.MADE.run_eagerly:3
#: tensorcircuit.applications.van.NMF.run_eagerly:3
#: tensorcircuit.applications.van.PixelCNN.run_eagerly:3
msgid ""
"Running eagerly means that your model will be run step by step, like "
"Python code. Your model might run slower, but it should become easier for"
" you to debug it by stepping into individual layer calls."
msgstr ""

#: of tensorcircuit.applications.van.MADE.run_eagerly:7
#: tensorcircuit.applications.van.NMF.run_eagerly:7
#: tensorcircuit.applications.van.PixelCNN.run_eagerly:7
msgid ""
"By default, we will attempt to compile your model to a static graph to "
"deliver the best execution performance."
msgstr ""

#: of tensorcircuit.applications.van.MADE.run_eagerly:10
#: tensorcircuit.applications.van.NMF.run_eagerly:10
#: tensorcircuit.applications.van.PixelCNN.run_eagerly:10
msgid "Boolean, whether the model should run eagerly."
msgstr ""

#: keras.engine.training.Model.save:1 of
msgid "Saves the model to Tensorflow SavedModel or a single HDF5 file."
msgstr ""

#: keras.engine.training.Model.save:3 of
msgid ""
"Please see `tf.keras.models.save_model` or the [Serialization and Saving "
"guide](https://keras.io/guides/serialization_and_saving/) for details."
msgstr ""

#: keras.engine.training.Model.save:7 of
msgid "String, PathLike, path to SavedModel or H5 file to save the model."
msgstr ""

#: keras.engine.training.Model.save:9
#: keras.engine.training.Model.save_weights:47 of
msgid ""
"Whether to silently overwrite any existing file at the target location, "
"or provide the user with a manual prompt."
msgstr ""

#: keras.engine.training.Model.save:11 of
msgid "If True, save optimizer's state together."
msgstr ""

#: keras.engine.training.Model.save:12 of
msgid ""
"Either `'tf'` or `'h5'`, indicating whether to save the model to "
"Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF"
" 1.X."
msgstr ""

#: keras.engine.training.Model.save:15 of
msgid ""
"Signatures to save with the SavedModel. Applicable to the 'tf' format "
"only. Please see the `signatures` argument in `tf.saved_model.save` for "
"details."
msgstr ""

#: keras.engine.training.Model.save:18 of
msgid ""
"(only applies to SavedModel format) `tf.saved_model.SaveOptions` object "
"that specifies options for saving to SavedModel."
msgstr ""

#: keras.engine.training.Model.save:21 of
msgid ""
"(only applies to SavedModel format) When enabled, the SavedModel will "
"store the function traces for each layer. This can be disabled, so that "
"only the configs of each layer are stored. Defaults to `True`. Disabling "
"this will decrease serialization time and reduce file size, but it "
"requires that all custom layers/models implement a `get_config()` method."
msgstr ""

#: keras.engine.training.Model.save:30 of
msgid "```python from keras.models import load_model"
msgstr ""

#: keras.engine.training.Model.save:33 of
msgid ""
"model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5' del model"
"  # deletes the existing model"
msgstr ""

#: keras.engine.training.Model.save:36 of
msgid ""
"# returns a compiled model # identical to the previous one model = "
"load_model('my_model.h5') ```"
msgstr ""

#: keras.engine.training.Model.save_spec:1 of
msgid "Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`."
msgstr ""

#: keras.engine.training.Model.save_spec:3 of
msgid ""
"This value is automatically defined after calling the model for the first"
" time. Afterwards, you can use it when exporting the model for serving:"
msgstr ""

#: keras.engine.training.Model.save_spec:6 of
msgid "```python model = tf.keras.Model(...)"
msgstr ""

#: keras.engine.training.Model.save_spec:9 of
msgid "@tf.function def serve(*args, **kwargs):"
msgstr ""

#: keras.engine.training.Model.save_spec:11 of
msgid ""
"outputs = model(*args, **kwargs) # Apply postprocessing steps, or add "
"additional outputs. ... return outputs"
msgstr ""

#: keras.engine.training.Model.save_spec:16 of
msgid ""
"# arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example,"
" is # an empty dict since functional models do not use keyword arguments."
" arg_specs, kwarg_specs = model.save_spec()"
msgstr ""

#: keras.engine.training.Model.save_spec:20 of
msgid "model.save(path, signatures={"
msgstr ""

#: keras.engine.training.Model.save_spec:21 of
msgid "'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)"
msgstr ""

#: keras.engine.training.Model.save_spec:25 of
msgid ""
"Whether to set the batch sizes of all the returned `tf.TensorSpec` to "
"`None`. (Note that when defining functional or Sequential models with "
"`tf.keras.Input([...], batch_size=X)`, the batch size will always be "
"preserved). Defaults to `True`."
msgstr ""

#: keras.engine.training.Model.save_spec:30 of
msgid ""
"If the model inputs are defined, returns a tuple `(args, kwargs)`. All "
"elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs "
"are not defined, returns `None`. The model inputs are automatically set "
"when calling the model, `model.fit`, `model.evaluate` or `model.predict`."
msgstr ""

#: keras.engine.training.Model.save_weights:1 of
msgid "Saves all layer weights."
msgstr ""

#: keras.engine.training.Model.save_weights:3 of
msgid ""
"Either saves in HDF5 or in TensorFlow format based on the `save_format` "
"argument."
msgstr ""

#: keras.engine.training.Model.save_weights:14 of
msgid "When saving in HDF5 format, the weight file has:"
msgstr ""

#: keras.engine.training.Model.save_weights:7 of
msgid "`layer_names` (attribute), a list of strings"
msgstr ""

#: keras.engine.training.Model.save_weights:8 of
msgid "(ordered names of model layers)."
msgstr ""

#: keras.engine.training.Model.save_weights:14 of
msgid "For every layer, a `group` named `layer.name`"
msgstr ""

#: keras.engine.training.Model.save_weights:11 of
msgid "For every such layer group, a group attribute `weight_names`,"
msgstr ""

#: keras.engine.training.Model.save_weights:11 of
msgid "a list of strings (ordered names of weights tensor of the layer)."
msgstr ""

#: keras.engine.training.Model.save_weights:14 of
msgid "For every weight in the layer, a dataset"
msgstr ""

#: keras.engine.training.Model.save_weights:14 of
msgid "storing the weight value, named after the weight tensor."
msgstr ""

#: keras.engine.training.Model.save_weights:16 of
msgid ""
"When saving in TensorFlow format, all objects referenced by the network "
"are saved in the same format as `tf.train.Checkpoint`, including any "
"`Layer` instances or `Optimizer` instances assigned to object attributes."
" For networks constructed from inputs and outputs using "
"`tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network "
"are tracked/saved automatically. For user-defined classes which inherit "
"from `tf.keras.Model`, `Layer` instances must be assigned to object "
"attributes, typically in the constructor. See the documentation of "
"`tf.train.Checkpoint` and `tf.keras.Model` for details."
msgstr ""

#: keras.engine.training.Model.save_weights:26 of
msgid ""
"While the formats are the same, do not mix `save_weights` and "
"`tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should "
"be loaded using `Model.load_weights`. Checkpoints saved using "
"`tf.train.Checkpoint.save` should be restored using the corresponding "
"`tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over "
"`save_weights` for training checkpoints."
msgstr ""

#: keras.engine.training.Model.save_weights:33 of
msgid ""
"The TensorFlow format matches objects and variables by starting at a root"
" object, `self` for `save_weights`, and greedily matching attribute "
"names. For `Model.save` this is the `Model`, and for `Checkpoint.save` "
"this is the `Checkpoint` even if the `Checkpoint` has a model attached. "
"This means saving a `tf.keras.Model` using `save_weights` and loading "
"into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will"
" not match the `Model`'s variables. See the [guide to training "
"checkpoints](https://www.tensorflow.org/guide/checkpoint) for details on "
"the TensorFlow format."
msgstr ""

#: keras.engine.training.Model.save_weights:43 of
msgid ""
"String or PathLike, path to the file to save the weights to. When saving "
"in TensorFlow format, this is the prefix used for checkpoint files "
"(multiple files are generated). Note that the '.h5' suffix causes weights"
" to be saved in HDF5 format."
msgstr ""

#: keras.engine.training.Model.save_weights:49 of
msgid ""
"Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will "
"default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to "
"'tf'."
msgstr ""

#: keras.engine.training.Model.save_weights:52 of
msgid ""
"Optional `tf.train.CheckpointOptions` object that specifies options for "
"saving weights."
msgstr ""

#: keras.engine.training.Model.save_weights:55 of
msgid "If `h5py` is not available when attempting to save in HDF5     format."
msgstr ""

#: keras.engine.base_layer.Layer.set_weights:1 of
msgid "Sets the weights of the layer, from NumPy arrays."
msgstr ""

#: keras.engine.base_layer.Layer.set_weights:3 of
msgid ""
"The weights of a layer represent the state of the layer. This function "
"sets the weight values from numpy arrays. The weight values should be "
"passed in the order they are created by the layer. Note that the layer's "
"weights must be instantiated before calling this function, by calling the"
" layer."
msgstr ""

#: keras.engine.base_layer.Layer.get_weights:8
#: keras.engine.base_layer.Layer.set_weights:9 of
msgid ""
"For example, a `Dense` layer returns a list of two values: the kernel "
"matrix and the bias vector. These can be used to set the weights of "
"another `Dense` layer:"
msgstr ""

#: keras.engine.base_layer.Layer.set_weights:33 of
msgid ""
"a list of NumPy arrays. The number of arrays and their shape must match "
"number of the dimensions of the weights of the layer (i.e. it should "
"match the output of `get_weights`)."
msgstr ""

#: keras.engine.base_layer.Layer.set_weights:39 of
msgid ""
"If the provided weights list does not match the     layer's "
"specifications."
msgstr ""

#: of tensorcircuit.applications.van.MADE.state_updates:3
#: tensorcircuit.applications.van.NMF.state_updates:3
#: tensorcircuit.applications.van.PixelCNN.state_updates:3
msgid "Returns the `updates` from all layers that are stateful."
msgstr ""

#: of tensorcircuit.applications.van.MADE.state_updates:5
#: tensorcircuit.applications.van.NMF.state_updates:5
#: tensorcircuit.applications.van.PixelCNN.state_updates:5
msgid ""
"This is useful for separating training updates and state updates, e.g. "
"when we need to update a layer's internal state during prediction."
msgstr ""

#: of tensorcircuit.applications.van.MADE.state_updates:9
#: tensorcircuit.applications.van.NMF.state_updates:9
#: tensorcircuit.applications.van.PixelCNN.state_updates:9
msgid "A list of update ops."
msgstr ""

#: of tensorcircuit.applications.van.MADE.submodules:1
#: tensorcircuit.applications.van.MaskedConv2D.submodules:1
#: tensorcircuit.applications.van.MaskedLinear.submodules:1
#: tensorcircuit.applications.van.NMF.submodules:1
#: tensorcircuit.applications.van.PixelCNN.submodules:1
#: tensorcircuit.applications.van.ResidualBlock.submodules:1
#: tensorcircuit.applications.vqes.Linear.submodules:1
#: tensorcircuit.keras.QuantumLayer.submodules:1
msgid "Sequence of all sub-modules."
msgstr ""

#: of tensorcircuit.applications.van.MADE.submodules:3
#: tensorcircuit.applications.van.MaskedConv2D.submodules:3
#: tensorcircuit.applications.van.MaskedLinear.submodules:3
#: tensorcircuit.applications.van.NMF.submodules:3
#: tensorcircuit.applications.van.PixelCNN.submodules:3
#: tensorcircuit.applications.van.ResidualBlock.submodules:3
#: tensorcircuit.applications.vqes.Linear.submodules:3
#: tensorcircuit.keras.QuantumLayer.submodules:3
msgid ""
"Submodules are modules which are properties of this module, or found as "
"properties of modules which are properties of this module (and so on)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.submodules:18
#: tensorcircuit.applications.van.MaskedConv2D.submodules:18
#: tensorcircuit.applications.van.MaskedLinear.submodules:18
#: tensorcircuit.applications.van.NMF.submodules:18
#: tensorcircuit.applications.van.PixelCNN.submodules:18
#: tensorcircuit.applications.van.ResidualBlock.submodules:18
#: tensorcircuit.applications.vqes.Linear.submodules:18
#: tensorcircuit.keras.QuantumLayer.submodules:18
msgid "A sequence of all submodules."
msgstr ""

#: keras.engine.training.Model.summary:1 of
msgid "Prints a string summary of the network."
msgstr ""

#: keras.engine.training.Model.summary:3 of
msgid ""
"Total length of printed lines (e.g. set this to adapt the display to "
"different terminal window sizes)."
msgstr ""

#: keras.engine.training.Model.summary:6 of
msgid ""
"Relative or absolute positions of log elements in each line. If not "
"provided, defaults to `[.33, .55, .67, 1.]`."
msgstr ""

#: keras.engine.training.Model.summary:9 of
msgid ""
"Print function to use. Defaults to `print`. It will be called on each "
"line of the summary. You can set it to a custom function in order to "
"capture the string summary."
msgstr ""

#: keras.engine.training.Model.summary:13 of
msgid "Whether to expand the nested models. If not provided, defaults to `False`."
msgstr ""

#: keras.engine.training.Model.summary:16 of
msgid "if `summary()` is called before the model is built."
msgstr ""

#: of tensorcircuit.applications.van.MADE.supports_masking:1
#: tensorcircuit.applications.van.MaskedConv2D.supports_masking:1
#: tensorcircuit.applications.van.MaskedLinear.supports_masking:1
#: tensorcircuit.applications.van.NMF.supports_masking:1
#: tensorcircuit.applications.van.PixelCNN.supports_masking:1
#: tensorcircuit.applications.van.ResidualBlock.supports_masking:1
#: tensorcircuit.applications.vqes.Linear.supports_masking:1
#: tensorcircuit.keras.QuantumLayer.supports_masking:1
msgid "Whether this layer supports computing a mask using `compute_mask`."
msgstr ""

#: keras.engine.training.Model.test_on_batch:1 of
msgid "Test the model on a single batch of samples."
msgstr ""

#: keras.engine.training.Model.test_on_batch:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays (in case the     model has multiple inputs). - A TensorFlow "
"tensor, or a list of tensors (in case the model has     multiple inputs)."
" - A dict mapping input names to the corresponding array/tensors, if     "
"the model has named inputs."
msgstr ""

#: keras.engine.training.Model.test_on_batch:8 of
msgid "A dict mapping input names to the corresponding array/tensors, if"
msgstr ""

#: keras.engine.training.Model.test_on_batch:9 of
msgid "the model has named inputs."
msgstr ""

#: keras.engine.training.Model.test_on_batch:10
#: keras.engine.training.Model.train_on_batch:10 of
msgid ""
"Target data. Like the input data `x`, it could be either Numpy array(s) "
"or TensorFlow tensor(s). It should be consistent with `x` (you cannot "
"have Numpy inputs and tensor targets, or inversely)."
msgstr ""

#: keras.engine.training.Model.test_on_batch:13
#: keras.engine.training.Model.train_on_batch:13 of
msgid ""
"Optional array of the same length as x, containing weights to apply to "
"the model's loss for each sample. In the case of temporal data, you can "
"pass a 2D array with shape (samples, sequence_length), to apply a "
"different weight to every timestep of every sample."
msgstr ""

#: keras.engine.training.Model.test_on_batch:18
#: keras.engine.training.Model.train_on_batch:22 of
msgid ""
"If `True`, the metrics returned will be only for this batch. If `False`, "
"the metrics will be statefully accumulated across batches."
msgstr ""

#: keras.engine.training.Model.test_on_batch:30 of
msgid "If `model.test_on_batch` is wrapped in a `tf.function`."
msgstr ""

#: keras.engine.training.Model.test_step:1 of
msgid "The logic for one evaluation step."
msgstr ""

#: keras.engine.training.Model.test_step:3 of
msgid ""
"This method can be overridden to support custom evaluation logic. This "
"method is called by `Model.make_test_function`."
msgstr ""

#: keras.engine.training.Model.test_step:6 of
msgid ""
"This function should contain the mathematical logic for one step of "
"evaluation. This typically includes the forward pass, loss calculation, "
"and metrics updates."
msgstr ""

#: keras.engine.training.Model.test_step:11 of
msgid ""
"Configuration details for *how* this logic is run (e.g. `tf.function` and"
" `tf.distribute.Strategy` settings), should be left to "
"`Model.make_test_function`, which can also be overridden."
msgstr ""

#: keras.engine.training.Model.test_step:17 of
msgid ""
"A `dict` containing values that will be passed to "
"`tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the "
"values of the `Model`'s metrics are returned."
msgstr ""

#: keras.engine.training.Model.to_json:1 of
msgid "Returns a JSON string containing the network configuration."
msgstr ""

#: keras.engine.training.Model.to_json:3 of
msgid ""
"To load a network from a JSON save file, use "
"`keras.models.model_from_json(json_string, custom_objects={})`."
msgstr ""

#: keras.engine.training.Model.to_json:6 of
msgid "Additional keyword arguments to be passed to `json.dumps()`."
msgstr ""

#: keras.engine.training.Model.to_json:9 of
msgid "A JSON string."
msgstr ""

#: keras.engine.training.Model.to_yaml:1 of
msgid "Returns a yaml string containing the network configuration."
msgstr ""

#: keras.engine.training.Model.to_yaml:3 of
msgid ""
"Note: Since TF 2.6, this method is no longer supported and will raise a "
"RuntimeError."
msgstr ""

#: keras.engine.training.Model.to_yaml:6 of
msgid ""
"To load a network from a yaml save file, use "
"`keras.models.model_from_yaml(yaml_string, custom_objects={})`."
msgstr ""

#: keras.engine.training.Model.to_yaml:9 of
msgid ""
"`custom_objects` should be a dictionary mapping the names of custom "
"losses / layers / etc to the corresponding functions / classes."
msgstr ""

#: keras.engine.training.Model.to_yaml:13 of
msgid "Additional keyword arguments to be passed to `yaml.dump()`."
msgstr ""

#: keras.engine.training.Model.to_yaml:16 of
msgid "A YAML string."
msgstr ""

#: keras.engine.training.Model.to_yaml:18 of
msgid "announces that the method poses a security risk"
msgstr ""

#: keras.engine.training.Model.train_on_batch:1 of
msgid "Runs a single gradient update on a single batch of data."
msgstr ""

#: keras.engine.training.Model.train_on_batch:3 of
msgid ""
"Input data. It could be: - A Numpy array (or array-like), or a list of "
"arrays     (in case the model has multiple inputs). - A TensorFlow "
"tensor, or a list of tensors     (in case the model has multiple inputs)."
" - A dict mapping input names to the corresponding array/tensors,     if "
"the model has named inputs."
msgstr ""

#: keras.engine.training.Model.train_on_batch:6 of
msgid "A TensorFlow tensor, or a list of tensors"
msgstr ""

#: keras.engine.training.Model.train_on_batch:8 of
msgid "A dict mapping input names to the corresponding array/tensors,"
msgstr ""

#: keras.engine.training.Model.train_on_batch:9 of
msgid "if the model has named inputs."
msgstr ""

#: keras.engine.training.Model.train_on_batch:18 of
msgid ""
"Optional dictionary mapping class indices (integers) to a weight (float) "
"to apply to the model's loss for the samples from this class during "
"training. This can be useful to tell the model to \"pay more attention\" "
"to samples from an under-represented class."
msgstr ""

#: keras.engine.training.Model.train_on_batch:29 of
msgid ""
"Scalar training loss (if the model has a single output and no metrics) or"
" list of scalars (if the model has multiple outputs and/or metrics). The "
"attribute `model.metrics_names` will give you the display labels for the "
"scalar outputs."
msgstr ""

#: keras.engine.training.Model.train_on_batch:35 of
msgid "If `model.train_on_batch` is wrapped in a `tf.function`."
msgstr ""

#: keras.engine.training.Model.train_step:1 of
msgid "The logic for one training step."
msgstr ""

#: keras.engine.training.Model.train_step:3 of
msgid ""
"This method can be overridden to support custom training logic. For "
"concrete examples of how to override this method see [Customizing what "
"happends in "
"fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)."
" This method is called by `Model.make_train_function`."
msgstr ""

#: keras.engine.training.Model.train_step:8 of
msgid ""
"This method should contain the mathematical logic for one step of "
"training. This typically includes the forward pass, loss calculation, "
"backpropagation, and metric updates."
msgstr ""

#: keras.engine.training.Model.train_step:12 of
msgid ""
"Configuration details for *how* this logic is run (e.g. `tf.function` and"
" `tf.distribute.Strategy` settings), should be left to "
"`Model.make_train_function`, which can also be overridden."
msgstr ""

#: keras.engine.training.Model.train_step:18 of
msgid ""
"A `dict` containing values that will be passed to "
"`tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the "
"values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, "
"'accuracy': 0.7}`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.trainable_variables:1
#: tensorcircuit.applications.van.MaskedConv2D.trainable_variables:1
#: tensorcircuit.applications.van.MaskedLinear.trainable_variables:1
#: tensorcircuit.applications.van.NMF.trainable_variables:1
#: tensorcircuit.applications.van.PixelCNN.trainable_variables:1
#: tensorcircuit.applications.van.ResidualBlock.trainable_variables:1
#: tensorcircuit.applications.vqes.Linear.trainable_variables:1
#: tensorcircuit.keras.QuantumLayer.trainable_variables:1
msgid "Sequence of trainable variables owned by this module and its submodules."
msgstr ""

#: of tensorcircuit.applications.van.MADE.trainable_weights:1
#: tensorcircuit.applications.van.MaskedConv2D.trainable_weights:1
#: tensorcircuit.applications.van.MaskedLinear.trainable_weights:1
#: tensorcircuit.applications.van.NMF.trainable_weights:1
#: tensorcircuit.applications.van.PixelCNN.trainable_weights:1
#: tensorcircuit.applications.van.ResidualBlock.trainable_weights:1
#: tensorcircuit.applications.vqes.Linear.trainable_weights:1
#: tensorcircuit.keras.QuantumLayer.trainable_weights:1
msgid "List of all trainable weights tracked by this layer."
msgstr ""

#: of tensorcircuit.applications.van.MADE.trainable_weights:3
#: tensorcircuit.applications.van.MaskedConv2D.trainable_weights:3
#: tensorcircuit.applications.van.MaskedLinear.trainable_weights:3
#: tensorcircuit.applications.van.NMF.trainable_weights:3
#: tensorcircuit.applications.van.PixelCNN.trainable_weights:3
#: tensorcircuit.applications.van.ResidualBlock.trainable_weights:3
#: tensorcircuit.applications.vqes.Linear.trainable_weights:3
#: tensorcircuit.keras.QuantumLayer.trainable_weights:3
msgid "Trainable weights are updated via gradient descent during training."
msgstr ""

#: of tensorcircuit.applications.van.MADE.trainable_weights:5
#: tensorcircuit.applications.van.MaskedConv2D.trainable_weights:5
#: tensorcircuit.applications.van.MaskedLinear.trainable_weights:5
#: tensorcircuit.applications.van.NMF.trainable_weights:5
#: tensorcircuit.applications.van.PixelCNN.trainable_weights:5
#: tensorcircuit.applications.van.ResidualBlock.trainable_weights:5
#: tensorcircuit.applications.vqes.Linear.trainable_weights:5
#: tensorcircuit.keras.QuantumLayer.trainable_weights:5
msgid "A list of trainable variables."
msgstr ""

#: of tensorcircuit.applications.van.MADE.variable_dtype:1
#: tensorcircuit.applications.van.MaskedConv2D.variable_dtype:1
#: tensorcircuit.applications.van.MaskedLinear.variable_dtype:1
#: tensorcircuit.applications.van.NMF.variable_dtype:1
#: tensorcircuit.applications.van.PixelCNN.variable_dtype:1
#: tensorcircuit.applications.van.ResidualBlock.variable_dtype:1
#: tensorcircuit.applications.vqes.Linear.variable_dtype:1
#: tensorcircuit.keras.QuantumLayer.variable_dtype:1
msgid "Alias of `Layer.dtype`, the dtype of the weights."
msgstr ""

#: of tensorcircuit.applications.van.MADE.variables:1
#: tensorcircuit.applications.van.MADE.weights:1
#: tensorcircuit.applications.van.MaskedConv2D.variables:1
#: tensorcircuit.applications.van.MaskedConv2D.weights:1
#: tensorcircuit.applications.van.MaskedLinear.variables:1
#: tensorcircuit.applications.van.MaskedLinear.weights:1
#: tensorcircuit.applications.van.NMF.variables:1
#: tensorcircuit.applications.van.NMF.weights:1
#: tensorcircuit.applications.van.PixelCNN.variables:1
#: tensorcircuit.applications.van.PixelCNN.weights:1
#: tensorcircuit.applications.van.ResidualBlock.variables:1
#: tensorcircuit.applications.van.ResidualBlock.weights:1
#: tensorcircuit.applications.vqes.Linear.variables:1
#: tensorcircuit.applications.vqes.Linear.weights:1
#: tensorcircuit.keras.QuantumLayer.variables:1
#: tensorcircuit.keras.QuantumLayer.weights:1
msgid "Returns the list of all layer variables/weights."
msgstr ""

#: of tensorcircuit.applications.van.MADE.variables:3
#: tensorcircuit.applications.van.MaskedConv2D.variables:3
#: tensorcircuit.applications.van.MaskedLinear.variables:3
#: tensorcircuit.applications.van.NMF.variables:3
#: tensorcircuit.applications.van.PixelCNN.variables:3
#: tensorcircuit.applications.van.ResidualBlock.variables:3
#: tensorcircuit.applications.vqes.Linear.variables:3
#: tensorcircuit.keras.QuantumLayer.variables:3
msgid "Alias of `self.weights`."
msgstr ""

#: of tensorcircuit.applications.van.MADE.variables:5
#: tensorcircuit.applications.van.MADE.weights:3
#: tensorcircuit.applications.van.MaskedConv2D.variables:5
#: tensorcircuit.applications.van.MaskedLinear.variables:5
#: tensorcircuit.applications.van.NMF.variables:5
#: tensorcircuit.applications.van.NMF.weights:3
#: tensorcircuit.applications.van.PixelCNN.variables:5
#: tensorcircuit.applications.van.PixelCNN.weights:3
#: tensorcircuit.applications.van.ResidualBlock.variables:5
#: tensorcircuit.applications.vqes.Linear.variables:5
#: tensorcircuit.keras.QuantumLayer.variables:5
msgid ""
"Note: This will not track the weights of nested `tf.Modules` that are not"
" themselves Keras layers."
msgstr ""

#: of tensorcircuit.applications.van.MADE.variables:8
#: tensorcircuit.applications.van.MADE.weights:6
#: tensorcircuit.applications.van.MaskedConv2D.variables:8
#: tensorcircuit.applications.van.MaskedConv2D.weights:3
#: tensorcircuit.applications.van.MaskedLinear.variables:8
#: tensorcircuit.applications.van.MaskedLinear.weights:3
#: tensorcircuit.applications.van.NMF.variables:8
#: tensorcircuit.applications.van.NMF.weights:6
#: tensorcircuit.applications.van.PixelCNN.variables:8
#: tensorcircuit.applications.van.PixelCNN.weights:6
#: tensorcircuit.applications.van.ResidualBlock.variables:8
#: tensorcircuit.applications.van.ResidualBlock.weights:3
#: tensorcircuit.applications.vqes.Linear.variables:8
#: tensorcircuit.applications.vqes.Linear.weights:3
#: tensorcircuit.keras.QuantumLayer.variables:8
#: tensorcircuit.keras.QuantumLayer.weights:3
msgid "A list of variables."
msgstr ""

#: of tensorflow.python.module.module.Module.with_name_scope:1
msgid "Decorator to automatically enter the module name scope."
msgstr ""

#: of tensorflow.python.module.module.Module.with_name_scope:10
msgid ""
"Using the above module would produce `tf.Variable`s and `tf.Tensor`s "
"whose names included the module name:"
msgstr ""

#: of tensorflow.python.module.module.Module.with_name_scope:20
msgid "The method to wrap."
msgstr ""

#: of tensorflow.python.module.module.Module.with_name_scope:22
msgid "The original method wrapped such that it enters the module's name scope."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D:1
#: tensorcircuit.applications.van.MaskedLinear:1
#: tensorcircuit.applications.van.ResidualBlock:1
#: tensorcircuit.applications.vqes.Linear:1 tensorcircuit.keras.QuantumLayer:1
msgid "Bases: :py:class:`keras.engine.base_layer.Layer`"
msgstr ""

#: keras.engine.base_layer.Layer.build:1 of
#: tensorcircuit.applications.van.MaskedConv2D.build:1
#: tensorcircuit.keras.QuantumLayer.build:1
msgid "Creates the variables of the layer (optional, for subclass implementers)."
msgstr ""

#: keras.engine.base_layer.Layer.build:3 of
#: tensorcircuit.applications.van.MaskedConv2D.build:3
#: tensorcircuit.keras.QuantumLayer.build:3
msgid ""
"This is a method that implementers of subclasses of `Layer` or `Model` "
"can override if they need a state-creation step in-between layer "
"instantiation and layer call."
msgstr ""

#: keras.engine.base_layer.Layer.build:7 of
#: tensorcircuit.applications.van.MaskedConv2D.build:7
#: tensorcircuit.keras.QuantumLayer.build:7
msgid "This is typically used to create the weights of `Layer` subclasses."
msgstr ""

#: keras.engine.base_layer.Layer.build:9 of
#: tensorcircuit.applications.van.MaskedConv2D.build:9
#: tensorcircuit.keras.QuantumLayer.build:9
msgid ""
"Instance of `TensorShape`, or list of instances of `TensorShape` if the "
"layer expects a list of inputs (one instance per input)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:1
#: tensorcircuit.applications.van.MaskedLinear.call:1
#: tensorcircuit.applications.van.ResidualBlock.call:1
#: tensorcircuit.applications.vqes.Linear.call:1
msgid "This is where the layer's logic lives."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:3
#: tensorcircuit.applications.van.MaskedLinear.call:3
#: tensorcircuit.applications.van.ResidualBlock.call:3
#: tensorcircuit.applications.vqes.Linear.call:3
msgid ""
"Note here that `call()` method in `tf.keras` is little bit different from"
" `keras` API. In `keras` API, you can pass support masking for layers as "
"additional arguments. Whereas `tf.keras` has `compute_mask()` method to "
"support masking."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:8
#: tensorcircuit.applications.van.MaskedLinear.call:8
#: tensorcircuit.applications.van.ResidualBlock.call:8
#: tensorcircuit.applications.vqes.Linear.call:8
msgid ""
"Input tensor, or dict/list/tuple of input tensors. The first positional "
"`inputs` argument is subject to special rules: - `inputs` must be "
"explicitly passed. A layer cannot have zero   arguments, and `inputs` "
"cannot be provided via the default value   of a keyword argument. - NumPy"
" array or Python scalar values in `inputs` get cast as tensors. - Keras "
"mask metadata is only collected from `inputs`. - Layers are built "
"(`build(input_shape)` method)   using shape info from `inputs` only. - "
"`input_spec` compatibility is only checked against `inputs`. - Mixed "
"precision input casting is only applied to `inputs`.   If a layer has "
"tensor arguments in `*args` or `**kwargs`, their   casting behavior in "
"mixed precision should be handled manually. - The SavedModel input "
"specification is generated using `inputs` only. - Integration with "
"various ecosystem packages like TFMOT, TFLite,   TF.js, etc is only "
"supported for `inputs` and not for tensors in   positional and keyword "
"arguments."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:8
#: tensorcircuit.applications.van.MaskedLinear.call:8
#: tensorcircuit.applications.van.ResidualBlock.call:8
#: tensorcircuit.applications.vqes.Linear.call:8
msgid ""
"Input tensor, or dict/list/tuple of input tensors. The first positional "
"`inputs` argument is subject to special rules: - `inputs` must be "
"explicitly passed. A layer cannot have zero"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:11
#: tensorcircuit.applications.van.MaskedLinear.call:11
#: tensorcircuit.applications.van.ResidualBlock.call:11
#: tensorcircuit.applications.vqes.Linear.call:11
msgid ""
"arguments, and `inputs` cannot be provided via the default value of a "
"keyword argument."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:13
#: tensorcircuit.applications.van.MaskedLinear.call:13
#: tensorcircuit.applications.van.ResidualBlock.call:13
#: tensorcircuit.applications.vqes.Linear.call:13
msgid "NumPy array or Python scalar values in `inputs` get cast as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:14
#: tensorcircuit.applications.van.MaskedLinear.call:14
#: tensorcircuit.applications.van.ResidualBlock.call:14
#: tensorcircuit.applications.vqes.Linear.call:14
msgid "Keras mask metadata is only collected from `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:15
#: tensorcircuit.applications.van.MaskedLinear.call:15
#: tensorcircuit.applications.van.ResidualBlock.call:15
#: tensorcircuit.applications.vqes.Linear.call:15
msgid ""
"Layers are built (`build(input_shape)` method) using shape info from "
"`inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:17
#: tensorcircuit.applications.van.MaskedLinear.call:17
#: tensorcircuit.applications.van.ResidualBlock.call:17
#: tensorcircuit.applications.vqes.Linear.call:17
msgid "`input_spec` compatibility is only checked against `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:18
#: tensorcircuit.applications.van.MaskedLinear.call:18
#: tensorcircuit.applications.van.ResidualBlock.call:18
#: tensorcircuit.applications.vqes.Linear.call:18
msgid ""
"Mixed precision input casting is only applied to `inputs`. If a layer has"
" tensor arguments in `*args` or `**kwargs`, their casting behavior in "
"mixed precision should be handled manually."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:21
#: tensorcircuit.applications.van.MaskedLinear.call:21
#: tensorcircuit.applications.van.ResidualBlock.call:21
#: tensorcircuit.applications.vqes.Linear.call:21
msgid "The SavedModel input specification is generated using `inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:22
#: tensorcircuit.applications.van.MaskedLinear.call:22
#: tensorcircuit.applications.van.ResidualBlock.call:22
#: tensorcircuit.applications.vqes.Linear.call:22
msgid ""
"Integration with various ecosystem packages like TFMOT, TFLite, TF.js, "
"etc is only supported for `inputs` and not for tensors in positional and "
"keyword arguments."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:25
#: tensorcircuit.applications.van.MaskedLinear.call:25
#: tensorcircuit.applications.van.ResidualBlock.call:25
#: tensorcircuit.applications.vqes.Linear.call:25
msgid ""
"Additional positional arguments. May contain tensors, although this is "
"not recommended, for the reasons above."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:27
#: tensorcircuit.applications.van.MaskedLinear.call:27
#: tensorcircuit.applications.van.ResidualBlock.call:27
#: tensorcircuit.applications.vqes.Linear.call:27
msgid ""
"Additional keyword arguments. May contain tensors, although this is not "
"recommended, for the reasons above. The following optional keyword "
"arguments are reserved: - `training`: Boolean scalar tensor of Python "
"boolean indicating   whether the `call` is meant for training or "
"inference. - `mask`: Boolean input mask. If the layer's `call()` method "
"takes a   `mask` argument, its default value will be set to the mask "
"generated   for `inputs` by the previous layer (if `input` did come from "
"a layer   that generated a corresponding mask, i.e. if it came from a "
"Keras   layer with masking support)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:27
#: tensorcircuit.applications.van.MaskedLinear.call:27
#: tensorcircuit.applications.van.ResidualBlock.call:27
#: tensorcircuit.applications.vqes.Linear.call:27
msgid ""
"Additional keyword arguments. May contain tensors, although this is not "
"recommended, for the reasons above. The following optional keyword "
"arguments are reserved: - `training`: Boolean scalar tensor of Python "
"boolean indicating"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:31
#: tensorcircuit.applications.van.MaskedLinear.call:31
#: tensorcircuit.applications.van.ResidualBlock.call:31
#: tensorcircuit.applications.vqes.Linear.call:31
msgid "whether the `call` is meant for training or inference."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:32
#: tensorcircuit.applications.van.MaskedLinear.call:32
#: tensorcircuit.applications.van.ResidualBlock.call:32
#: tensorcircuit.applications.vqes.Linear.call:32
msgid ""
"`mask`: Boolean input mask. If the layer's `call()` method takes a `mask`"
" argument, its default value will be set to the mask generated for "
"`inputs` by the previous layer (if `input` did come from a layer that "
"generated a corresponding mask, i.e. if it came from a Keras layer with "
"masking support)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:38
#: tensorcircuit.applications.van.MaskedLinear.call:38
#: tensorcircuit.applications.van.ResidualBlock.call:38
#: tensorcircuit.applications.vqes.Linear.call:38
msgid "A tensor or list/tuple of tensors."
msgstr ""

#: keras.engine.base_layer.Layer.get_weights:1 of
msgid "Returns the current weights of the layer, as NumPy arrays."
msgstr ""

#: keras.engine.base_layer.Layer.get_weights:3 of
msgid ""
"The weights of a layer represent the state of the layer. This function "
"returns both trainable and non-trainable weight values associated with "
"this layer as a list of NumPy arrays, which can in turn be used to load "
"state into similarly parameterized layers."
msgstr ""

#: keras.engine.base_layer.Layer.get_weights:32 of
msgid "Weights values as a list of NumPy arrays."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.metrics:1
#: tensorcircuit.applications.van.MaskedLinear.metrics:1
#: tensorcircuit.applications.van.ResidualBlock.metrics:1
#: tensorcircuit.applications.vqes.Linear.metrics:1
#: tensorcircuit.keras.QuantumLayer.metrics:1
msgid "List of metrics added using the `add_metric()` API."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.metrics:13
#: tensorcircuit.applications.van.MaskedLinear.metrics:13
#: tensorcircuit.applications.van.ResidualBlock.metrics:13
#: tensorcircuit.applications.vqes.Linear.metrics:13
#: tensorcircuit.keras.QuantumLayer.metrics:13
msgid "A list of `Metric` objects."
msgstr ""

#: ../../source/api/applications/vqes.rst:2
msgid "tensorcircuit.applications.vqes"
msgstr ""

#: of tensorcircuit.applications.vqes:1
msgid "VQNHE application"
msgstr ""

#: of tensorcircuit.applications.vqes.JointSchedule:1
msgid ""
"Bases: "
":py:class:`keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule`"
msgstr ""

#: keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule.from_config:1
#: of
msgid "Instantiates a `LearningRateSchedule` from its config."
msgstr ""

#: keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule.from_config:3
#: of
msgid "Output of `get_config()`."
msgstr ""

#: keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule.from_config:5
#: of
msgid "A `LearningRateSchedule` instance."
msgstr ""

#: of tensorcircuit.applications.vqes.Linear:1
msgid "Dense layer but with complex weights, used for building complex RBM"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:1
msgid "VQNHE"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.evaluation:5
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:11
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:11
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:11
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split:6
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split:8
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter:7
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter:9
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:11
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:15
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:13
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:7
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:7
#: tensorcircuit.backends.jax_backend.JaxBackend.concat:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:5
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:7
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:9
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:8
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:11
#: tensorcircuit.backends.jax_backend.JaxBackend.max:3
#: tensorcircuit.backends.jax_backend.JaxBackend.max:7
#: tensorcircuit.backends.jax_backend.JaxBackend.min:3
#: tensorcircuit.backends.jax_backend.JaxBackend.min:7
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:6
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:8
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:3
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:5
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:7
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:9
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:15
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:3
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:7
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:3
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:36
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:53
#: tensorcircuit.quantum.count_d2s:10 tensorcircuit.quantum.count_d2s:14
#: tensorcircuit.quantum.count_s2d:4 tensorcircuit.quantum.count_s2d:8
#: tensorcircuit.simplify.pseudo_contract_between:3
#: tensorcircuit.simplify.pseudo_contract_between:5
#: tensorcircuit.simplify.pseudo_contract_between:7
#: tensorcircuit.templates.graphs.Line1D:3
#: tensorcircuit.templates.graphs.Line1D:7
msgid "[description]"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.plain_evaluation:1
msgid "VQE"
msgstr ""

#: ../../source/api/backends.rst:2
msgid "tensorcircuit.backends"
msgstr ""

#: ../../source/api/backends/backend_factory.rst:2
msgid "tensorcircuit.backends.backend_factory"
msgstr ""

#: of tensorcircuit.backends.backend_factory:1
msgid "Backend register"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:1
msgid "Get the `tc.backend` object."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:5
msgid "Backend doesn't exist for `backend` argument."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:6
#: tensorcircuit.cons.set_tensornetwork_backend:32
msgid "The `tc.backend` object that with all registered universal functions."
msgstr ""

#: ../../source/api/backends/jax_backend.rst:2
msgid "tensorcircuit.backends.jax_backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend:1
msgid "Backend magic inherited from tensornetwork: jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid ""
"Bases: :py:class:`tensornetwork.backends.jax.jax_backend.JaxBackend`, "
":py:class:`tensorcircuit.backends.abstract_backend.ExtendedBackend`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid ""
"See the original backend API at `jax backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.abs:1
msgid ""
"Returns the elementwise absolute value of tensor. :param tensor: An input"
" tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:4
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.abs:4
msgid "Its elementwise absolute value."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acos:1
#: tensorcircuit.backends.jax_backend.JaxBackend.asin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:1
msgid "Return the acos of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.acos:3
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.asin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.atan:3
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2:3
#: tensorcircuit.backends.jax_backend.JaxBackend.atanh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.copy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:5
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tan:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:3
msgid "tensor in matrix form"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acos:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:5
msgid "acos of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acosh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:1
msgid "Return the acosh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acosh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:5
msgid "acosh of ``a``"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.addition:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.addition:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.addition:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.addition:1
msgid ""
"Return the default addition of `tensor`. A backend can override such "
"implementation. :param tensor1: A tensor. :param tensor2: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:4
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:4
#: tensorcircuit.backends.jax_backend.JaxBackend.sin:4
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:9
#: tensorcircuit.backends.pytorch_backend._conj_torch:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:9
#: tensorcircuit.experimental.hamiltonian_evol:12
#: tensornetwork.backends.abstract_backend.AbstractBackend.exp:4
#: tensornetwork.backends.abstract_backend.AbstractBackend.log:4
#: tensornetwork.backends.jax.jax_backend.JaxBackend.addition:6
#: tensornetwork.backends.jax.jax_backend.JaxBackend.conj:4
#: tensornetwork.backends.jax.jax_backend.JaxBackend.divide:6
#: tensornetwork.backends.jax.jax_backend.JaxBackend.exp:4
#: tensornetwork.backends.jax.jax_backend.JaxBackend.log:4
#: tensornetwork.backends.jax.jax_backend.JaxBackend.multiply:7
#: tensornetwork.backends.jax.jax_backend.JaxBackend.subtraction:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.addition:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.conj:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.divide:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.exp:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.log:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.multiply:7
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.subtraction:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.addition:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.divide:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.multiply:7
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.subtraction:6
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.addition:6
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.conj:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.divide:6
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.exp:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.log:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.multiply:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.subtraction:6
msgid "Tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint:1
msgid "Return the conjugate and transpose of a tensor ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem:3
#: tensorcircuit.backends.jax_backend.JaxBackend.relu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:9
msgid "Input tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.adjoint:5
msgid "adjoint tensor of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.arange:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange:1
msgid "Values are generated within the half-open interval [start, stop)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.arange:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange:3
msgid "start index"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.arange:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange:5
msgid "end index, defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.arange:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.arange:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.arange:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.arange:7
msgid "steps, defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:1
msgid "Return the index of maximum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:5
msgid "[description], defaults to 0, different behavior from numpy defaults!"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:1
msgid "Return the index of minimum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:5
msgid "asin of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asinh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:1
msgid "Return the asinh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asinh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:5
msgid "asinh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:1
msgid "Return the atan of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:5
msgid "atan of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan2:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:1
msgid "Return the atan of a tensor ``y``/``x``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan2:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:5
msgid "atan2 of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atanh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh:1
msgid "Return the atanh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atanh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atanh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atanh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atanh:5
msgid "atanh of ``a``"
msgstr ""

#: of
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_left_multiplication:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_left_multiplication:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_left_multiplication:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_left_multiplication:1
msgid ""
"Perform broadcasting for multiplication of `tensor1` onto `tensor2`, i.e."
" `tensor1` * tensor2`, where `tensor2` is an arbitrary tensor and "
"`tensor1` is a one-dimensional tensor. The broadcasting is applied to the"
" first index of `tensor2`. :param tensor1: A tensor. :param tensor2: A "
"tensor."
msgstr ""

#: of
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_left_multiplication:8
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_right_multiplication:8
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_left_multiplication:8
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_right_multiplication:8
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_left_multiplication:8
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_right_multiplication:8
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_left_multiplication:8
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_right_multiplication:8
msgid "The result of multiplying `tensor1` onto `tensor2`."
msgstr ""

#: of
#: tensornetwork.backends.jax.jax_backend.JaxBackend.broadcast_right_multiplication:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.broadcast_right_multiplication:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.broadcast_right_multiplication:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.broadcast_right_multiplication:1
msgid ""
"Perform broadcasting for multiplication of `tensor2` onto `tensor1`, i.e."
" `tensor1` * tensor2`, where `tensor1` is an arbitrary tensor and "
"`tensor2` is a one-dimensional tensor. The broadcasting is applied to the"
" last index of `tensor1`. :param tensor1: A tensor. :param tensor2: A "
"tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:1
msgid "Cast the tensor dtype of a ``a``."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cast:3
#: tensorcircuit.backends.jax_backend.JaxBackend.imag:3
#: tensorcircuit.backends.jax_backend.JaxBackend.real:3
#: tensorcircuit.backends.jax_backend.JaxBackend.size:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:3
msgid "tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:5
msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:7
msgid "``a`` of new dtype"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:9
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.concat:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:31
msgid "[description], defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cond:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:1
msgid ""
"The native cond for XLA compiling, wrapper for ``tf.cond`` and limited "
"functionality of ``jax.lax.cond``."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._conj_torch:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.conj:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.conj:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.conj:1
msgid "Return the complex conjugate of `tensor` :param tensor: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.convert_to_tensor:1
#: tensorcircuit.backends.numpy_backend._convert_to_tensor_numpy:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.convert_to_tensor:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.convert_to_tensor:1
msgid "Convert a np.array or a tensor to a tensor type for the backend."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:1
msgid ""
"Generate the coo format sparse matrix from indices and values, which is "
"the only sparse format supported in different ML backends."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:4
msgid "shape [n, 2] for n non zero values in the returned matrix"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:6
msgid "shape [n]"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:8
msgid "Tuple[int, ...]"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy:1
msgid "Generate the coo format sparse matrix from scipy coo sparse matrix."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy:3
msgid "Scipy coo format sparse matrix"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.coo_sparse_matrix_from_numpy:5
msgid "SparseTensor in backend format"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:1
msgid "Return the copy of ``a``, matrix exponential."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:5
msgid "matrix exponential of matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:1
msgid "Return cos of `tensor`. :param tensor: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cosh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:1
msgid "Return the cosh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cosh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:5
msgid "cosh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:1
msgid "Return the cumulative sum of the elements along a given axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:5
msgid ""
"The default behavior is the same as numpy, different from tf/torch as "
"cumsum of the flatten 1D array, defaults to None"
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.deserialize_tensor:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.deserialize_tensor:1
msgid "Return a tensor given a serialized tensor string."
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.deserialize_tensor:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.deserialize_tensor:3
msgid "The input string representing a serialized tensor."
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.deserialize_tensor:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.deserialize_tensor:5
msgid "The tensor object represented by the string."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device:1
msgid "get the universal device str for the tensor, in the format of tf"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device:3
#: tensorcircuit.backends.jax_backend.JaxBackend.device_move:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move:3
#: tensorcircuit.interfaces.tensortrans.which_backend:3
msgid "the tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device:5
msgid "device str where the tensor lives on"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device_move:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move:1
msgid "move tensor ``a`` to device ``dev``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device_move:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move:5
msgid "device str or device obj in corresponding backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.device_move:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.device_move:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.device_move:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.device_move:7
msgid "the tensor on new device"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagflat:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagflat:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagflat:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagflat:1
msgid ""
"Flattens tensor and creates a new matrix of zeros with its elements on "
"the k'th diagonal. :param tensor: A tensor. :param k: The diagonal upon "
"which to place its elements."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagflat:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagflat:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagflat:6
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagflat:6
msgid "A new tensor with all zeros save the specified diagonal."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:1
msgid "Return specified diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:3
msgid ""
"If tensor is 2-D, returns the diagonal of tensor with the given offset, "
"i.e., the collection of elements of the form a[i, i+offset]. If a has "
"more than two dimensions, then the axes specified by axis1 and axis2 are "
"used to determine the 2-D sub-array whose diagonal is returned. The shape"
" of the resulting array can be determined by removing axis1 and axis2 and"
" appending an index to the right equal to the size of the resulting "
"diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:11
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:11
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:11
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:11
msgid ""
"This function only extracts diagonals. If you wish to create diagonal "
"matrices from vectors, use diagflat."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._tensordot_tf:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:14
#: tensornetwork.backends.jax.jax_backend.JaxBackend.reshape:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tensor:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tuple:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.slice:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.tensordot:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace:10
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:14
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.reshape:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tensor:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tuple:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.slice:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.tensordot:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:10
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:14
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.reshape:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tensor:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tuple:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.slice:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.tensordot:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:13
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:14
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.reshape:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tensor:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tuple:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.slice:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:10
msgid "A tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:15
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace:11
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:11
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:15
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:15
msgid "Offset of the diagonal from the main diagonal."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:16
#: tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:19
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace:12
#: tensornetwork.backends.jax.jax_backend.JaxBackend.trace:15
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to second last/last axis."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:23
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:23
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:25
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:33
msgid ""
"A dim = min(1, tensor.ndim - 2) tensor storing                     the "
"batched diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:25
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:25
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:27
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:35
msgid "A dim = min(1, tensor.ndim - 2) tensor storing"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.diagonal:26
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:26
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:28
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:36
msgid "the batched diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.divide:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.divide:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.divide:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.divide:1
msgid ""
"Return the default divide of `tensor`. A backend can override such "
"implementation. :param tensor1: A tensor. :param tensor2: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.dtype:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype:1
msgid "Obtain dtype string for tensor ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.dtype:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype:3
msgid "The tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.dtype:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.dtype:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.dtype:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.dtype:5
msgid "dtype str, such as \"complex64\""
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigh:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh:1
msgid "Compute eigenvectors and eigenvalues of a hermitian matrix."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigh:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh:3
msgid "A symetric matrix."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigh:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigh:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigh:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eigh:5
msgid "The eigenvalues in ascending order. Tensor: The eigenvectors."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:1
msgid ""
"Implicitly restarted Arnoldi method for finding the lowest eigenvector-"
"eigenvalue pairs of a linear operator `A`. `A` is a function implementing"
" the matrix-vector product."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:6
msgid ""
"WARNING: This routine uses jax.jit to reduce runtimes. jitting is "
"triggered at the first invocation of `eigs`, and on any subsequent calls "
"if the python `id` of `A` changes, even if the formal definition of `A` "
"stays the same. Example: the following will jit once at the beginning, "
"and then never again:"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:12
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:12
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:10
msgid "```python import jax import numpy as np def A(H,x):"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:16
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:31
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:16
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:31
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:14
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:29
msgid "return jax.np.dot(H,x)"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:19
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:19
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:17
msgid "for n in range(100):"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:18
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigs(A, [H],x) #jitting is "
"triggerd only at `n=0`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:23
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:23
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:21
msgid ""
"The following code triggers jitting at every iteration, which results in "
"considerably reduced performance"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:26
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:26
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:24
msgid "```python import jax import numpy as np for n in range(100):"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:30
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:30
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:28
msgid "def A(H,x):"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:32
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigs(A, [H],x) #jitting is "
"triggerd at every step `n`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:37
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:37
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:35
msgid ""
"A (sparse) implementation of a linear operator. Call signature of `A` is "
"`res = A(vector, *args)`, where `vector` can be an arbitrary `Tensor`, "
"and `res.shape` has to be `vector.shape`."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:6
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:40
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:40
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:38
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:9
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:6
msgid ""
"A list of arguments to `A`.  `A` will be called as `res = "
"A(initial_state, *args)`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:42
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:42
msgid ""
"An initial vector for the algorithm. If `None`, a random initial `Tensor`"
" is created using the `backend.randn` method"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:12
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:12
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:10
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:44
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:44
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:42
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:14
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:12
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:10
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:10
msgid "The shape of the input-dimension of `A`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:45
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:45
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:43
msgid ""
"The dtype of the input `A`. If no `initial_state` is provided, a random "
"initial state with shape `shape` and dtype `dtype` is created."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:15
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:15
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:13
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:47
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:47
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:45
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:17
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:13
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:13
msgid "The number of iterations (number of krylov vectors)."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:48
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:48
msgid "The number of eigenvector-eigenvalue pairs to be computed."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:49
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:49
msgid ""
"The desired precision of the eigenvalues. For the jax backend this has "
"currently no effect, and precision of eigenvalues is not guaranteed. This"
" feature may be added at a later point. To increase precision the caller "
"can either increase `maxiter` or `num_krylov_vecs`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:53
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:53
msgid ""
"Flag for targetting different types of eigenvalues. Currently supported "
"are `which = 'LR'` (larges real part) and `which = 'LM'` (larges "
"magnitude)."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:56
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:56
msgid ""
"Maximum number of restarts. For `maxiter=0` the routine becomes "
"equivalent to a simple Arnoldi method."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:59
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:59
msgid ""
"(eigvals, eigvecs)  eigvals: A list of `numeig` eigenvalues  eigvecs: A "
"list of `numeig` eigenvectors"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:33
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:62
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:62
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:66
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:33
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:33
msgid "(eigvals, eigvecs)"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigs:62
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:62
msgid ""
"eigvals: A list of `numeig` eigenvalues eigvecs: A list of `numeig` "
"eigenvectors"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:1
msgid ""
"Implicitly restarted Lanczos method for finding the lowest eigenvector-"
"eigenvalue pairs of a symmetric (hermitian) linear operator `A`. `A` is a"
" function implementing the matrix-vector product."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:6
msgid ""
"WARNING: This routine uses jax.jit to reduce runtimes. jitting is "
"triggered at the first invocation of `eigsh`, and on any subsequent calls"
" if the python `id` of `A` changes, even if the formal definition of `A` "
"stays the same. Example: the following will jit once at the beginning, "
"and then never again:"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:18
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigsh(A, [H],x) #jitting is "
"triggerd only at `n=0`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh:32
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigsh(A, [H],x) #jitting is "
"triggerd at every step `n`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:1
msgid ""
"Lanczos method for finding the lowest eigenvector-eigenvalue pairs of a "
"hermitian linear operator `A`. `A` is a function implementing the matrix-"
"vector product. WARNING: This routine uses jax.jit to reduce runtimes. "
"jitting is triggered at the first invocation of `eigsh_lanczos`, and on "
"any subsequent calls if the python `id` of `A` changes, even if the "
"formal definition of `A` stays the same. Example: the following will jit "
"once at the beginning, and then never again:"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:16
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigsh_lanczos(A, [H],x) "
"#jitting is triggerd only at `n=0`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:30
msgid ""
"H = jax.np.array(np.random.rand(10,10)) x = "
"jax.np.array(np.random.rand(10,10)) res = eigsh_lanczos(A, [H],x) "
"#jitting is triggerd at every step `n`"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:8
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:40
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:8
msgid ""
"An initial vector for the Lanczos algorithm. If `None`, a random initial "
"`Tensor` is created using the `backend.randn` method"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:46
msgid ""
"The number of eigenvector-eigenvalue pairs to be computed. If `numeig > "
"1`, `reorthogonalize` has to be `True`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:48
msgid ""
"The desired precision of the eigenvalues. For the jax backend this has "
"currently no effect, and precision of eigenvalues is not guaranteed. This"
" feature may be added at a later point. To increase precision the caller "
"can increase `num_krylov_vecs`."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:20
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:52
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:20
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:20
msgid ""
"Stopping criterion for Lanczos iteration. If a Krylov vector :math: `x_n`"
" has an L2 norm :math:`\\lVert x_n\\rVert < delta`, the iteration is "
"stopped. It means that an (approximate) invariant subspace has been "
"found."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:57
msgid ""
"The tridiagonal Operator is diagonalized every `ndiag` iterations to "
"check convergence. This has currently no effect for the jax backend, but "
"may be added at a later point."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:27
#: tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:60
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:27
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:27
msgid ""
"If `True`, Krylov vectors are kept orthogonal by explicit "
"orthogonalization (more costly than `reorthogonalize=False`)"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:63
msgid ""
"(eigvals, eigvecs)  eigvals: A jax-array containing `numeig` lowest "
"eigenvalues  eigvecs: A list of `numeig` lowest eigenvectors"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eigsh_lanczos:66
msgid ""
"eigvals: A jax-array containing `numeig` lowest eigenvalues eigvecs: A "
"list of `numeig` lowest eigenvectors"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:1
msgid "Get the eigenvalues of matrix ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:5
msgid "eigenvalues of ``a``"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.einsum:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.einsum:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.einsum:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.einsum:1
msgid "Calculate sum of products of tensors according to expression."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eps:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps:1
msgid "Return machine epsilon for given `dtype`"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eps:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps:3
msgid "A dtype."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.eps:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eps:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eps:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.eps:5
msgid "Machine epsilon."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.exp:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.exp:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.exp:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.exp:1
msgid "Return elementwise exp of `tensor`. :param tensor: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.expm:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:1
msgid "Return expm log of `matrix`, matrix exponential. :param matrix: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:4
msgid "Return an identity matrix of dimension `dim`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:2
msgid ""
"Depending on specific backends, `dim` has to be either an int (numpy, "
"torch, tensorflow) or a `ShapeType` object (for block-sparse backends). "
"Block-sparse behavior is currently not supported"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:6
#: tensorcircuit.backends.jax_backend.JaxBackend.eye:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:9
msgid "The dimension of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:8
msgid "The dtype of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack:1
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack:1
msgid "Transform a dlpack capsule to a tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.from_dlpack:3
#: tensorcircuit.backends.jax_backend.JaxBackend.from_dlpack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.from_dlpack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.from_dlpack:3
msgid "the dlpack capsule"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:1
msgid ""
"Return ``operand[indices]``, both ``operand`` and ``indices`` are rank-1 "
"tensor."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:3
msgid "rank-1 tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:5
msgid "rank-1 tensor with int dtype"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.gather1d:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:7
msgid "``operand[indices]``"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.get_random_state:1
msgid "Get the backend specific random state object."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.get_random_state:3
msgid "[description], defaults to be None"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.get_random_state:5
msgid ":return:the backend specific random state object :rtype: Any"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:1
msgid ""
"GMRES solves the linear system A @ x = b for x given a vector `b` and a "
"general (not necessarily symmetric/Hermitian) linear operator `A`."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:4
msgid ""
"As a Krylov method, GMRES does not require a concrete matrix "
"representation of the n by n `A`, but only a function `vector1 = "
"A_mv(vector0, *A_args, **A_kwargs)` prescribing a one-to-one linear map "
"from vector0 to vector1 (that is, A must be square, and thus vector0 and "
"vector1 the same size). If `A` is a dense matrix, or if it is a "
"symmetric/Hermitian operator, a different linear solver will usually be "
"preferable."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:12
msgid ""
"GMRES works by first constructing the Krylov basis K = (x0, A_mv@x0, "
"A_mv@A_mv@x0, ..., (A_mv^num_krylov_vectors)@x_0) and then solving a "
"certain dense linear system K @ q0 = q1 from whose solution x can be "
"approximated. For `num_krylov_vectors = n` the solution is provably exact"
" in infinite precision, but the expense is cubic in `num_krylov_vectors` "
"so one is typically interested in the `num_krylov_vectors << n` case. The"
" solution can in this case be repeatedly improved, to a point, by "
"restarting the Arnoldi iterations each time `num_krylov_vectors` is "
"reached. Unfortunately the optimal parameter choices balancing expense "
"and accuracy are difficult to predict in advance, so applying this "
"function requires a degree of experimentation."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:24
msgid ""
"In a tensor network code one is typically interested in A_mv implementing"
" some tensor contraction. This implementation thus allows `b` and `x0` to"
" be of whatever arbitrary, though identical, shape `b = A_mv(x0, ...)` "
"expects. Reshaping to and from a matrix problem is handled internally."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:29
msgid ""
"A function `v0 = A_mv(v, *A_args, **A_kwargs)` where `v0` and `v` have "
"the same shape."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:31
msgid "The `b` in `A @ x = b`; it should be of the shape `A_mv` operates on."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:33
msgid ""
"Positional arguments to `A_mv`, supplied to this interface as a list. "
"Default: None."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:36
msgid ""
"Keyword arguments to `A_mv`, supplied to this interface as a dictionary. "
"Default: None."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:39
msgid ""
"An optional guess solution. Zeros are used by default. If `x0` is "
"supplied, its shape and dtype must match those of `b`, or an error will "
"be thrown. Default: zeros."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:44
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres:48
msgid ""
"Solution tolerance to achieve, norm(residual) <= max(tol*norm(b), atol). "
"Default: tol=1E-05          atol=tol"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:44
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres:48
msgid ""
"Solution tolerance to achieve, norm(residual) <= max(tol*norm(b), atol). "
"Default: tol=1E-05"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:47
#: tensornetwork.backends.abstract_backend.AbstractBackend.gmres:51
msgid "atol=tol"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:52
msgid ""
": Size of the Krylov space to build at each restart.   Expense is cubic "
"in this parameter. It must be positive.   If greater than b.size, it will"
" be set to b.size.   Default: 20"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:55
msgid ": Size of the Krylov space to build at each restart."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:54
msgid ""
"Expense is cubic in this parameter. It must be positive. If greater than "
"b.size, it will be set to b.size. Default: 20"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:57
msgid ""
"The Krylov space will be repeatedly rebuilt up to this many times. Large "
"values of this argument should be used only with caution, since "
"especially for nearly symmetric matrices and small `num_krylov_vectors` "
"convergence might well freeze at a value significantly larger than `tol`."
" Default: 1."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:63
msgid ""
"Inverse of the preconditioner of A; see the docstring for "
"`scipy.sparse.linalg.gmres`. This is only supported in the numpy backend."
" Supplying this argument to other backends will trigger "
"NotImplementedError. Default: None."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:69
msgid ""
"-if `x0` is supplied but its shape differs from that of `b`.     -in "
"NumPy, if the ARPACK solver reports a breakdown (which      usually "
"indicates some kind of floating point issue).     -if num_krylov_vectors "
"is 0 or exceeds b.size.     -if tol was negative.     -if M was supplied "
"with any backend but NumPy."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.gmres:71
msgid ""
"The converged solution. It has the same shape as `b`. info    : 0 if "
"convergence was achieved, the number of restarts otherwise."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:1
msgid "Return the function which is the grad function of input ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:13
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:13
msgid "the function to be differentiated"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:15
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:15
msgid ""
"the position of args in ``f`` that are to be differentiated, defaults to "
"be 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:17
msgid "the grad function of ``f`` with the same set of arguments as ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:1
msgid "Return 1.j in as a tensor compatible with the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:3
msgid "\"complex64\" or \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:5
msgid "1.j tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:1
msgid "Return the elementwise imaginary value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:5
msgid "imaginary value of ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:1
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:1
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:1
msgid "[summary]"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:5
msgid "The possible options"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:7
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:7
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:7
msgid "Sampling output shape"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randc:9
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randc:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:9
msgid ""
"probability for each option in a, defaults to None, as equal probability "
"distribution"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:1
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:1
msgid ""
"Call the random normal function with the random state management behind "
"the scene."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:7
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:7
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:11
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:7
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:9
msgid "[description], defaults to 1"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randn:9
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.implicit_randu:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:9
msgid "[description], defaults to \"32\""
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.index_update:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.index_update:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.index_update:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.index_update:1
msgid "Update `tensor` at elements defined by `mask` with value `assignee`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.index_update:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.index_update:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.index_update:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.index_update:3
msgid "A `Tensor` object."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.index_update:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.index_update:4
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.index_update:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.index_update:4
msgid "A boolean mask."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.index_update:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.index_update:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.index_update:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.index_update:5
msgid ""
"A scalar `Tensor`. The values to assigned to `tensor` at positions where "
"`mask` is `True`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.inv:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv:1
msgid "Compute the matrix inverse of `matrix`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.inv:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv:3
msgid "A matrix."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.inv:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.inv:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.inv:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.inv:5
msgid "The inverse of `matrix`"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse:1
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:1
msgid "Determine whether the type of input ``a`` is  ``sparse``."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse:3
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:3
msgid "input matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.is_sparse:5
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:5
msgid "a bool indicating whether the matrix ``a`` is sparse"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:1
msgid "Return a boolean on whether ``a`` is a tensor in backend package."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:3
msgid "a tensor to be determined"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:5
msgid "whether ``a`` is a tensor"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.item:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.item:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.item:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.item:1
msgid "Return the item of a 1-element tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.item:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.item:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.item:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.item:3
msgid "A 1-element tensor"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.item:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.item:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.item:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.item:5
msgid "The value in tensor."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev:1
msgid "Compute the Jacobian of ``f`` using reverse mode AD."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev:3
msgid "The function whose Jacobian is required"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev:5
msgid "the position of the arg as Jacobian input, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacrev:7
msgid "outer tuple for output, inner tuple for input args"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd:1
msgid "Compute the Jacobian of ``f`` using the forward mode AD."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd:3
msgid "the function whose Jacobian is required"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.jacfwd:7
msgid "outer tuple for input args, inner tuple for outputs"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:1
msgid ""
"Return a jitted or graph-compiled version of `fun` for JAX backend. For "
"all other backends returns `fun`. :param fun: Callable :param args: "
"Arguments to `fun`. :param kwargs: Keyword arguments to `fun`."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:7
msgid "jitted/graph-compiled version of `fun`, or just `fun`."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:1
msgid ""
"Function that computes a (forward-mode) Jacobian-vector product of ``f``."
" Strictly speaking, this function is value_and_jvp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:4
msgid "The function to compute jvp"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:6
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:7
msgid "input for ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:8
msgid "tangents"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:10
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:10
msgid ""
"(``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as the "
"output of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:1
msgid "Return the kronecker product of two matrices ``a`` and ``b``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:7
msgid "kronecker product of ``a`` and ``b``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.left_shift:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift:1
msgid "Shift the bits of an integer x to the left y bits."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.left_shift:3
#: tensorcircuit.backends.jax_backend.JaxBackend.mod:3
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift:3
msgid "input values"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.left_shift:5
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift:5
msgid "Number of bits shift to ``x``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.left_shift:7
#: tensorcircuit.backends.jax_backend.JaxBackend.right_shift:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.left_shift:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.left_shift:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.left_shift:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift:7
msgid "result with the same shape as ``x``"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.log:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.log:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.log:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.log:1
msgid "Return elementwise natural logarithm of `tensor`. :param tensor: A tensor."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._matmul_tf:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul:1
msgid ""
"Perform a possibly batched matrix-matrix multiplication between `tensor1`"
" and `tensor2`. The following behaviour is similar to `numpy.matmul`: - "
"If both arguments are 2-D they are multiplied like conventional"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._matmul_tf:5
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul:5
msgid "matrices."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._matmul_tf:6
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul:6
msgid ""
"If either argument is N-D, N > 2, it is treated as a stack of matrices "
"residing in the last two indexes and broadcast accordingly."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._matmul_tf:8
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul:8
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul:8
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul:8
msgid ""
"Both arguments to `matmul` have to be tensors of order >= 2. :param "
"tensor1: An input tensor. :param tensor2: An input tensor."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._matmul_tf:12
#: tensornetwork.backends.jax.jax_backend.JaxBackend.matmul:12
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.matmul:12
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.matmul:12
msgid "The result of performing the matmul."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:1
msgid "Return the maximum of an array or maximum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:5
#: tensorcircuit.backends.jax_backend.JaxBackend.min:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:5
#: tensorcircuit.keras.QuantumLayer.__init__:11
msgid "[description], defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:1
msgid "Compute the arithmetic mean for ``a`` along the specified ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:3
msgid "tensor to take average"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:5
msgid "the axis to take mean, defaults to None indicating sum over flatten array"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:7
msgid "_description_, defaults to False"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.min:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:1
msgid "Return the minimum of an array or minimum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mod:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod:1
msgid ""
"Compute y-mod of x (negative number behavior is not guaranteed to be "
"consistent)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mod:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod:5
msgid "mod ``y``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mod:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mod:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mod:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mod:7
msgid "results"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.multiply:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.multiply:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.multiply:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.multiply:1
msgid "Return the default multiplication of `tensor`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.multiply:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.multiply:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.multiply:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.multiply:3
msgid ""
"A backend can override such implementation. :param tensor1: A tensor. "
":param tensor2: A tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.norm:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.norm:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.norm:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.norm:1
msgid "Calculate the L2-norm of the elements of `tensor`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:1
msgid ""
"Return the numpy array of a tensor ``a``, but may not work in a jitted "
"function."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:5
msgid "numpy array of ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.one_hot:1
msgid "See doc for :py:meth:`onehot`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:1
msgid ""
"One-hot encodes the given ``a``. Each index in the input ``a`` is encoded"
" as a vector of zeros of length ``num`` with the element at index set to "
"one:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:5
msgid "input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:7
msgid "number of features in onehot dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:9
msgid "onehot tensor with the last extra dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:1
msgid ""
"Return an ones-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends). Block-sparse behavior is currently "
"not supported :param shape: The dimension of the returned matrix. :type "
"shape: int :param dtype: The dtype of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._outer_product_tf:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.outer_product:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.outer_product:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.outer_product:1
msgid "Calculate the outer product of the two given tensors."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.pivot:1
msgid ""
"Reshapes a tensor into a matrix, whose columns (rows) are the vectorized "
"dimensions to the left (right) of pivot_axis."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.pivot:4
msgid ""
"In other words, with tensor.shape = (1, 2, 4, 5) and pivot_axis=2, this "
"function returns an (8, 5) matrix."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.pivot:7
msgid "The tensor to pivot."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.pivot:8
msgid "The axis about which to pivot."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.pivot:10
msgid "The pivoted tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:1
msgid ""
"Returns the power of tensor a to the value of b. In the case b is a "
"tensor, then the power is by element"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:3
msgid "with a as the base and b as the exponent."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:5
msgid "In the case b is a scalar, then the power of each value in a"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:5
msgid "is raised to the exponent of b."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:7
msgid "The tensor that contains the base."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.power:8
msgid "The tensor that contains the exponent or a single scalar."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:1
msgid ""
"Drawn ``shots`` samples from probability distribution p, given the "
"external randomness determined by uniform distributed ``status`` tensor "
"or backend random generator ``g``. This method is similar with "
"``stateful_randc``, but it supports ``status`` beyond ``g``, which is "
"convenient when jit or vmap"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:6
msgid "Number of samples to draw with replacement"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:8
msgid "prbability vector"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:10
msgid ""
"external randomness as a tensor with each element drawn uniformly from "
"[0, 1], defaults to None"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:13
msgid "backend random genrator, defaults to None"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.probability_sample:15
msgid "The drawn sample as an int tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend._qr_jax:1
msgid ""
"Computes the QR decomposition of a tensor. See "
"tensornetwork.backends.tensorflow.decompositions for details."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.randn:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.randn:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.randn:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.randn:1
msgid ""
"Return a random-normal-matrix of dimension `dim` Depending on specific "
"backends, `dim` has to be either an int (numpy, torch, tensorflow) or a "
"`ShapeType` object (for block-sparse backends)."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.randn:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.randn:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.randn:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.randn:5
msgid ""
"Block-sparse behavior is currently not supported :param shape: The "
"dimension of the returned matrix. :type shape: int :param dtype: The "
"dtype of the returned matrix. :param seed: The seed for the random number"
" generator"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.random_split:1
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:1
msgid ""
"A jax like split API, but it doesn't split the key generator for other "
"backends. It is just for a consistent interface of random code; make sure"
" you know what the function actually does. This function is mainly a "
"utility to write backend agnostic code instead of doing magic things."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.random_uniform:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.random_uniform:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.random_uniform:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.random_uniform:1
msgid "Return a random uniform matrix of dimension `dim`."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.random_uniform:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.random_uniform:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.random_uniform:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.random_uniform:3
msgid ""
"Depending on specific backends, `dim` has to be either an int (numpy, "
"torch, tensorflow) or a `ShapeType` object (for block-sparse backends). "
"Block-sparse behavior is currently not supported :param shape: The "
"dimension of the returned matrix. :type shape: int :param boundaries: The"
" boundaries of the uniform distribution. :type boundaries: tuple :param "
"dtype: The dtype of the returned matrix. :param seed: The seed for the "
"random number generator"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.random_uniform:14
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.random_uniform:14
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.random_uniform:14
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.random_uniform:14
msgid "random uniform initialized tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:1
msgid "Return the elementwise real value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:5
msgid "real value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:1
msgid ""
"Rectified linear unit activation function. Computes the element-wise "
"function:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:4
msgid "\\mathrm{relu}(x)=\\max(x,0)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:11
msgid "Tensor after relu"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.reshape:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.reshape:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.reshape:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.reshape:1
msgid "Reshape tensor to the given shape."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.reshape:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.reshape:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.reshape:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.reshape:5
msgid "The reshaped tensor."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2:1
msgid "Reshape a tensor to the [2, 2, ...] shape."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reshape2:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem:5
msgid "the reshaped tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reshapem:1
msgid "Reshape a tensor to the [l, l] shape."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse:1
msgid "return ``a[::-1]``, only 1D tensor is guaranteed for consistent behavior"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse:3
msgid "1D tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.reverse:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.reverse:5
msgid "1D tensor in reverse order"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.right_shift:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.right_shift:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.right_shift:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.right_shift:1
msgid "Shift the bits of an integer x to the right y bits."
msgstr ""

#: of tensorcircuit.backends.jax_backend._rq_jax:1
msgid ""
"Computes the RQ (reversed QR) decomposition of a tensor. See "
"tensornetwork.backends.tensorflow.decompositions for details."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.scatter:1
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:1
msgid ""
"Roughly equivalent to operand[indices] = updates, indices only support "
"shape with rank 2 for now."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.searchsorted:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted:1
msgid "Find indices where elements should be inserted to maintain order."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.searchsorted:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted:3
msgid "input array sorted in ascending order"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.searchsorted:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted:5
msgid "value to inserted"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.searchsorted:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted:7
msgid ""
"If left, the index of the first suitable location found is given. If "
"right, return the last such index. If there is no suitable index, "
"return either 0 or N (where N is the length of a), defaults to \"left\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.searchsorted:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.searchsorted:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.searchsorted:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.searchsorted:12
msgid ""
"Array of insertion points with the same shape as v, or an integer if v is"
" a scalar."
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.serialize_tensor:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.serialize_tensor:1
msgid "Return a string that serializes the given tensor."
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.serialize_tensor:3
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sign:7
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.serialize_tensor:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.sign:7
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.sign:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sign:7
msgid "The input tensor."
msgstr ""

#: of
#: tensornetwork.backends.abstract_backend.AbstractBackend.serialize_tensor:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.serialize_tensor:5
msgid "A string representing the serialized tensor."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.set_random_state:1
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:1
msgid "Set the random state attached to the backend."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.set_random_state:3
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:3
msgid "the random seed, defaults to be None"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.set_random_state:5
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:5
msgid ""
"If set to be true, only get the random state in return instead of setting"
" the state on the backend"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_concat:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_concat:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_concat:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_concat:1
msgid "Concatenate a sequence of tensors together about the given axis."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_prod:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_prod:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_prod:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_prod:1
msgid "Take the product of all of the elements in values"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tensor:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tensor:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tensor:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tensor:1
msgid "Get the shape of a tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tensor:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tensor:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tensor:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tensor:5
msgid "The shape of the input tensor returned as another tensor."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tuple:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tuple:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tuple:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tuple:1
msgid "Get the shape of a tensor as a tuple of integers."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.shape_tuple:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.shape_tuple:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.shape_tuple:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.shape_tuple:5
msgid "The shape of the input tensor returned as a tuple of ints."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:1
msgid "Compute sigmoid of input ``a``"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.sign:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.sign:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sign:1
msgid ""
"Returns an elementwise tensor with entries y[i] = 1, 0, -1 where "
"tensor[i] > 0, == 0, and < 0 respectively."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.sign:4
msgid ""
"For complex input the behaviour of this function may depend on the "
"backend. The Jax backend version returns y[i] = x[i]/sqrt(x[i]^2)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:1
msgid "Return sin of `tensor`. :param tensor: A tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sinh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:1
msgid "Return the sinh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sinh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:5
msgid "sinh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:1
msgid "Return the total number of elements in ``a`` in tensor form."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:5
msgid "the total number of elements in ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen:1
msgid "Return the total number of elements in tensor ``a``, but in integer form."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sizen:5
msgid "the total number of elements in tensor ``a``"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.slice:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.slice:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.slice:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.slice:1
msgid "Obtains a slice of a tensor based on start_indices and slice_sizes."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.slice:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.slice:4
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.slice:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.slice:4
msgid "Tuple of integers denoting start indices of slice."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.slice:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.slice:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.slice:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.slice:5
msgid "Tuple of integers denoting size of slice along each axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:1
msgid ""
"Softmax function. Computes the function which rescales elements to the "
"range [0,1] such that the elements along axis sum to 1."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:4
msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:11
msgid ""
"A dimension along which Softmax will be computed , defaults to None for "
"all axis sum."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stack:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:7
msgid "concatenated tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:1
msgid "Solve the linear system Ax=b and return the solution x."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:3
msgid "The multiplied matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:5
msgid "The resulted matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:7
msgid "The solution of the linear system."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:1
msgid "A sparse matrix multiplies a dense matrix."
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:3
msgid "a sparse matrix"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:5
msgid "a dense matrix"
msgstr ""

#: of
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:7
msgid "dense matrix"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.sqrt:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.sqrt:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.sqrt:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sqrt:1
msgid "Take the square root (element wise) of a given tensor."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh:1
msgid "Return the sqrtm of a Hermitian matrix ``a``."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.sqrtmh:5
msgid "sqrtm of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:1
msgid "Concatenates a sequence of tensors ``a`` along a new dimension ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:3
msgid "List of tensors in the same shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:5
msgid "the stack axis, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:5
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:3
msgid "stateful register for each package"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:7
msgid "shape of output sampling tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randn:13
#: tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:11
msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:1
msgid "Uniform random sampler from ``low`` to ``high``."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.stateful_randu:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:5
msgid "shape of output sampling tensor, defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.std:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std:1
msgid "Compute the standard deviation along the specified axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.std:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std:5
msgid ""
"Axis or axes along which the standard deviation is computed, defaults to "
"None, implying all axis"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.std:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.std:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.std:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.std:8
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one, defaults to False"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:1
msgid "Stop backpropagation from ``a``."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.subtraction:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.subtraction:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.subtraction:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.subtraction:1
msgid ""
"Return the default substraction of `tensor`. A backend can override such "
"implementation. :param tensor1: A tensor. :param tensor2: A tensor."
msgstr ""

#: of tensorcircuit.backends.numpy_backend._sum_numpy:1
#: tensorcircuit.backends.pytorch_backend._sum_torch:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum:1
msgid ""
"Sum elements of `tensor` along the specified `axis`. Results in a new "
"Tensor with the summed axis removed. :param tensor: An input tensor."
msgstr ""

#: of tensorcircuit.backends.numpy_backend._sum_numpy:5
#: tensorcircuit.backends.pytorch_backend._sum_torch:5
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum:5
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum:5
msgid ""
"The result of performing the summation. The order of the tensor   will be"
" reduced by 1."
msgstr ""

#: of tensorcircuit.backends.numpy_backend._sum_numpy:7
#: tensorcircuit.backends.pytorch_backend._sum_torch:7
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum:7
msgid "The result of performing the summation. The order of the tensor"
msgstr ""

#: of tensorcircuit.backends.numpy_backend._sum_numpy:8
#: tensorcircuit.backends.pytorch_backend._sum_torch:8
#: tensornetwork.backends.jax.jax_backend.JaxBackend.sum:8
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sum:8
msgid "will be reduced by 1."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:1
msgid "Computes the singular value decomposition (SVD) of a tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:3
msgid ""
"The SVD is performed by treating the tensor as a matrix, with an "
"effective left (row) index resulting from combining the axes "
"`tensor.shape[:pivot_axis]` and an effective right (column) index "
"resulting from combining the axes `tensor.shape[pivot_axis:]`."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:8
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:8
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:8
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:8
msgid ""
"For example, if `tensor` had a shape (2, 3, 4, 5) and `pivot_axis` was 2,"
" then `u` would have shape (2, 3, 6), `s` would have shape (6), and `vh` "
"would have shape (6, 4, 5)."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:12
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:12
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:12
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:12
msgid ""
"If `max_singular_values` is set to an integer, the SVD is truncated to "
"keep at most this many singular values."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:15
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:15
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:15
msgid ""
"If `max_truncation_error > 0`, as many singular values will be truncated "
"as possible, so that the truncation error (the norm of discarded singular"
" values) is at most `max_truncation_error`. If `relative` is set `True` "
"then `max_truncation_err` is understood relative to the largest singular "
"value."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:21
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:21
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:21
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:21
msgid ""
"If both `max_singular_values` and `max_truncation_error` are specified, "
"the number of retained singular values will be `min(max_singular_values, "
"nsv_auto_trunc)`, where `nsv_auto_trunc` is the number of singular values"
" that must be kept to maintain a truncation error smaller than "
"`max_truncation_error`."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:27
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:27
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:27
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:27
msgid "The output consists of three tensors `u, s, vh` such that: ```python"
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:29
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:29
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:29
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:29
msgid "u[i1,...,iN, j] * s[j] * vh[j, k1,...,kM] == tensor[i1,...,iN, k1,...,kM]"
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:30
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:30
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:30
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:30
msgid ""
"``` Note that the output ordering matches numpy.linalg.svd rather than "
"tf.svd."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:33
#: tensorcircuit.backends.pytorch_backend._qr_torch:18
#: tensorcircuit.backends.pytorch_backend._rq_torch:18
#: tensorcircuit.backends.tensorflow_backend._qr_tf:18
#: tensorcircuit.backends.tensorflow_backend._rq_tf:18
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:33
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:33
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:33
msgid "A tensor to be decomposed."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:34
#: tensorcircuit.backends.pytorch_backend._qr_torch:20
#: tensorcircuit.backends.pytorch_backend._rq_torch:20
#: tensorcircuit.backends.tensorflow_backend._qr_tf:20
#: tensorcircuit.backends.tensorflow_backend._rq_tf:20
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:34
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:34
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:34
msgid "Where to split the tensor's axes before flattening into a matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:36
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:36
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:36
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:36
msgid "The number of singular values to keep, or `None` to keep them all."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:38
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:38
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:38
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:38
msgid "The maximum allowed truncation error or `None` to not do any truncation."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:40
#: tensorcircuit.cons.split_rules:7
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:24
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:40
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:40
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:40
msgid "Multiply `max_truncation_err` with the largest singular value."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:42
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:42
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:42
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:42
msgid ""
"Left tensor factor. s: Vector of ordered singular values from largest to "
"smallest. vh: Right tensor factor. s_rest: Vector of discarded singular "
"values (length zero if no         truncation)."
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:42
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:42
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:42
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:42
msgid ""
"Left tensor factor. s: Vector of ordered singular values from largest to "
"smallest. vh: Right tensor factor. s_rest: Vector of discarded singular "
"values (length zero if no"
msgstr ""

#: of tensorcircuit.backends.jax_backend._svd_jax:46
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.svd:46
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.svd:46
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.svd:46
msgid "truncation)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.switch:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:1
msgid "``branches[index]()``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tan:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:1
msgid "Return the tan of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tan:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:5
msgid "tan of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tanh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:1
msgid "Return the tanh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tanh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:5
msgid "tanh of ``a``"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._tensordot_tf:1
#: tensornetwork.backends.jax.jax_backend.JaxBackend.tensordot:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.tensordot:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.tensordot:1
msgid "Do a tensordot of tensors `a` and `b` over the given axes."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._tensordot_tf:4
#: tensornetwork.backends.jax.jax_backend.JaxBackend.tensordot:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.tensordot:4
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.tensordot:4
msgid "Another tensor."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend._tensordot_tf:5
#: tensornetwork.backends.jax.jax_backend.JaxBackend.tensordot:5
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.tensordot:5
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.tensordot:5
msgid "Two lists of integers. These values are the contraction axes."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:1
msgid "Constructs a tensor by tiling a given tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:5
msgid "1d tensor with length the same as the rank of ``a``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense:1
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:1
msgid "Convert a sparse matrix to dense tensor."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dense:5
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:5
msgid "the resulted dense matrix"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.to_dlpack:1
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dlpack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.to_dlpack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dlpack:1
msgid "Transform the tensor ``a`` as a dlpack capsule"
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.trace:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:1
msgid "Return summed entries along diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.trace:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:3
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:3
msgid ""
"If tensor is 2-D, the sum is over the diagonal of tensor with the given "
"offset, i.e., the collection of elements of the form a[i, i+offset]. If a"
" has more than two dimensions, then the axes specified by axis1 and axis2"
" are used to determine the 2-D sub-array whose diagonal is summed."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.trace:19
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:19
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:31
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:28
msgid "The batched summed diagonals."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.transpose:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.transpose:1
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.transpose:1
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.transpose:1
msgid ""
"Transpose a tensor according to a given permutation. By default the axes "
"are reversed. :param tensor: A tensor. :param perm: The permutation of "
"the axes."
msgstr ""

#: of tensornetwork.backends.jax.jax_backend.JaxBackend.transpose:6
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.transpose:6
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.transpose:6
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.transpose:6
msgid "The transposed tensor"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten:1
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten:1
msgid "Flatten python structure to 1D list"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten:3
msgid "python structure to be flattened"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_flatten:5
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_flatten:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_flatten:5
msgid ""
"The 1D list of flattened structure and treedef which can be used for "
"later unflatten"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map:1
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map:1
msgid "Return the new tree map with multiple arg function ``f`` through pytrees."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map:3
msgid "The function"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map:5
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map:5
msgid "inputs as any python structure"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map:7
msgid "raise when neither tensorflow or jax is installed."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_map:8
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_map:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_map:8
msgid "The new tree map with the same structure but different values."
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten:1
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten:1
msgid "Pack 1D list to pytree defined via ``treedef``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten:3
msgid "Def of pytree structure, the second return from ``tree_flatten``"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten:5
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten:5
msgid "the 1D list of flattened data structure"
msgstr ""

#: of tensorcircuit.backends.abstract_backend.ExtendedBackend.tree_unflatten:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tree_unflatten:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tree_unflatten:7
msgid "Packed pytree"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:1
msgid ""
"Find the unique elements and their corresponding counts of the given "
"tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:5
msgid "Unique elements, corresponding counts"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:1
msgid "Return the function which returns the value and grad of ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:17
msgid ""
"the value and grad function of ``f`` with the same set of arguments as "
"``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:1
msgid ""
"Return the VVAG function of ``f``. The inputs for ``f`` is (args[0], "
"args[1], args[2], ...), and the output of ``f`` is a scalar. Suppose "
"VVAG(f) is a function with inputs in the form (vargs[0], args[1], "
"args[2], ...), where vagrs[0] has one extra dimension than args[0] in the"
" first axis and consistent with args[0] in shape for remaining "
"dimensions, i.e. shape(vargs[0]) = [batch] + shape(args[0]). (We only "
"cover cases where ``vectorized_argnums`` defaults to 0 here for "
"demonstration). VVAG(f) returns a tuple as a value tensor with shape "
"[batch, 1] and a gradient tuple with shape: ([batch]+shape(args[argnum]) "
"for argnum in argnums). The gradient for argnums=k is defined as"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:9
msgid ""
"g^k = \\frac{\\partial \\sum_{i\\in batch} f(vargs[0][i], args[1], "
"...)}{\\partial args[k]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:13
msgid "Therefore, if argnums=0, the gradient is reduced to"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:15
msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:19
msgid ""
", which is specifically suitable for batched VQE optimization, where "
"args[0] is the circuit parameters."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:21
msgid "And if argnums=1, the gradient is like"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:23
msgid ""
"g^1_i = \\frac{\\partial \\sum_j f(vargs[0][j], args[1])}{\\partial "
"args[1][i]}\n"
"\n"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:26
msgid ""
", which is suitable for quantum machine learning scenarios, where ``f`` "
"is the loss function, args[0] corresponds to the input data and args[1] "
"corresponds to the weights in the QML model."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:6
msgid ""
"the args to be vectorized, these arguments should share the same batch "
"shape in the fist dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:1
msgid ""
"Function that computes the dot product between a vector v and the "
"Jacobian of the given function at the point given by the inputs. (reverse"
" mode AD relevant) Strictly speaking, this function is value_and_vjp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:5
msgid "the function to carry out vjp calculation"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:9
msgid ""
"value vector or gradient from downstream in reverse mode AD the same "
"shape as return of function ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:12
msgid "(``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:1
msgid ""
"Return the vectorized map or batched version of ``f`` on the first extra "
"axis. The general interface supports ``f`` with multiple arguments and "
"broadcast in the fist dimension."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:4
msgid "function to be broadcasted."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:9
msgid "vmap version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:1
msgid ""
"Return a zeros-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:5
msgid ""
"Block-sparse behavior is currently not supported :param shape: The "
"dimension of the returned matrix. :type shape: int :param dtype: The "
"dtype of the returned matrix."
msgstr ""

#: ../../source/api/backends/numpy_backend.rst:2
msgid "tensorcircuit.backends.numpy_backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend:1
msgid "Backend magic inherited from tensornetwork: numpy backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`, "
":py:class:`tensorcircuit.backends.abstract_backend.ExtendedBackend`"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid ""
"see the original backend API at `numpy backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:16
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.diagonal:19
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:12
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.trace:15
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to second-last/last axis."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:1
msgid ""
"Arnoldi method for finding the lowest eigenvector-eigenvalue pairs of a "
"linear operator `A`. If no `initial_state` is provided then `shape` and "
"`dtype` are required so that a suitable initial state can be randomly "
"generated. This is a wrapper for scipy.sparse.linalg.eigs which only "
"supports a subset of the arguments of scipy.sparse.linalg.eigs."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:8
msgid "A (sparse) implementation of a linear operator"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:9
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:9
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:11
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:9
msgid ""
"An initial vector for the algorithm. If `None`, a random initial `Tensor`"
" is created using the `numpy.random.randn` method."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:13
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:13
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:11
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:13
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:11
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:11
msgid ""
"The dtype of the input `A`. If both no `initial_state` is provided, a "
"random initial state with shape `shape` and dtype `dtype` is created."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:16
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:16
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:14
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:18
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:16
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:14
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:14
msgid ""
"The nummber of eigenvector-eigenvalue pairs to be computed. If `numeig > "
"1`, `reorthogonalize` has to be `True`."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:18
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:18
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:20
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:18
msgid "The desired precision of the eigenvalus. Uses"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:21
msgid ""
"['LM' | 'SM' | 'LR' | 'SR' | 'LI'] Which `k` eigenvectors and eigenvalues"
" to find:     'LM' : largest magnitude     'SM' : smallest magnitude     "
"'LR' : largest real part     'SR' : smallest real part     'LI' : largest"
" imaginary part"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:21
msgid ""
"['LM' | 'SM' | 'LR' | 'SR' | 'LI'] Which `k` eigenvectors and eigenvalues"
" to find:"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:23
msgid ""
"'LM' : largest magnitude 'SM' : smallest magnitude 'LR' : largest real "
"part 'SR' : smallest real part 'LI' : largest imaginary part"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:28
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:28
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:28
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:28
msgid "The maximum number of iterations."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:30
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:30
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:30
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:30
msgid ""
"An array of `numeig` lowest eigenvalues `list`: A list of `numeig` lowest"
" eigenvectors"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigs:32
msgid "`np.ndarray`"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:1
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:1
msgid ""
"Lanczos method for finding the lowest eigenvector-eigenvalue pairs of a "
"symmetric (hermitian) linear operator `A`. `A` is a callable implementing"
" the matrix-vector product. If no `initial_state` is provided then "
"`shape` and `dtype` have to be passed so that a suitable initial state "
"can be randomly  generated. :param A: A (sparse) implementation of a "
"linear operator :param arsg: A list of arguments to `A`.  `A` will be "
"called as"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:8
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:8
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:8
msgid "`res = A(initial_state, *args)`."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:19
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:19
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:19
msgid ""
"['LM' | 'SM' | 'LR' | 'SR' | 'LI' | 'SI'] Which `k` eigenvectors and "
"eigenvalues to find:     'LM' : largest magnitude     'SM' : smallest "
"magnitude     'LR' : largest real part     'SR' : smallest real part     "
"'LI' : largest imaginary part     'SI' : smallest imaginary part Note "
"that not all of those might be supported by specialized backends."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:19
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:19
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:19
msgid ""
"['LM' | 'SM' | 'LR' | 'SR' | 'LI' | 'SI'] Which `k` eigenvectors and "
"eigenvalues to find:"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:21
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:21
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:21
msgid ""
"'LM' : largest magnitude 'SM' : smallest magnitude 'LR' : largest real "
"part 'SR' : smallest real part 'LI' : largest imaginary part 'SI' : "
"smallest imaginary part"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:27
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:27
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:27
msgid "Note that not all of those might be supported by specialized backends."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:32
#: tensornetwork.backends.abstract_backend.AbstractBackend.eigsh:32
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh:32
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:12
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor:10
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.position:10
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize:9
msgid "`Tensor`"
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:1
msgid ""
"Lanczos method for finding the lowest eigenvector-eigenvalue pairs of a "
"linear operator `A`. :param A: A (sparse) implementation of a linear "
"operator."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:4
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:4
msgid ""
"Call signature of `A` is `res = A(vector, *args)`, where `vector` can be "
"an arbitrary `Tensor`, and `res.shape` has to be `vector.shape`."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:16
msgid ""
"The desired precision of the eigenvalus. Uses "
"`np.linalg.norm(eigvalsnew[0:numeig] - eigvalsold[0:numeig]) < tol` as "
"stopping criterion between two diagonalization steps of the tridiagonal "
"operator."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:25
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:25
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:25
msgid ""
"The tridiagonal Operator is diagonalized every `ndiag` iterations to "
"check convergence."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:30
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:30
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:30
msgid ""
"(eigvals, eigvecs)  eigvals: A list of `numeig` lowest eigenvalues  "
"eigvecs: A list of `numeig` lowest eigenvectors"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:33
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.eigsh_lanczos:33
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:33
msgid ""
"eigvals: A list of `numeig` lowest eigenvalues eigvecs: A list of "
"`numeig` lowest eigenvectors"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:7
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:7
msgid "Returns the exponentiation of tensor a raised to b."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:4
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:4
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:4
msgid "If b is a tensor, then the exponentiation is element-wise"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:3
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:3
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:3
msgid ""
"between the two tensors, with a as the base and b as the power. Note that"
" a and b must be broadcastable to the same shape if b is a tensor."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:7
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:7
msgid "If b is a scalar, then the exponentiation is each value in a"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:7
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:7
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:7
msgid "raised to the power of b."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:9
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:9
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:9
msgid "The tensor containing the bases."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:10
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:10
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:10
msgid "The tensor containing the powers; or a single scalar as the power."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:12
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:12
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:12
msgid ""
"The tensor that is each element of a raised to the   power of b.  Note "
"that the shape of the returned tensor   is that produced by the broadcast"
" of a and b."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:15
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:15
msgid "The tensor that is each element of a raised to the"
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.power:15
#: tensornetwork.backends.numpy.numpy_backend.NumPyBackend.power:15
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.power:15
msgid ""
"power of b.  Note that the shape of the returned tensor is that produced "
"by the broadcast of a and b."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.qr:1
msgid "Computes the QR decomposition of a tensor."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.rq:1
msgid "Computes the RQ (reversed QR) decomposition of a tensor."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.sign:1
msgid ""
"Returns an elementwise tensor with entries y[i] = 1, 0, -1 tensor[i] > 0,"
" == 0, and < 0 respectively."
msgstr ""

#: of tensornetwork.backends.numpy.numpy_backend.NumPyBackend.sign:4
msgid ""
"For complex input the behaviour of this function may depend on the "
"backend. The NumPy version returns y[i] = x[i]/sqrt(x[i]^2)."
msgstr ""

#: ../../source/api/backends/pytorch_backend.rst:2
msgid "tensorcircuit.backends.pytorch_backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend:1
msgid "Backend magic inherited from tensornetwork: pytorch backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`,"
" :py:class:`tensorcircuit.backends.abstract_backend.ExtendedBackend`"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"See the original backend API at `pytorch backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:4
msgid ""
"Note the functionality provided by pytorch backend is incomplete, it "
"currenly lacks native efficicent jit and vmap support."
msgstr ""

#: of tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:16
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.diagonal:20
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:16
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:24
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to second-last and last axis "
"(note this differs from the NumPy defaults)."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigs:1
msgid ""
"Arnoldi method for finding the lowest eigenvector-eigenvalue pairs of a "
"linear operator `A`. `A` is a callable implementing the matrix-vector "
"product. If no `initial_state` is provided then `shape` and `dtype` have "
"to be passed so that a suitable initial state can be randomly  generated."
" :param A: A (sparse) implementation of a linear operator :param arsg: A "
"list of arguments to `A`.  `A` will be called as"
msgstr ""

#: of
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:1
msgid ""
"Lanczos method for finding the lowest eigenvector-eigenvalue pairs of a "
"`LinearOperator` `A`. :param A: A (sparse) implementation of a linear "
"operator."
msgstr ""

#: of
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:8
msgid ""
"An initial vector for the Lanczos algorithm. If `None`, a random initial "
"`Tensor` is created using the `torch.randn` method"
msgstr ""

#: of
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.eigsh_lanczos:16
msgid ""
"The desired precision of the eigenvalus. Uses "
"`torch.norm(eigvalsnew[0:numeig] - eigvalsold[0:numeig]) < tol` as "
"stopping criterion between two diagonalization steps of the tridiagonal "
"operator."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:1
#: tensorcircuit.backends.tensorflow_backend._qr_tf:1
msgid ""
"Computes the QR decomposition of a tensor. The QR decomposition is "
"performed by treating the tensor as a matrix, with an effective left "
"(row) index resulting from combining the axes `tensor.shape[:pivot_axis]`"
" and an effective right (column) index resulting from combining the axes "
"`tensor.shape[pivot_axis:]`."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:9
#: tensorcircuit.backends.tensorflow_backend._qr_tf:9
msgid ""
"If `tensor` had a shape (2, 3, 4, 5) and `pivot_axis` was 2, then `q` "
"would have shape (2, 3, 6), and `r` would have shape (6, 4, 5). The "
"output consists of two tensors `Q, R` such that:"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:14
#: tensorcircuit.backends.pytorch_backend._rq_torch:14
#: tensorcircuit.backends.tensorflow_backend._qr_tf:14
#: tensorcircuit.backends.tensorflow_backend._rq_tf:14
msgid "Q[i1,...,iN, j] * R[j, k1,...,kM] == tensor[i1,...,iN, k1,...,kM]"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:16
#: tensorcircuit.backends.pytorch_backend._rq_torch:16
#: tensorcircuit.backends.tensorflow_backend._qr_tf:16
#: tensorcircuit.backends.tensorflow_backend._rq_tf:16
msgid "Note that the output ordering matches numpy.linalg.svd rather than tf.svd."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:22
#: tensorcircuit.backends.pytorch_backend._rq_torch:22
#: tensorcircuit.backends.tensorflow_backend._qr_tf:22
#: tensorcircuit.backends.tensorflow_backend._rq_tf:22
msgid "a bool indicating whether the tenor is diagonal non-negative matrix."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._qr_torch:24
#: tensorcircuit.backends.pytorch_backend._rq_torch:24
#: tensorcircuit.backends.tensorflow_backend._qr_tf:24
#: tensorcircuit.backends.tensorflow_backend._rq_tf:24
msgid "Q, the left tensor factor, and R, the right tensor factor."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._rq_torch:1
#: tensorcircuit.backends.tensorflow_backend._rq_tf:1
msgid ""
"Computes the RQ decomposition of a tensor. The QR decomposition is "
"performed by treating the tensor as a matrix, with an effective left "
"(row) index resulting from combining the axes `tensor.shape[:pivot_axis]`"
" and an effective right (column) index resulting from combining the axes "
"`tensor.shape[pivot_axis:]`."
msgstr ""

#: of tensorcircuit.backends.pytorch_backend._rq_torch:9
#: tensorcircuit.backends.tensorflow_backend._rq_tf:9
msgid ""
"If `tensor` had a shape (2, 3, 4, 5) and `pivot_axis` was 2, then `r` "
"would have shape (2, 3, 6), and `q` would have shape (6, 4, 5). The "
"output consists of two tensors `Q, R` such that:"
msgstr ""

#: of tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.sign:4
msgid ""
"For complex input the behaviour of this function may depend on the "
"backend. The PyTorch version is not implemented in this case."
msgstr ""

#: of tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:10
msgid ""
"In the PyTorch backend the trace is always over the main diagonal of the "
"last two entries."
msgstr ""

#: of tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:14
msgid ""
"Offset of the diagonal from the main diagonal. This argument is not "
"supported  by the PyTorch backend and an error will be raised if they are"
" specified."
msgstr ""

#: of tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:18
#: tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend.trace:24
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to first/second axis. These "
"arguments are not supported by the PyTorch backend and an error will be "
"raised if they are specified."
msgstr ""

#: ../../source/api/backends/tensorflow_backend.rst:2
msgid "tensorcircuit.backends.tensorflow_backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend:1
msgid "Backend magic inherited from tensornetwork: tensorflow backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`,"
" :py:class:`tensorcircuit.backends.abstract_backend.ExtendedBackend`"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"See the original backend API at `tensorflow backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
msgstr ""

#: of
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:16
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:24
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to second-last and last axis "
"(note this differs from the NumPy defaults).  These arguments are not "
"supported in the TensorFlow backend and an error will be raised if they "
"are specified."
msgstr ""

#: of
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:21
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.diagonal:29
msgid ""
"These arguments are not supported in the TensorFlow backend and an error "
"will be raised if they are specified."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:1
msgid ""
"Lanczos method for finding the lowest eigenvector-eigenvalue pairs of "
"`A`. :param A: A (sparse) implementation of a linear operator."
msgstr ""

#: of tensornetwork.backends.abstract_backend.AbstractBackend.eigsh_lanczos:16
msgid ""
"The desired precision of the eigenvalus. Uses "
"`backend.norm(eigvalsnew[0:numeig] - eigvalsold[0:numeig]) < tol` as "
"stopping criterion between two diagonalization steps of the tridiagonal "
"operator."
msgstr ""

#: of
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.sign:4
msgid ""
"For complex input the behaviour of this function may depend on the "
"backend. The TensorFlow version returns y[i] = x[i] / abs(x[i])."
msgstr ""

#: of
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:11
msgid ""
"Offset of the diagonal from the main diagonal. This argument is not "
"supported in the TensorFlow backend and an error will be raised if they "
"are specified."
msgstr ""

#: of
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:15
#: tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend.trace:21
msgid ""
"Axis to be used as the first/second axis of the 2D sub-arrays from which "
"the diagonals should be taken. Defaults to first/second axis. These "
"arguments are not supported in the TensorFlow backend and an error will "
"be raised if they are specified."
msgstr ""

#: ../../source/api/basecircuit.rst:2
msgid "tensorcircuit.basecircuit"
msgstr ""

#: of tensorcircuit.basecircuit:1
msgid "Quantum circuit: common methods for all circuit classes as MixIn"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit:1
#: tensorcircuit.mpscircuit.MPSCircuit:1
msgid "Bases: :py:class:`tensorcircuit.abstractcircuit.AbstractCircuit`"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.amplitude:1
msgid ""
"Returns the amplitude of the circuit given the bitstring l. For state "
"simulator, it computes :math:`\\langle l\\vert \\psi\\rangle`, for "
"density matrix simulator, it computes :math:`Tr(\\rho \\vert l\\rangle "
"\\langle 1\\vert)` Note how these two are different up to a square "
"operation."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.amplitude:16
msgid "The bitstring of 0 and 1s."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.amplitude:18
msgid "The amplitude of the circuit."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.copy:1
msgid "copy all nodes and dangling edges correspondingly"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.expectation_before:1
msgid ""
"Get the tensor network in the form of a list of nodes for the expectation"
" calculation before the real contraction"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.expectation_before:4
#: tensorcircuit.quantum.sample2count:7 tensorcircuit.utils.benchmark:7
msgid "_description_, defaults to True"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.get_quvector:1
msgid ""
"Get the representation of the output state in the form of ``QuVector`` "
"while maintaining the circuit uncomputed"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.get_quvector:4
#: tensorcircuit.mpscircuit.MPSCircuit.get_quvector:4
msgid "``QuVector`` representation of the output state from the circuit"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.measure_jit:1
msgid ""
"Take measurement to the given quantum lines. This method is jittable is "
"and about 100 times faster than unjit version!"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.measure_jit:4
#: tensorcircuit.circuit.Circuit.measure_reference:16
#: tensorcircuit.mpscircuit.MPSCircuit.measure:3
msgid "Measure on which quantum line."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.measure_jit:6
#: tensorcircuit.circuit.Circuit.measure_reference:17
#: tensorcircuit.mpscircuit.MPSCircuit.measure:5
msgid "If true, theoretical probability is also returned."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.measure_jit:8
#: tensorcircuit.mpscircuit.MPSCircuit.measure:7
msgid "external randomness, with shape [index], defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.measure_jit:10
#: tensorcircuit.circuit.Circuit.measure_reference:18
#: tensorcircuit.mpscircuit.MPSCircuit.measure:9
msgid "The sample output and probability (optional) of the quantum line."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.perfect_sampling:1
msgid ""
"Sampling bistrings from the circuit output based on quantum amplitudes. "
"Reference: arXiv:1201.3974."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.perfect_sampling:4
msgid "external randomness, with shape [nqubits], defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.perfect_sampling:6
msgid "Sampled bit string and the corresponding theoretical probability."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.probability:1
msgid "get the 2^n length probability vector over computational basis"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.probability:3
msgid "probability vector"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.readouterror_bs:1
msgid ""
"Apply readout error to original probabilities of bit string and return "
"the noisy probabilities."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.readouterror_bs:10
msgid "list of readout error for each qubits."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.readouterror_bs:12
msgid "probabilities of bit string"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.replace_inputs:1
msgid "Replace the input state with the circuit structure unchanged."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.replace_inputs:3
msgid "Input wavefunction."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:1
msgid "batched sampling from state or circuit tensor network directly"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:3
msgid "number of samples, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:5
msgid ""
"if true, we sample from the final state if memory allows, True is "
"preferred, defaults to False"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:8
msgid "readout_error, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:10
msgid ""
"sample format, defaults to None as backward compatibility check the doc "
"in :py:meth:`tensorcircuit.quantum.measurement_results`"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:13
#: tensorcircuit.quantum.measurement_counts:45
#: tensorcircuit.quantum.sample2all:10
msgid "alias for the argument ``format``"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:15
msgid "random generator,  defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:17
#: tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:41
#: tensorcircuit.quantum.measurement_counts:52
msgid ""
"external randomness given by tensor uniformly from [0, 1], if set, can "
"overwrite random_generator"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample:20
msgid ""
"List (if batch) of tuple (binary configuration tensor and corresponding "
"probability) if the format is None, and consistent with format when given"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:1
msgid ""
"Compute the expectation with given Pauli string with measurement shots "
"numbers"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:31
msgid "index for Pauli X, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:33
msgid "index for Pauli Y, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:35
msgid "index for Pauli Z, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:37
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:15
msgid ""
"number of measurement shots, defaults to None, indicating analytical "
"result"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:39
#: tensorcircuit.quantum.measurement_counts:50
msgid "random_generator, defaults to None"
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.sample_expectation_ps:44
msgid "readout_error, defaults to None. Overrided if noise_conf is provided."
msgstr ""

#: of tensorcircuit.basecircuit.BaseCircuit.to_graphviz:1
msgid ""
"Not an ideal visualization for quantum circuit, but reserve here as a "
"general approach to show the tensornetwork [Deprecated, use "
"``Circuit.vis_tex`` or ``Circuit.draw`` instead]"
msgstr ""

#: ../../source/api/channels.rst:2
msgid "tensorcircuit.channels"
msgstr ""

#: of tensorcircuit.channels:1
msgid "Some common noise quantum channels."
msgstr ""

#: of tensorcircuit.channels.KrausList:1
msgid "Bases: :py:class:`list`"
msgstr ""

#: of tensorcircuit.channels.KrausList.append:1
msgid "Append object to the end of the list."
msgstr ""

#: of tensorcircuit.channels.KrausList.clear:1
msgid "Remove all items from list."
msgstr ""

#: of tensorcircuit.channels.KrausList.copy:1
msgid "Return a shallow copy of the list."
msgstr ""

#: of tensorcircuit.channels.KrausList.count:1
msgid "Return number of occurrences of value."
msgstr ""

#: of tensorcircuit.channels.KrausList.extend:1
msgid "Extend list by appending elements from the iterable."
msgstr ""

#: of tensorcircuit.channels.KrausList.index:1
msgid "Return first index of value."
msgstr ""

#: of tensorcircuit.channels.KrausList.index:3
#: tensorcircuit.channels.KrausList.remove:3
msgid "Raises ValueError if the value is not present."
msgstr ""

#: of tensorcircuit.channels.KrausList.insert:1
msgid "Insert object before index."
msgstr ""

#: of tensorcircuit.channels.KrausList.pop:1
msgid "Remove and return item at index (default last)."
msgstr ""

#: of tensorcircuit.channels.KrausList.pop:3
msgid "Raises IndexError if list is empty or index is out of range."
msgstr ""

#: of tensorcircuit.channels.KrausList.remove:1
msgid "Remove first occurrence of value."
msgstr ""

#: of tensorcircuit.channels.KrausList.reverse:1
msgid "Reverse *IN PLACE*."
msgstr ""

#: of tensorcircuit.channels.KrausList.sort:1
msgid "Sort the list in ascending order and return None."
msgstr ""

#: of tensorcircuit.channels.KrausList.sort:3
msgid ""
"The sort is in-place (i.e. the list itself is modified) and stable (i.e. "
"the order of two equal elements is maintained)."
msgstr ""

#: of tensorcircuit.channels.KrausList.sort:6
msgid ""
"If a key function is given, apply it once to each list item and sort "
"them, ascending or descending, according to their function values."
msgstr ""

#: of tensorcircuit.channels.KrausList.sort:9
msgid "The reverse flag can be set to sort in descending order."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:1
msgid ""
"Return an amplitude damping channel. Notice: Amplitude damping "
"corrspondings to p = 1."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:4
msgid ""
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    \\sqrt{1-\\gamma} & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    \\sqrt{\\gamma} & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:31
msgid "the damping parameter of amplitude (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:33
msgid ":math:`p`"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:35
msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
msgstr ""

#: of tensorcircuit.channels.check_rep_transformation:1
msgid "Check the convertation between those representations."
msgstr ""

#: of tensorcircuit.channels.check_rep_transformation:3
#: tensorcircuit.channels.kraus_to_super:23
#: tensorcircuit.channels.kraus_to_super_gate:6
msgid "A sequence of Gate"
msgstr ""

#: of tensorcircuit.channels.check_rep_transformation:5
#: tensorcircuit.channels.evol_kraus:13 tensorcircuit.channels.evol_superop:11
msgid "Initial density matrix"
msgstr ""

#: of tensorcircuit.channels.check_rep_transformation:7
msgid "Whether print Kraus and new Kraus operators, defaults to False"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:1
msgid "Convert the Choi matrix representation to Kraus operator representation."
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:3
msgid "This can be done by firstly geting eigen-decomposition of Choi-matrix"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:5
msgid ""
"\\Lambda = \\sum_k \\gamma_k  \\vert \\phi_k \\rangle \\langle \\phi_k "
"\\vert\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:8
msgid "Then define Kraus operators"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:10
msgid ""
"K_k = \\sqrt{\\gamma_k} V_k\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:13
msgid ""
"where :math:`\\gamma_k\\geq0` and :math:`\\phi_k` is the col-val "
"vectorization of :math:`V_k` ."
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus tensorcircuit.channels.evol_kraus
#: tensorcircuit.channels.evol_superop
#: tensorcircuit.channels.kraus_identity_check
#: tensorcircuit.channels.kraus_to_super tensorcircuit.channels.super_to_choi
msgid "Examples"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:24
#: tensorcircuit.channels.choi_to_super:3
#: tensorcircuit.channels.kraus_to_choi:5
#: tensorcircuit.channels.super_to_choi:28
msgid "Choi matrix"
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:26
msgid ""
"A dictionary to restrict the calculation of kraus matrices. The "
"restriction can be the number of kraus terms, which is jitable. It can "
"also be the truncattion error, which is not jitable."
msgstr ""

#: of tensorcircuit.channels.choi_to_kraus:29
#: tensorcircuit.channels.krausgate_to_krausmatrix:5
#: tensorcircuit.channels.krausmatrix_to_krausgate:5
msgid "A list of Kraus operators"
msgstr ""

#: of tensorcircuit.channels.choi_to_super:1
msgid "Convert from Choi representation to Superoperator representation."
msgstr ""

#: of tensorcircuit.channels.choi_to_super:5
#: tensorcircuit.channels.evol_superop:13
#: tensorcircuit.channels.super_to_choi:26
#: tensorcircuit.channels.super_to_kraus:3
msgid "Superoperator"
msgstr ""

#: of tensorcircuit.channels.composedkraus:1
msgid "Compose the noise channels"
msgstr ""

#: of tensorcircuit.channels.composedkraus:3
msgid "One noise channel"
msgstr ""

#: of tensorcircuit.channels.composedkraus:5
msgid "Another noise channel"
msgstr ""

#: of tensorcircuit.channels.composedkraus:7
msgid "Composed nosie channel"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:1
msgid "Return a Depolarizing Channel"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:3
msgid ""
"\\sqrt{1-p_x-p_y-p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_x}\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    1 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_y}\n"
"\\begin{bmatrix}\n"
"    0 & -1j\\\\\n"
"    1j & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & -1\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:30
msgid ":math:`p_x`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:32
msgid ":math:`p_y`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:34
msgid ":math:`p_z`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:36
#: tensorcircuit.channels.generaldepolarizingchannel:15
msgid "Sequences of Gates"
msgstr ""

#: of tensorcircuit.channels.evol_kraus:1
msgid "The dynamic evolution according to Kraus operators."
msgstr ""

#: of tensorcircuit.channels.evol_kraus:3
msgid ""
"\\rho' = \\sum_{k} K_k \\rho K_k^{\\dagger}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.evol_kraus:15
#: tensorcircuit.channels.super_to_kraus:5
msgid "A list of Kraus operator"
msgstr ""

#: of tensorcircuit.channels.evol_kraus:17
#: tensorcircuit.channels.evol_superop:15
msgid "Final density matrix"
msgstr ""

#: of tensorcircuit.channels.evol_superop:1
msgid "The dynamic evolution according to Superoperator."
msgstr ""

#: of tensorcircuit.channels.generaldepolarizingchannel:1
msgid "Return a Depolarizing Channel for 1 qubit or 2 qubits"
msgstr ""

#: of tensorcircuit.channels.generaldepolarizingchannel:11
msgid "parameter for each Pauli channel"
msgstr ""

#: of tensorcircuit.channels.generaldepolarizingchannel:13
msgid "number of qubits, 1 and 2 are avaliable, defaults 1"
msgstr ""

#: of tensorcircuit.channels.is_hermitian_matrix:1
msgid "Test if an array is a Hermitian matrix"
msgstr ""

#: of tensorcircuit.channels.is_hermitian_matrix:3
msgid "Matrix"
msgstr ""

#: of tensorcircuit.channels.is_hermitian_matrix:5
msgid "_description_, defaults to 1e-8"
msgstr ""

#: of tensorcircuit.channels.is_hermitian_matrix:7
msgid "_description_, defaults to 1e-5"
msgstr ""

#: of tensorcircuit.channels.kraus_identity_check:1
msgid "Check identity of Kraus operators."
msgstr ""

#: of tensorcircuit.channels.kraus_identity_check:3
msgid ""
"\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_identity_check:12
msgid "List of Kraus operators."
msgstr ""

#: of tensorcircuit.channels.kraus_to_choi:1
msgid "Convert from Kraus representation to Choi representation."
msgstr ""

#: of tensorcircuit.channels.kraus_to_choi:3
msgid "A list Kraus operators"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:1
msgid ""
"Convert Kraus operator representation to Louivile-Superoperator "
"representation."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:3
msgid ""
"In the col-vec basis, the evolution of a state :math:`\\rho` in terms of "
"tensor components of superoperator :math:`\\varepsilon` can be expressed "
"as"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:6
msgid ""
"\\rho'_{mn} = \\sum_{\\mu \\nu}^{} \\varepsilon_{nm,\\nu \\mu} "
"\\rho_{\\mu \\nu}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:9
msgid ""
"The superoperator :math:`\\varepsilon` must make the dynamic map from "
":math:`\\rho` to :math:`\\rho'` to satisfy hermitian-perserving (HP), "
"trace-preserving (TP), and completely positive (CP)."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:12
msgid "We can construct the superoperator from Kraus operators by"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:14
msgid ""
"\\varepsilon = \\sum_{k} K_k^{*} \\otimes K_k\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super:25
msgid "The corresponding Tensor of Superoperator"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:1
msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:3
msgid ""
"\\sum_{k}^{} K_k \\otimes K_k^{*}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:8
msgid "The corresponding Tensor of the list of Kraus operators"
msgstr ""

#: of tensorcircuit.channels.krausgate_to_krausmatrix:1
msgid "Convert Kraus of Gate form to Matrix form."
msgstr ""

#: of tensorcircuit.channels.krausgate_to_krausmatrix:3
#: tensorcircuit.channels.krausmatrix_to_krausgate:3
msgid "A list of Kraus"
msgstr ""

#: of tensorcircuit.channels.krausmatrix_to_krausgate:1
msgid "Convert Kraus of Matrix form to Gate form."
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:1
msgid "Return a phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:18
msgid "The damping parameter of phase (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:20
msgid "A phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.resetchannel:1
#: tensorcircuit.channels.resetchannel:18
msgid "Reset channel"
msgstr ""

#: of tensorcircuit.channels.resetchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.reshuffle:1
msgid "Reshuffle the dimension index of a matrix."
msgstr ""

#: of tensorcircuit.channels.reshuffle:3
msgid "Input matrix"
msgstr ""

#: of tensorcircuit.channels.reshuffle:5
msgid "required order"
msgstr ""

#: of tensorcircuit.channels.reshuffle:7
msgid "Reshuffled matrix"
msgstr ""

#: of tensorcircuit.channels.super_to_choi:1
msgid "Convert Louivile-Superoperator representation to Choi representation."
msgstr ""

#: of tensorcircuit.channels.super_to_choi:3
msgid ""
"In the col-vec basis, the evolution of a state :math:`\\rho` in terms of "
"Choi matrix :math:`\\Lambda` can be expressed as"
msgstr ""

#: of tensorcircuit.channels.super_to_choi:6
msgid ""
"\\rho'_{mn} = \\sum_{\\mu,\\nu}^{} \\Lambda_{\\mu m,\\nu n} \\rho_{\\mu "
"\\nu}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.super_to_choi:9
msgid ""
"The Choi matrix :math:`\\Lambda` must make the dynamic map from "
":math:`\\rho` to :math:`\\rho'` to satisfy hermitian-perserving (HP), "
"trace-preserving (TP), and completely positive (CP)."
msgstr ""

#: of tensorcircuit.channels.super_to_choi:12
msgid ""
"Interms of tensor components we have the relationship of Louivile-"
"Superoperator representation and Choi representation"
msgstr ""

#: of tensorcircuit.channels.super_to_choi:15
msgid ""
"\\Lambda_{mn,\\mu \\nu} = \\varepsilon_{\\nu n,\\mu m}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.super_to_kraus:1
msgid "Convert from Superoperator representation to Kraus representation."
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:1
msgid "Return a thermal_relaxation_channel"
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:9
msgid "the T1 relaxation time."
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:11
msgid "the T2 dephasing time."
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:13
msgid "gate time"
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:15
msgid ""
"method to express error (default: \"ByChoi\"). When :math:`T1>T2`, choose"
" method \"ByKraus\" or \"ByChoi\" for jit. When :math:`T1<T2`,choose "
"method \"ByChoi\" for jit. Users can also set method as \"AUTO\" and "
"never mind the relative magnitude of :math:`T1,T2`, which is not jitable."
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:19
msgid "the population of  state :math:`|1\\rangle` at equilibrium (default: 0)"
msgstr ""

#: of tensorcircuit.channels.thermalrelaxationchannel:21
msgid "A thermal_relaxation_channel"
msgstr ""

#: ../../source/api/circuit.rst:2
msgid "tensorcircuit.circuit"
msgstr ""

#: of tensorcircuit.circuit:1
msgid "Quantum circuit: the state simulator"
msgstr ""

#: of tensorcircuit.circuit.Circuit:1 tensorcircuit.densitymatrix.DMCircuit:1
msgid "Bases: :py:class:`tensorcircuit.basecircuit.BaseCircuit`"
msgstr ""

#: of tensorcircuit.circuit.Circuit:1
msgid "``Circuit`` class. Simple usage demo below."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ANY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.any_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:4
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:4
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:4
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:4
msgid "Qubit number that the gate applies on."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:6
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:7
msgid "Parameters for the gate."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CNOT** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cnot_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid "Qubit number that the gate applies on. The matrix for the gate is"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CPHASE** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cphase_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CR** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cr_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.crx_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cry_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.crz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CU** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cu_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CY** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cy_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"0.-1.j\\\\    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.-1.j\\\\    "
"0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CZ** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **EXP** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.exp_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **EXP1** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.exp1_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **FREDKIN** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.fredkin_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **H** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.h_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    "
"0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    0.70710677+0.j"
" & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **I** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.i_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ISWAP** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.iswap_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply mpo gate in MPO format on the circuit. See "
":py:meth:`tensorcircuit.gates.mpo_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply multicontrol gate in MPO format on the circuit. See "
":py:meth:`tensorcircuit.gates.multicontrol_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.orx_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.ory_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.orz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OX** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.ox_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\"
"    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    1.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OY** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.oy_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 0.-1.j & 0.+0.j & 0.+0.j\\\\"
"    0.+1.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.+0.j & 0.-1.j & 0.+0.j & 0.+0.j\\\\    0.+1.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OZ** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.oz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & -1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"-1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **PHASE** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.phase_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **R** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.r_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rx_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RXX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rxx_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.ry_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RYY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.ryy_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RZZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rzz_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **S** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.s_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j "
"\\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **SD** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.sd_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j "
"\\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **SWAP** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.swap_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **T** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.t_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **TD** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.td_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **TOFFOLI** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.toffoli_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **U** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.u_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **WROOT** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.wroot_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    "
"0.5 & -0.5j & 0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    0.5 & -0.5j & "
"0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **X** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.x_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **Y** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.y_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **Z** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.z_gate`."
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.abstractcircuit.AbstractCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:1
msgid "Circuit object based on state simulator."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:3
#: tensorcircuit.mpscircuit.MPSCircuit.__init__:3
msgid "The number of qubits in the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``inputs`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:8
msgid "QuVector for a MPS like initial wavefunction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:10
#: tensorcircuit.densitymatrix.DMCircuit.__init__:15
msgid ""
"dict if two qubit gate is ready for split, including parameters for at "
"least one of ``max_singular_values`` and ``max_truncation_err``."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply amplitudedamping quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.amplitudedampingchannel`"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:6
msgid "uniform external random number between 0 and 1"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:6
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:6
msgid "Parameters for the channel."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:1
msgid ""
"Monte Carlo trajectory simulation of general Kraus channel whose Kraus "
"operators cannot be amplified to unitary operators. For unitary operators"
" composed Kraus channel, :py:meth:`unitary_kraus` is much faster."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:5
msgid ""
"This function is jittable in theory. But only jax+GPU combination is "
"recommended for jit since the graph building time is too long for other "
"backend options; though the running time of the function is very fast for"
" every case."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:9
msgid "A list of ``tn.Node`` for Kraus operators."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:11
msgid "The qubits index that Kraus channel is applied on."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:13
msgid ""
"Random tensor uniformly between 0 or 1, defaults to be None, when the "
"random number will be generated automatically"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply depolarizing quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.depolarizingchannel`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:1
msgid ""
"Apply depolarizing channel in a Monte Carlo way, i.e. for each call of "
"this method, one of gates from X, Y, Z, I are applied on the circuit "
"based on the probability indicated by ``px``, ``py``, ``pz``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:6
msgid "The qubit that depolarizing channel is on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:8
msgid "probability for X noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:10
msgid "probability for Y noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:12
msgid "probability for Z noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:14
msgid "random seed uniformly from 0 to 1, defaults to None (generated implicitly)"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing_reference:16
msgid "int Tensor, the element lookup: [0: x, 1: y, 2: z, 3: I]"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:1
#: tensorcircuit.densitymatrix.DMCircuit.expectation:1
msgid "Compute the expectation of corresponding operators."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:24
#: tensorcircuit.densitymatrix.DMCircuit.expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(tc.gates.z(), [1, ]), "
"(tc.gates.x(), [2, ])`` is for operator :math:`Z_1X_2`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:27
#: tensorcircuit.mpscircuit.MPSCircuit.expectation:6
msgid ""
"If True, then the wavefunction tensor is cached for further expectation "
"evaluation, defaults to be true."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:30
msgid "whether enable light cone simplification, defaults to False"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:39
#: tensorcircuit.circuit.expectation:50
msgid "\"Cannot measure two operators in one index\""
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:40
#: tensorcircuit.densitymatrix.DMCircuit.expectation:13
msgid "Tensor with one element"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply generaldepolarizing quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.generaldepolarizingchannel`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.get_quoperator:1
msgid ""
"Get the ``QuOperator`` MPO like representation of the circuit unitary "
"without contraction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.get_quoperator:3
msgid ""
"``QuOperator`` object for the circuit unitary (open indices for the input"
" state)"
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:1
msgid "[WIP], check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:3
msgid "The bool indicating whether the circuit is legal"
msgstr ""

#: of tensorcircuit.circuit.Circuit.matrix:1
msgid ""
"Get the unitary matrix for the circuit irrespective with the circuit "
"input state."
msgstr ""

#: of tensorcircuit.circuit.Circuit.matrix:3
msgid "The circuit unitary matrix"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_reference:1
msgid "Take measurement on the given quantum lines by ``index``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:1
msgid ""
"Middle measurement in z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalize the state manually if needed. This is a post-selection method "
"as keep is provided as a prior."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:5
msgid "The index of qubit that the Z direction postselection applied on."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:7
msgid "0 for spin up, 1 for spin down, defaults to be 0."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply phasedamping quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.phasedampingchannel`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_mps_inputs:1
msgid ""
"Replace the input state in MPS representation while keep the circuit "
"structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_mps_inputs:18
msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply reset quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.resetchannel`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:1
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:1
msgid "Compute the output wavefunction from the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:3
msgid ""
"The str indicating the form of the output wavefunction. \"default\": "
"[-1], \"ket\": [-1, 1], \"bra\": [1, -1]"
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:6
msgid "Tensor with the corresponding shape."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply thermalrelaxation quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.thermalrelaxationchannel`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:1
msgid ""
"Apply unitary gates in ``kraus`` randomly based on corresponding "
"``prob``. If ``prob`` is ``None``, this is reduced to kraus channel "
"language."
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:4
msgid "List of ``tc.gates.Gate`` or just Tensors"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:6
msgid "prob list with the same size as ``kraus``, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:8
msgid "random seed between 0 to 1, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:10
msgid "shape [] int dtype tensor indicates which kraus gate is actually applied"
msgstr ""

#: of tensorcircuit.circuit.expectation:1
msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`."
msgstr ""

#: of tensorcircuit.circuit.expectation:3
msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:24
msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:42
msgid ":math:`ket`. The state in tensor or ``QuVector`` format"
msgstr ""

#: of tensorcircuit.circuit.expectation:44
msgid ":math:`bra`, defaults to None, which is the same as ``ket``."
msgstr ""

#: of tensorcircuit.circuit.expectation:46
msgid ""
":math:`bra` changes to the adjoint matrix of :math:`bra`, defaults to "
"True."
msgstr ""

#: of tensorcircuit.circuit.expectation:48
msgid "Normalize the :math:`ket` and :math:`bra`, defaults to False."
msgstr ""

#: of tensorcircuit.circuit.expectation:51
msgid "The result of :math:`\\langle bra\\vert ops \\vert ket\\rangle`."
msgstr ""

#: ../../source/api/compiler.rst:2
msgid "tensorcircuit.compiler"
msgstr ""

#: ../../source/api/compiler/qiskit_compiler.rst:2
msgid "tensorcircuit.compiler.qiskit_compiler"
msgstr ""

#: of tensorcircuit.compiler.qiskit_compiler:1
msgid "compiler interface via qiskit"
msgstr ""

#: ../../source/api/cons.rst:2
msgid "tensorcircuit.cons"
msgstr ""

#: of tensorcircuit.cons:1
msgid "Constants and setups"
msgstr ""

#: of tensorcircuit.cons.get_contractor:1 tensorcircuit.cons.set_contractor:1
msgid ""
"To set runtime contractor of the tensornetwork for a better contraction "
"path. For more information on the usage of contractor, please refer to "
"independent tutorial."
msgstr ""

#: of tensorcircuit.cons.get_contractor:4 tensorcircuit.cons.set_contractor:4
msgid ""
"\"auto\", \"greedy\", \"branch\", \"plain\", \"tng\", \"custom\", "
"\"custom_stateful\". defaults to None (\"auto\")"
msgstr ""

#: of tensorcircuit.cons.get_contractor:6 tensorcircuit.cons.set_contractor:6
msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:8 tensorcircuit.cons.set_contractor:8
msgid ""
"It is not very useful, as ``memory_limit`` leads to ``branch`` "
"contraction instead of ``greedy`` which is rather slow, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:11 tensorcircuit.cons.set_contractor:11
msgid "Tensornetwork version is too low to support some of the contractors."
msgstr ""

#: of tensorcircuit.cons.get_contractor:12 tensorcircuit.cons.set_contractor:12
msgid "Unknown method options."
msgstr ""

#: of tensorcircuit.cons.get_contractor:13 tensorcircuit.cons.set_contractor:13
msgid "The new tensornetwork with its contractor set."
msgstr ""

#: of tensorcircuit.cons.get_dtype:1 tensorcircuit.cons.set_dtype:1
msgid "Set the global runtime numerical dtype of tensors."
msgstr ""

#: of tensorcircuit.cons.get_dtype:3 tensorcircuit.cons.set_dtype:3
msgid ""
"\"complex64\"/\"float32\" or \"complex128\"/\"float64\", defaults to "
"None, which is equivalent to \"complex64\"."
msgstr ""

#: of tensorcircuit.cons.get_dtype:6 tensorcircuit.cons.set_dtype:6
msgid "complex dtype str and the corresponding real dtype str"
msgstr ""

#: of tensorcircuit.cons.plain_contractor:1
msgid "The naive state-vector simulator contraction path."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:3
msgid "The list of ``tn.Node``."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:5
msgid "The list of dangling node edges, defaults to be None."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:7
msgid "The ``tn.Node`` after contraction"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:1
msgid "Context manager to set with-level runtime backend"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:3
#: tensorcircuit.cons.set_function_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\", defaults to None"
msgstr ""

#: of tensorcircuit.cons.runtime_backend tensorcircuit.cons.runtime_contractor
#: tensorcircuit.cons.runtime_dtype
msgid "yield"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:5
msgid "the backend object"
msgstr ""

#: of tensorcircuit.cons.runtime_contractor:1
msgid "Context manager to change with-levek contractor"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:1
msgid "Context manager to set with-level runtime dtype"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:3
msgid "\"complex64\" or \"complex128\", defaults to None (\"complex64\")"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:5
msgid "complex dtype str and real dtype str"
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:1
msgid "To set the runtime backend of tensorcircuit."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:3
msgid ""
"Note: ``tc.set_backend`` and ``tc.cons.set_tensornetwork_backend`` are "
"the same."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:27
msgid ""
"\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". defaults to None, which "
"gives the same behavior as "
"``tensornetwork.backend_contextmanager.get_default_backend()``."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:30
msgid "Whether the object should be set as global."
msgstr ""

#: of tensorcircuit.cons.set_function_backend:1
msgid "Function decorator to set function-level runtime backend"
msgstr ""

#: of tensorcircuit.cons.set_function_backend:5
msgid "Decorated function"
msgstr ""

#: of tensorcircuit.cons.set_function_contractor:1
msgid "Function decorate to change function-level contractor"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:1
msgid "Function decorator to set function-level numerical dtype"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:3
msgid "\"complex64\" or \"complex128\", defaults to None"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:5
msgid "The decorated function"
msgstr ""

#: of tensorcircuit.cons.split_rules:1
msgid "Obtain the direcionary of truncation rules"
msgstr ""

#: of tensorcircuit.cons.split_rules:3
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:11
msgid "The maximum number of singular values to keep."
msgstr ""

#: of tensorcircuit.cons.split_rules:5
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:13
msgid "The maximum allowed truncation error."
msgstr ""

#: ../../source/api/densitymatrix.rst:2
msgid "tensorcircuit.densitymatrix"
msgstr ""

#: of tensorcircuit.densitymatrix:1
msgid "Quantum circuit class but with density matrix simulator"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:1
msgid "The density matrix simulator based on tensornetwork engine."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:3
msgid "Number of qubits"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:5
msgid "if True, nothing initialized, only for internal use, defaults to False"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:7
msgid "the state input for the circuit, defaults to None"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:9
msgid "QuVector for a MPS like initial pure state."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:11
msgid "the density matrix input for the circuit, defaults to None"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:13
msgid "QuOperator for a MPO like initial density matrix."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:1
msgid "Return the output density matrix of the circuit."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:3
msgid ""
"check whether the final return is a legal density matrix, defaults to "
"False"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:5
msgid "whether to reuse previous results, defaults to True"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:7
msgid "The output densitymatrix in 2D shape tensor form"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.expectation:6
msgid "whether contract the density matrix in advance, defaults to True"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.get_dm_as_quoperator:1
msgid ""
"Get the representation of the output state in the form of ``QuOperator`` "
"while maintaining the circuit uncomputed"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.get_dm_as_quoperator:4
msgid "``QuOperator`` representation of the output state from the circuit"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.to_circuit:1
msgid ""
"convert into state simulator (current implementation ignores all noise "
"channels)"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.to_circuit:4
msgid "kws to initialize circuit object, defaults to None"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.to_circuit:7
msgid "Circuit with no noise"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.wavefunction:1
msgid ""
"get the wavefunction of outputs, raise error if the final state is not "
"purified [Experimental: the phase factor is not fixed for different "
"backend]"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.wavefunction:5
msgid "wavefunction vector"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit2:1
msgid "Bases: :py:class:`tensorcircuit.densitymatrix.DMCircuit`"
msgstr ""

#: ../../source/api/experimental.rst:2
msgid "tensorcircuit.experimental"
msgstr ""

#: of tensorcircuit.experimental:1
msgid "Experimental features"
msgstr ""

#: of tensorcircuit.experimental.hamiltonian_evol:1
msgid ""
"Fast implementation of static full Hamiltonian evolution (default as "
"imaginary time)"
msgstr ""

#: of tensorcircuit.experimental.hamiltonian_evol:13
msgid "result dynamics on ``tlist``"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:1
msgid ""
"similar to `grad` function but using parameter shift internally instead "
"of AD, vmap is utilized for evaluation, so the speed is still ok"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:4
#: tensorcircuit.experimental.parameter_shift_grad_v2:6
msgid "quantum function with weights in and expectation out"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:6
#: tensorcircuit.experimental.parameter_shift_grad_v2:8
msgid "label which args should be differentiated, defaults to 0"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:9
#: tensorcircuit.experimental.parameter_shift_grad_v2:11
msgid "whether jit the original function `f` at the beginning, defaults to False"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:12
#: tensorcircuit.experimental.parameter_shift_grad_v2:14
msgid ""
"two floats for the delta shift on the numerator and dominator, defaults "
"to (pi/2, 2) for parameter shift"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad:15
#: tensorcircuit.experimental.parameter_shift_grad_v2:17
msgid "the grad function"
msgstr ""

#: of tensorcircuit.experimental.parameter_shift_grad_v2:1
msgid ""
"similar to `grad` function but using parameter shift internally instead "
"of AD, vmap is utilized for evaluation, v2 also supports random generator"
" for finite measurememt shot, only jax backend is supported, since no "
"vmap randomness is available in tensorflow"
msgstr ""

#: ../../source/api/gates.rst:2
msgid "tensorcircuit.gates"
msgstr ""

#: of tensorcircuit.gates:1
msgid ""
"Declarations of single-qubit and two-qubit gates and their corresponding "
"matrix."
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Bases: :py:class:`tensornetwork.network_components.Node`"
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Wrapper of tn.Node, quantum gate"
msgstr ""

#: of tensornetwork.network_components.Node.__init__:1
msgid "Create a node."
msgstr ""

#: of tensornetwork.network_components.Node.__init__:3
msgid ""
"The concrete that is represented by this node, or a `AbstractNode` "
"object. If a tensor is passed, it can be be either a numpy array or the "
"tensor-type of the used backend. If a `AbstractNode` is passed, the "
"passed node has to have the same         backend as given by `backend`."
msgstr ""

#: of tensornetwork.network_components.Node.__init__:7
msgid "Name of the node. Used primarily for debugging."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_axis_names:3
#: tensornetwork.network_components.Node.__init__:8
msgid "List of names for each of the tensor's axes."
msgstr ""

#: of tensornetwork.network_components.Node.__init__:9
msgid "The name of the backend or an instance of a `AbstractBackend`."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_axis_names:5
#: tensornetwork.network_components.Node.__init__:11
msgid ""
"If there is a repeated name in `axis_names` or if the length     doesn't "
"match the shape of the tensor."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_axis_names:1
msgid "Add axis names to a Node."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_edge:1
msgid "Add an edge to the node on the given axis."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_edge:3
msgid "The edge to add."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_edge:4
msgid "The axis the edge points to."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_edge:5
msgid "If true, replace the existing edge with the new one."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.add_edge:7
msgid "If the edge on axis is not dangling."
msgstr ""

#: of tensornetwork.network_components.Node.from_serial_dict:1
msgid "Return a node given a serialized dict representing it."
msgstr ""

#: of tensornetwork.network_components.Node.from_serial_dict:3
msgid "A python dict representing a serialized node."
msgstr ""

#: of tensornetwork.network_components.Node.from_serial_dict:5
msgid "A node."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_all_dangling:1
msgid "Return the set of dangling edges connected to this node."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_all_nondangling:1
msgid "Return the set of nondangling edges connected to this node."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_axis_number:1
msgid "Get the axis number for a given axis name or value."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_dimension:1
msgid "Get the dimension of the given axis."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_dimension:3
msgid "The axis of the underlying tensor."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_dimension:5
msgid "The dimension of the given axis."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_dimension:7
msgid "if axis isn't an int or if axis is too large or small."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.get_rank:1
msgid "Return rank of tensor represented by self."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_axes:1
msgid "Reorder axes of the node's tensor."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_axes:3
msgid "This will also update all of the node's edges."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_axes:5
msgid "Permutation of the dimensions of the node's tensor."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_axes:7
#: tensornetwork.network_components.AbstractNode.reorder_edges:9
msgid "This node post reordering."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_axes:9
#: tensornetwork.network_components.AbstractNode.reorder_edges:12
msgid "If the Node has no tensor."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_edges:1
msgid "Reorder the edges for this given Node."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_edges:3
msgid ""
"This will reorder the node's edges and transpose the underlying tensor "
"accordingly."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_edges:6
msgid "List of edges. The order in the list determines the new edge ordering."
msgstr ""

#: of tensornetwork.network_components.AbstractNode.reorder_edges:11
msgid ""
"If either the list of edges is not the same as expected or     if you try"
" to reorder with a trace edge."
msgstr ""

#: of tensornetwork.network_components.Node.to_serial_dict:1
msgid "Return a serializable dict representing the node."
msgstr ""

#: of tensornetwork.network_components.Node.to_serial_dict:3
msgid "Returns: A dict object."
msgstr ""

#: of tensorcircuit.gates.GateVF:1
msgid "Bases: :py:class:`tensorcircuit.gates.GateF`"
msgstr ""

#: of tensorcircuit.gates.any_gate:1
msgid "Note one should provide the gate with properly reshaped."
msgstr ""

#: of tensorcircuit.gates.any_gate:3
msgid "corresponding gate"
msgstr ""

#: of tensorcircuit.gates.any_gate:5
msgid "The name of the gate."
msgstr ""

#: of tensorcircuit.gates.any_gate:7
msgid "the resulted gate"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:1
msgid "Convert the inputs to Tensor with specified dtype."
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:35
msgid "inputs"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:37
msgid "dtype of the output Tensors"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:39
msgid "List of Tensors"
msgstr ""

#: of tensorcircuit.gates.bmatrix:1
msgid "Returns a :math:`\\LaTeX` bmatrix."
msgstr ""

#: of tensorcircuit.gates.bmatrix:13
msgid "Formatted Display:"
msgstr ""

#: of tensorcircuit.gates.bmatrix:15
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
"\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.bmatrix:18
msgid "2D numpy array"
msgstr ""

#: of tensorcircuit.gates.bmatrix:20
msgid "ValueError(\"bmatrix can at most display two dimensions\")"
msgstr ""

#: of tensorcircuit.gates.bmatrix:21
msgid ":math:`\\LaTeX`-formatted string for bmatrix of the array a"
msgstr ""

#: of tensorcircuit.gates.cr_gate:1
msgid ""
"Controlled rotation gate. When the control qubit is 1, `rgate` is applied"
" to the target qubit."
msgstr ""

#: of tensorcircuit.gates.cr_gate:3 tensorcircuit.gates.cr_gate:5
#: tensorcircuit.gates.cr_gate:7 tensorcircuit.gates.exponential_gate:12
#: tensorcircuit.gates.exponential_gate_unity:13
#: tensorcircuit.gates.iswap_gate:12 tensorcircuit.gates.r_gate:9
#: tensorcircuit.gates.r_gate:11 tensorcircuit.gates.r_gate:13
#: tensorcircuit.gates.rgate_theoretical:8
#: tensorcircuit.gates.rgate_theoretical:10
#: tensorcircuit.gates.rgate_theoretical:12 tensorcircuit.gates.rx_gate:6
#: tensorcircuit.gates.rxx_gate:13 tensorcircuit.gates.ry_gate:6
#: tensorcircuit.gates.ryy_gate:13 tensorcircuit.gates.rz_gate:6
#: tensorcircuit.gates.rzz_gate:13
msgid "angle in radians"
msgstr ""

#: of tensorcircuit.gates.cr_gate:10
msgid "CR Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:1
#: tensorcircuit.gates.rxx_gate:1 tensorcircuit.gates.ryy_gate:1
#: tensorcircuit.gates.rzz_gate:1
msgid ""
"Faster exponential gate directly implemented based on RHS. Only works "
"when :math:`U^2 = I` is an identity matrix."
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:3
#: tensorcircuit.gates.rxx_gate:3 tensorcircuit.gates.ryy_gate:3
#: tensorcircuit.gates.rzz_gate:3
msgid ""
"\\textrm{exp}(U) &= e^{-j \\theta U} \\\\\n"
"        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:6
#: tensorcircuit.gates.exponential_gate_unity:7
#: tensorcircuit.gates.multicontrol_gate:7 tensorcircuit.gates.rxx_gate:7
#: tensorcircuit.gates.ryy_gate:7 tensorcircuit.gates.rzz_gate:7
msgid "input unitary :math:`U`"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:8
#: tensorcircuit.gates.exponential_gate:10
#: tensorcircuit.gates.exponential_gate_unity:9
#: tensorcircuit.gates.exponential_gate_unity:11 tensorcircuit.gates.rxx_gate:9
#: tensorcircuit.gates.rxx_gate:11 tensorcircuit.gates.ryy_gate:9
#: tensorcircuit.gates.ryy_gate:11 tensorcircuit.gates.rzz_gate:9
#: tensorcircuit.gates.rzz_gate:11
msgid "alias for the argument ``unitary``"
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:15
#: tensorcircuit.gates.rxx_gate:15 tensorcircuit.gates.ryy_gate:15
#: tensorcircuit.gates.rzz_gate:15
msgid "if True, the angel theta is mutiplied by 1/2, defaults to False"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:14
#: tensorcircuit.gates.exponential_gate_unity:18
#: tensorcircuit.gates.rxx_gate:18 tensorcircuit.gates.ryy_gate:18
#: tensorcircuit.gates.rzz_gate:18
msgid "suffix of Gate name"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:15
#: tensorcircuit.gates.exponential_gate_unity:20
#: tensorcircuit.gates.rxx_gate:20 tensorcircuit.gates.ryy_gate:20
#: tensorcircuit.gates.rzz_gate:20
msgid "Exponential Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:1
msgid "Exponential gate."
msgstr ""

#: of tensorcircuit.gates.exponential_gate:3
msgid ""
"\\textrm{exp}(U) = e^{-j \\theta U}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.get_u_parameter:1
msgid "From the single qubit unitary to infer three angles of IBMUgate,"
msgstr ""

#: of tensorcircuit.gates.get_u_parameter:3
msgid "numpy array, no backend agnostic version for now"
msgstr ""

#: of tensorcircuit.gates.get_u_parameter:5
msgid "theta, phi, lbd"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:1
msgid "iSwap gate."
msgstr ""

#: of tensorcircuit.gates.iswap_gate:3
msgid ""
"\\textrm{iSwap}(\\theta) =\n"
"\\begin{pmatrix}\n"
"    1 & 0 & 0 & 0\\\\\n"
"    0 & \\cos(\\frac{\\pi}{2} \\theta ) & j \\sin(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & j \\sin(\\frac{\\pi}{2} \\theta ) & \\cos(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & 0 & 0 & 1\\\\\n"
"\\end{pmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:14
msgid "iSwap Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:1
msgid "Convert Gate to numpy array."
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:10
msgid "input Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:12
msgid "Corresponding Tensor"
msgstr ""

#: of tensorcircuit.gates.meta_gate:1
msgid ""
"Inner helper function to generate gate functions, such as ``z()`` from "
"``_z_matrix``"
msgstr ""

#: of tensorcircuit.gates.multicontrol_gate:1
msgid ""
"Multicontrol gate. If the control qubits equal to ``ctrl``, :math:`U` is "
"applied to the target qubits."
msgstr ""

#: of tensorcircuit.gates.multicontrol_gate:5
msgid ""
"E.g., ``multicontrol_gate(tc.gates._zz_matrix, [1, 0, 1])`` returns a "
"gate of 5 qubits,"
msgstr ""

#: of tensorcircuit.gates.multicontrol_gate:4
msgid ""
"where the last 2 qubits are applied :math:`ZZ` gate, if the first 3 "
"qubits are :math:`\\ket{101}`."
msgstr ""

#: of tensorcircuit.gates.multicontrol_gate:9
msgid "control bit sequence"
msgstr ""

#: of tensorcircuit.gates.multicontrol_gate:11
msgid "Multicontrol Gate"
msgstr ""

#: of tensorcircuit.gates.phase_gate:1
msgid "The phase gate"
msgstr ""

#: of tensorcircuit.gates.phase_gate:3
msgid ""
"\\textrm{phase}(\\theta) =\n"
"\\begin{pmatrix}\n"
"    1 & 0 \\\\\n"
"    0 & e^{i\\theta} \\\\\n"
"\\end{pmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.phase_gate:10
msgid "angle in radians, defaults to 0"
msgstr ""

#: of tensorcircuit.gates.phase_gate:12
msgid "phase gate"
msgstr ""

#: of tensorcircuit.gates.r_gate:1
msgid "General single qubit rotation gate"
msgstr ""

#: of tensorcircuit.gates.r_gate:3
msgid ""
"R(\\theta, \\alpha, \\phi) = j \\cos(\\theta) I\n"
"- j \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
"- j \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
"- j \\sin(\\theta) \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:16
msgid "R Gate"
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:1
msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:3
msgid "A random single-qubit gate"
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:1
msgid "Returns a random two-qubit gate."
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:3
msgid "A random two-qubit gate"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:1
msgid ""
"Rotation gate implemented by matrix exponential. The output is the same "
"as `rgate`."
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:3
msgid ""
"R(\\theta, \\alpha, \\phi) = e^{-j \\theta \\left[\\sin(\\alpha) "
"\\cos(\\phi) X\n"
"                                           + \\sin(\\alpha) \\sin(\\phi) "
"Y\n"
"                                           + \\cos(\\alpha) Z\\right]}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:14
msgid "Rotation Gate"
msgstr ""

#: of tensorcircuit.gates.rx_gate:1
msgid "Rotation gate along :math:`x` axis."
msgstr ""

#: of tensorcircuit.gates.rx_gate:3
msgid ""
"RX(\\theta) = e^{-j\\frac{\\theta}{2}X}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rx_gate:8
msgid "RX Gate"
msgstr ""

#: of tensorcircuit.gates.ry_gate:1
msgid "Rotation gate along :math:`y` axis."
msgstr ""

#: of tensorcircuit.gates.ry_gate:3
msgid ""
"RY(\\theta) = e^{-j\\frac{\\theta}{2}Y}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.ry_gate:8
msgid "RY Gate"
msgstr ""

#: of tensorcircuit.gates.rz_gate:1
msgid "Rotation gate along :math:`z` axis."
msgstr ""

#: of tensorcircuit.gates.rz_gate:3
msgid ""
"RZ(\\theta) = e^{-j\\frac{\\theta}{2}Z}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rz_gate:8
msgid "RZ Gate"
msgstr ""

#: of tensorcircuit.gates.u_gate:1
msgid ""
"IBMQ U gate following the converntion of OpenQASM3.0. See `OpenQASM doc "
"<https://openqasm.com/language/gates.html#built-in-gates>`_"
msgstr ""

#: of tensorcircuit.gates.u_gate:4
msgid ""
"\\begin{split}U(\\theta,\\phi,\\lambda) := \\left(\\begin{array}{cc}\n"
"\\cos(\\theta/2) & -e^{i\\lambda}\\sin(\\theta/2) \\\\\n"
"e^{i\\phi}\\sin(\\theta/2) & e^{i(\\phi+\\lambda)}\\cos(\\theta/2) "
"\\end{array}\\right).\\end{split}"
msgstr ""

#: of tensorcircuit.gates.u_gate:10 tensorcircuit.gates.u_gate:12
#: tensorcircuit.gates.u_gate:14
msgid "_description_, defaults to 0"
msgstr ""

#: ../../source/api/interfaces.rst:2
msgid "tensorcircuit.interfaces"
msgstr ""

#: ../../source/api/interfaces/numpy.rst:2
msgid "tensorcircuit.interfaces.numpy"
msgstr ""

#: of tensorcircuit.interfaces.numpy:1
msgid "Interface wraps quantum function as a numpy function"
msgstr ""

#: of tensorcircuit.interfaces.numpy.numpy_interface:1
msgid "Convert ``fun`` on ML backend into a numpy function"
msgstr ""

#: of tensorcircuit.interfaces.numpy.numpy_interface:23
msgid "The quantum function"
msgstr ""

#: of tensorcircuit.interfaces.numpy.numpy_interface:25
#: tensorcircuit.interfaces.scipy.scipy_optimize_interface:39
msgid "whether to jit ``fun``, defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.numpy.numpy_interface:27
msgid "The numpy interface compatible version of ``fun``"
msgstr ""

#: ../../source/api/interfaces/scipy.rst:2
msgid "tensorcircuit.interfaces.scipy"
msgstr ""

#: of tensorcircuit.interfaces.scipy:1
msgid "Interface wraps quantum function as a scipy function for optimization"
msgstr ""

#: of tensorcircuit.interfaces.scipy.scipy_optimize_interface:1
msgid "Convert ``fun`` into a scipy optimize interface compatible version"
msgstr ""

#: of tensorcircuit.interfaces.scipy.scipy_optimize_interface:35
msgid "The quantum function with scalar out that to be optimized"
msgstr ""

#: of tensorcircuit.interfaces.scipy.scipy_optimize_interface:37
msgid "the shape of parameters that ``fun`` accepts, defaults to None"
msgstr ""

#: of tensorcircuit.interfaces.scipy.scipy_optimize_interface:41
msgid ""
"whether using gradient-based or gradient free scipy optimize interface, "
"defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.scipy.scipy_optimize_interface:44
msgid "The scipy interface compatible version of ``fun``"
msgstr ""

#: ../../source/api/interfaces/tensorflow.rst:2
msgid "tensorcircuit.interfaces.tensorflow"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow:1
msgid "Interface wraps quantum function as a tensorflow function"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:1
msgid ""
"Wrap a quantum function on different ML backend with a tensorflow "
"interface."
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:22
#: tensorcircuit.interfaces.torch.torch_interface:28
msgid "The quantum function with tensor in and tensor out"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:24
msgid "output tf dtype or in str"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:26
#: tensorcircuit.interfaces.torch.torch_interface:30
msgid "whether to jit ``fun``, defaults to False"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:28
#: tensorcircuit.interfaces.torch.torch_interface:32
msgid "whether transform tensor backend via dlpack, defaults to False"
msgstr ""

#: of tensorcircuit.interfaces.tensorflow.tensorflow_interface:30
#: tensorcircuit.interfaces.torch.torch_interface:34
msgid ""
"The same quantum function but now with torch tensor in and torch tensor "
"out while AD is also supported"
msgstr ""

#: ../../source/api/interfaces/tensortrans.rst:2
msgid "tensorcircuit.interfaces.tensortrans"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans:1
msgid "general function for interfaces transformation"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:1
msgid ""
"Function decorator that automatically convert inputs to tensors on "
"current backend"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:63
msgid ""
"the wrapped function whose arguments in ``argnums`` position are expected"
" to be tensor format"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:66
msgid "position of args under the auto conversion, defaults to 0"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:68
msgid ""
"try reshape all input tensor as matrix with shape rank 2, defaults to "
"False"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:71
msgid "convert ``Gate`` to tensor, defaults to False"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:73
msgid "reshape tensor from ``Gate`` input as matrix, defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:75
msgid "convert ``QuOperator`` to tensor, defaults to False"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:77
msgid "reshape tensor from ``QuOperator`` input as matrix, defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:79
msgid "whether cast to backend dtype, defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.args_to_tensor:81
msgid "The wrapped function"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.general_args_to_numpy:1
msgid "Given a pytree, get the corresponding numpy array pytree"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.general_args_to_numpy:3
msgid "pytree"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.general_args_to_numpy:5
msgid "the same format pytree with all tensor replaced by numpy array"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.numpy_args_to_backend:1
msgid "Given a pytree of numpy arrays, get the corresponding tensor pytree"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.numpy_args_to_backend:3
msgid "pytree of numpy arrays"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.numpy_args_to_backend:5
msgid "str of str of the same pytree shape as args, defaults to None"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.numpy_args_to_backend:7
msgid ""
"str or backend object, defaults to None, indicating the current default "
"backend"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.numpy_args_to_backend:10
msgid ""
"the same format pytree with all numpy array replaced by the tensors in "
"the target backend"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.which_backend:1
msgid "Given a tensor ``a``, return the corresponding backend"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.which_backend:5
msgid ""
"if true, return backend object, if false, return backend str, defaults to"
" True"
msgstr ""

#: of tensorcircuit.interfaces.tensortrans.which_backend:8
msgid "the backend object or backend str"
msgstr ""

#: ../../source/api/interfaces/torch.rst:2
msgid "tensorcircuit.interfaces.torch"
msgstr ""

#: of tensorcircuit.interfaces.torch:1
msgid "Interface wraps quantum function as a torch function"
msgstr ""

#: of tensorcircuit.interfaces.torch.torch_interface:1
msgid "Wrap a quantum function on different ML backend with a pytorch interface."
msgstr ""

#: ../../source/api/keras.rst:2
msgid "tensorcircuit.keras"
msgstr ""

#: of tensorcircuit.keras:1
msgid "Keras layer for tc quantum function"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:1
msgid ""
"`QuantumLayer` wraps the quantum function `f` as a `keras.Layer` so that "
"tensorcircuit is better integrated with tensorflow. Note that the input "
"of the layer can be tensors or even list/dict of tensors."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:5
msgid "Callabel function."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:7
msgid "The shape of the weights."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:9
msgid "The initializer of the weights, defaults to \"glorot_uniform\""
msgstr ""

#: of tensorcircuit.keras.load_func:1
msgid ""
"Load function from the files in the ``tf.savedmodel`` format. We can load"
" several functions at the same time, as they can be the same function of "
"different input shapes."
msgstr ""

#: of tensorcircuit.keras.load_func:24
msgid ""
"The fallback function when all functions loaded are failed, defaults to "
"None"
msgstr ""

#: of tensorcircuit.keras.load_func:26
msgid ""
"When there is not legal loaded function of the input shape and no "
"fallback callable."
msgstr ""

#: of tensorcircuit.keras.load_func:27
msgid ""
"A function that tries all loaded function against the input until the "
"first success one."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:1
msgid "The keras loss function that directly taking the model output as the loss."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:3
msgid "Ignoring this parameter."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:5
msgid "Model output."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:7
msgid "Model output, which is y_pred."
msgstr ""

#: of tensorcircuit.keras.save_func:1
msgid "Save tf function in the file (``tf.savedmodel`` format)."
msgstr ""

#: of tensorcircuit.keras.save_func:30
msgid "``tf.function`` ed function with graph building"
msgstr ""

#: of tensorcircuit.keras.save_func:32
msgid "the dir path to save the function"
msgstr ""

#: ../../source/api/mps_base.rst:2
msgid "tensorcircuit.mps_base"
msgstr ""

#: of tensorcircuit.mps_base:1
msgid "FiniteMPS from tensornetwork with bug fixed"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS:1
msgid "Bases: :py:class:`tensornetwork.matrixproductstates.finite_mps.FiniteMPS`"
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:4
msgid "Initialize a `FiniteMPS`. If `canonicalize` is `True` the state"
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:2
msgid ""
"is brought into canonical form, with `BaseMPS.center_position` at "
"`center_position`. if `center_position` is `None` and `canonicalize = "
"True`, `BaseMPS.center_position` is set to 0."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:6
msgid "A list of `Tensor` objects."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:7
msgid "The initial position of the center site."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:8
msgid "If `True` the mps is canonicalized at initialization."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.__init__:9
msgid ""
"The name of the backend that should be used to perform contractions. "
"Available backends are currently 'numpy', 'tensorflow', 'pytorch', 'jax'"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_one_site_gate:1
msgid ""
"Apply a one-site gate to an MPS. This routine will in general destroy any"
" canonical form of the state. If a canonical form is needed, the user can"
" restore it using `FiniteMPS.position` :param gate: a one-body gate "
":param site: the site where the gate should be applied"
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:1
msgid "Compute the action of the MPS transfer-operator at site `site`."
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:3
msgid "A site of the MPS"
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:4
msgid ""
"* if `1, 'l'` or `'left'`: compute the left-action   of the MPS transfer-"
"operator at `site` on the input `matrix`. * if `-1, 'r'` or `'right'`: "
"compute the right-action   of the MPS transfer-operator at `site` on the "
"input `matrix`"
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:5
msgid ""
"if `1, 'l'` or `'left'`: compute the left-action of the MPS transfer-"
"operator at `site` on the input `matrix`."
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:7
msgid ""
"if `-1, 'r'` or `'right'`: compute the right-action of the MPS transfer-"
"operator at `site` on the input `matrix`"
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:9
msgid "A rank-2 tensor or matrix."
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.apply_transfer_operator:11
msgid "The result of applying the MPS transfer-operator to `matrix`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:1
msgid ""
"Apply a two-site gate to an MPS. This routine will in general destroy any"
" canonical form of the state. If a canonical form is needed, the user can"
" restore it using `FiniteMPS.position`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:5
msgid "A two-body gate."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:7
msgid "The first site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:9
msgid "The second site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:15
msgid ""
"An optional value to choose the MPS tensor at `center_position` to be "
"isometric after the application of the gate. Defaults to `site1`. If the "
"MPS is canonical (i.e.`BaseMPS.center_position != None`), and if the "
"orthogonality center coincides with either `site1` or `site2`,  the "
"orthogonality center will be shifted to `center_position` (`site1` by "
"default). If the orthogonality center does not coincide with `(site1, "
"site2)` then `MPS.center_position` is set to `None`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:26
msgid ""
"\"rank of gate is {} but has to be 4\", \"site1 = {} is not between 0 <= "
"site < N - 1 = {}\", \"site2 = {} is not between 1 <= site < N = "
"{}\",\"Found site2 ={}, site1={}. Only nearest neighbor gates are "
"currently supported\", \"f center_position = {center_position} not  f in "
"{(site1, site2)} \", or \"center_position = {}, but gate is applied at "
"sites {}, {}. Truncation should only be done if the gate is applied at "
"the center position of the MPS.\""
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:32
msgid "A scalar tensor containing the truncated weight of the truncation."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.bond_dimension:1
msgid "The bond dimension of `bond`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.bond_dimensions:1
msgid "A list of bond dimensions of `BaseMPS`"
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize:1
msgid ""
"Bring the MPS into canonical form according to `center_position`. If "
"`center_position` is `None`, the MPS is canonicalized with "
"`center_position = 0`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize:5
msgid "If `True`, normalize matrices when shifting the orthogonality center."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.canonicalize:8
msgid "The norm of the MPS."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.check_canonical:1
msgid "Check whether the MPS is in the expected canonical form."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.check_canonical:3
msgid "The L2 norm of the vector of local deviations."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:1
msgid "Check orthonormality of tensor at site `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:3
msgid ""
"* if `'l'` or `'left'`: check left orthogonality * if `'r`' or `'right'`:"
" check right orthogonality"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:4
msgid "if `'l'` or `'left'`: check left orthogonality"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:5
msgid "if `'r`' or `'right'`: check right orthogonality"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:6
msgid "The site of the tensor."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:8
msgid "The L2 norm of the deviation from identity."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:9
msgid "scalar `Tensor`"
msgstr ""

#: of
#: tensornetwork.matrixproductstates.base_mps.BaseMPS.check_orthonormality:11
msgid "If which is different from 'l','left', 'r' or 'right'."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor:1
msgid "Returns the `Tensor` object at `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor:3
msgid ""
"If `site==len(self) - 1` `BaseMPS.connector_matrix` is absorbed fromt the"
" right-hand side into the returned `Tensor` object."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor:7
msgid "The site for which to return the `Node`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.get_tensor:9
msgid "The tensor at `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:1
msgid ""
"Compute left reduced density matrices for site `sites`. This returns a "
"dict `left_envs` mapping sites (int) to Tensors. `left_envs[site]` is the"
" left-reduced density matrix to the left of site `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:5
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:5
msgid "A list of sites of the MPS."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:8
msgid "The left-reduced density matrices   at each  site in `sites`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:10
msgid "The left-reduced density matrices"
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:11
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:11
msgid "at each  site in `sites`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.left_envs:12
#: tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:12
msgid "`dict` mapping `int` to `Tensor`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:1
msgid "Measure the expectation value of local operators `ops` site `sites`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:3
msgid "A list Tensors of rank 2; the local operators to be measured."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:5
msgid "Sites where `ops` act."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:7
msgid "measurements :math:`\\langle` `ops[n]`:math:`\\rangle` for n in `sites`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:1
msgid ""
"Compute the correlator :math:`\\langle` `op1[site1], "
"op2[s]`:math:`\\rangle` between `site1` and all sites `s` in `sites2`. If"
" `s == site1`, `op2[s]` will be applied first."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:6
msgid "Tensor of rank 2; the local operator at `site1`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:8
msgid "Tensor of rank 2; the local operator at `sites2`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:10
msgid "The site where `op1`  acts"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:12
msgid "Sites where operator `op2` acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:14
msgid ""
"Correlator :math:`\\langle` `op1[site1], op2[s]`:math:`\\rangle` for `s` "
":math:`\\in` `sites2`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.physical_dimensions:1
msgid "A list of physical Hilbert-space dimensions of `BaseMPS`"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:1
msgid "Shift `center_position` to `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:3
msgid "The site to which FiniteMPS.center_position should be shifted"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:4
msgid "If `True`, normalize matrices when shifting."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:5
msgid "If not `None`, truncate the MPS bond dimensions to `D`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:6
msgid ""
"if not `None`, truncate each bond dimension, but keeping the truncation "
"error below `max_truncation_err`."
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:9
msgid "The norm of the tensor at `FiniteMPS.center_position`"
msgstr ""

#: of tensornetwork.matrixproductstates.base_mps.BaseMPS.position:12
msgid "If `center_position` is `None`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:1
msgid ""
"Initialize a random `FiniteMPS`. The resulting state is normalized. Its "
"center-position is at 0."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:4
msgid "A list of physical dimensions."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:5
msgid "A list of bond dimensions."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:6
msgid "A numpy dtype."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:7
msgid "An optional backend."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.random:9
msgid "`FiniteMPS`"
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:1
msgid ""
"Compute right reduced density matrices for site `sites. This returns a "
"dict `right_envs` mapping sites (int) to Tensors. `right_envs[site]` is "
"the right-reduced density matrix to the right of site `site`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:8
msgid "The right-reduced density matrices   at each  site in `sites`."
msgstr ""

#: of tensornetwork.matrixproductstates.finite_mps.FiniteMPS.right_envs:10
msgid "The right-reduced density matrices"
msgstr ""

#: ../../source/api/mpscircuit.rst:2
msgid "tensorcircuit.mpscircuit"
msgstr ""

#: of tensorcircuit.mpscircuit:1
msgid "Quantum circuit: MPS state simulator"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit:1
msgid "``MPSCircuit`` class. Simple usage demo below."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.MPO_to_gate:1
msgid "Convert MPO to gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:1
msgid "MPSCircuit object based on state simulator."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:5
msgid "The center position of MPS, default to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:7
msgid ""
"If not None, the initial state of the circuit is taken as ``tensors`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None. When "
"``tensors`` are specified, if ``center_position`` is None, then the "
"tensors are canonicalized, otherwise it is assumed the tensors are "
"already canonicalized at the ``center_position``"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:12
msgid ""
"If not None, it is transformed to the MPS form according to the split "
"rules"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:14
msgid "Split rules"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:1
msgid "Apply a general qubit gate on MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:3
msgid "The Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:6
msgid "Qubit indices of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:5
msgid "\"MPS does not support application of gate on > 2 qubits.\""
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_MPO:1
msgid "Apply a MPO to the MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:1
msgid ""
"Apply a double qubit gate on adjacent qubits of Matrix Product States "
"(MPS)."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:5
msgid "The first qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:7
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:7
msgid "The second qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:9
msgid "Center position of MPS, default is None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:1
msgid "Apply a double qubit gate on MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_nqubit_gate:1
msgid "Apply a n-qubit gate by transforming the gate to MPO"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:1
msgid "Apply a single qubit gate on MPS; no truncation is needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:3
msgid "gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:5
msgid "Qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:1
msgid "Compute the conjugate of the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:3
msgid "The constructed MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy:1
msgid "Copy the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:1
msgid "Copy the current MPS without the tensors."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:1
msgid "Compute the expectation of corresponding operators in the form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(gates.Z(), [1]), "
"(gates.X(), [2])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:9
msgid "If not None, will be used as bra"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:11
msgid "Whether to conjugate the bra state"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:13
msgid "Whether to normalize the MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:15
#: tensorcircuit.mpscircuit.MPSCircuit.set_split_rules:5
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:5
msgid "Truncation split"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation:17
msgid "The expectation of corresponding operators"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.gate_to_MPO:1
msgid "Convert gate to MPO form with identities at empty sites"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_bond_dimensions:1
msgid "Get the MPS bond dimensions"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_bond_dimensions:3
#: tensorcircuit.mpscircuit.MPSCircuit.get_tensors:3
msgid "MPS tensors"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_center_position:1
msgid "Get the center position of the MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_center_position:3
msgid "center position"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:1
msgid "Get the normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:3
msgid "Normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_quvector:2
msgid "Get the representation of the output state in the form of ``QuVector``"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_quvector:2
msgid "has to be full contracted in MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_tensors:1
msgid "Get the MPS tensors"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:1
msgid "Check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:3
msgid "Whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.measure:1
msgid "Take measurement to the given quantum lines."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:1
msgid ""
"Middle measurement in the z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalized the state manually if needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:4
msgid "The index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.normalize:1
msgid "Normalize MPS Circuit according to the center position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:1
msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:4
msgid "The orthogonality center"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:1
msgid "Compute the projection between `other` as bra and `self` as ket."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:3
msgid "ket of the other MPS, which will be converted to bra automatically"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:5
msgid "The projection in form of tensor"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.reduce_dimension:1
msgid "Reduce the bond dimension between two adjacent sites by SVD"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.reduce_tensor_dimension:1
msgid "Reduce the bond dimension between two general tensors by SVD"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.set_split_rules:1
msgid ""
"Set truncation split when double qubit gates are applied. If nothing is "
"specified, no truncation will take place and the bond dimension will keep"
" growing. For more details, refer to `split_tensor`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.slice:1
msgid "Get a slice of the MPS (only for internal use)"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:3
msgid "the str indicating the form of the output wavefunction"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:5
msgid "Tensor with shape [1, -1]"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:9
msgid "i--A--B--j  -> i--XX--j"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:1
msgid "Construct the MPS tensors from a given wavefunction."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:3
msgid "The given wavefunction (any shape is OK)"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:7
msgid "Physical dimension, 2 for MPS and 4 for MPO"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:9
msgid "Whether to normalize the wavefunction"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction_to_tensors:11
msgid "The tensors"
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:1
msgid "Split the tensor by SVD or QR depends on whether a truncation is required."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:3
msgid "The input tensor to split."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:5
msgid "Determine the orthogonal center is on the left tensor or the right tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:7
msgid "Two tensors after splitting"
msgstr ""

#: ../../source/api/noisemodel.rst:2
msgid "tensorcircuit.noisemodel"
msgstr ""

#: of tensorcircuit.noisemodel:1
msgid "General Noise Model Construction."
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf:1
msgid "``Noise Configuration`` class."
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf.__init__:1
msgid "Establish a noise configuration."
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf.add_noise:1
msgid ""
"Add noise channels on specific gates and specific qubits in form of Kraus"
" operators."
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf.add_noise:3
msgid "noisy gate"
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf.add_noise:5
msgid "noise channel"
msgstr ""

#: of tensorcircuit.noisemodel.NoiseConf.add_noise:7
msgid ""
"the list of noisy qubit, defaults to None, indicating applying the noise "
"channel on all qubits"
msgstr ""

#: of tensorcircuit.noisemodel.apply_qir_with_noise:1
msgid "A newly defined circuit"
msgstr ""

#: of tensorcircuit.noisemodel.apply_qir_with_noise:3
msgid "The qir of the clean circuit"
msgstr ""

#: of tensorcircuit.noisemodel.apply_qir_with_noise:5
#: tensorcircuit.noisemodel.circuit_with_noise:5
msgid "Noise Configuration"
msgstr ""

#: of tensorcircuit.noisemodel.apply_qir_with_noise:7
#: tensorcircuit.noisemodel.circuit_with_noise:7
msgid "The status for Monte Carlo sampling, defaults to None"
msgstr ""

#: of tensorcircuit.noisemodel.apply_qir_with_noise:9
#: tensorcircuit.noisemodel.circuit_with_noise:9
msgid "A newly constructed circuit with noise"
msgstr ""

#: of tensorcircuit.noisemodel.circuit_with_noise:1
msgid "Noisify a clean circuit."
msgstr ""

#: of tensorcircuit.noisemodel.circuit_with_noise:3
msgid "A clean circuit"
msgstr ""

#: of tensorcircuit.noisemodel.expectation_noisfy:1
msgid "Calculate expectation value with noise configuration."
msgstr ""

#: of tensorcircuit.noisemodel.expectation_noisfy:3
#: tensorcircuit.noisemodel.sample_expectation_ps_noisfy:3
msgid "The clean circuit"
msgstr ""

#: of tensorcircuit.noisemodel.expectation_noisfy:12
msgid "expectation value with noise"
msgstr ""

#: of tensorcircuit.noisemodel.sample_expectation_ps_noisfy:1
msgid "Calculate sample_expectation_ps with noise configuration."
msgstr ""

#: of tensorcircuit.noisemodel.sample_expectation_ps_noisfy:13
msgid ""
"repetition time for Monte Carlo sampling  for noisfy calculation, "
"defaults to 1000"
msgstr ""

#: of tensorcircuit.noisemodel.sample_expectation_ps_noisfy:20
msgid ""
"external randomness given by tensor uniformly from [0, 1], defaults to "
"None, used for measurement sampling"
msgstr ""

#: of tensorcircuit.noisemodel.sample_expectation_ps_noisfy:23
msgid "sample expectation value with noise"
msgstr ""

#: ../../source/api/quantum.rst:2
msgid "tensorcircuit.quantum"
msgstr ""

#: of tensorcircuit.quantum:1
msgid "Quantum state and operator class backend by tensornetwork"
msgstr ""

#: of tensorcircuit.quantum
msgid "IMPORT"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:1
#: tensorcircuit.quantum.PauliStringSum2COO_tf:1
msgid "Generate tensorflow sparse matrix from Pauli string sum"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:3
msgid ""
"1D Tensor representing for a Pauli string, e.g. [1, 0, 0, 3, 2] is for "
":math:`X_0Z_3Y_4`"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:6
msgid ""
"the weight for the Pauli string defaults to None (all Pauli strings "
"weight 1.0)"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:9
msgid "the tensorflow sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:1
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:1
msgid ""
"Generate sparse tensor from Pauli string sum. Currently requires "
"tensorflow installed"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:4
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:4
#: tensorcircuit.quantum.PauliStringSum2COO_tf:3
#: tensorcircuit.quantum.PauliStringSum2Dense:5
msgid ""
"2D Tensor, each row is for a Pauli string, e.g. [1, 0, 0, 3, 2] is for "
":math:`X_0Z_3Y_4`"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:7
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:7
#: tensorcircuit.quantum.PauliStringSum2COO_tf:6
#: tensorcircuit.quantum.PauliStringSum2Dense:8
msgid ""
"1D Tensor, each element corresponds the weight for each Pauli string "
"defaults to None (all Pauli strings weight 1.0)"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:10
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:10
#: tensorcircuit.quantum.PauliStringSum2Dense:11
msgid ""
"default False. If True, return numpy coo else return backend compatible "
"sparse tensor"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:13
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:13
msgid "the scipy coo sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO_tf:9
msgid "the tensorflow coo sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2Dense:1
msgid ""
"Generate dense matrix from Pauli string sum. Currently requires "
"tensorflow installed."
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2Dense:14
msgid "the tensorflow dense matrix"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1 tensorcircuit.quantum.QuScalar:1
#: tensorcircuit.quantum.QuVector:1
msgid "Bases: :py:class:`tensorcircuit.quantum.QuOperator`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1
msgid "Represents an adjoint (row) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:1
msgid ""
"Constructs a new `QuAdjointVector` from a tensor network. This "
"encapsulates an existing tensor network, interpreting it as an adjoint "
"vector (row vector)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:5
#: tensorcircuit.quantum.QuOperator.__init__:9
msgid "The edges of the network to be used as the input edges."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:7
#: tensorcircuit.quantum.QuOperator.__init__:11
#: tensorcircuit.quantum.QuVector.__init__:6
msgid ""
"Nodes used to refer to parts of the tensor network that are not connected"
" to any input or output edges (for example: a scalar factor)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:10
#: tensorcircuit.quantum.QuScalar.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:9
msgid "Optional collection of edges to ignore when performing consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:1
msgid ""
"The adjoint of the operator. This creates a new `QuOperator` with "
"complex-conjugate copies of all tensors in the network and with the input"
" and output edges switched."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:5
msgid "The adjoint of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.check_network:1
msgid ""
"Check that the network has the expected dimensionality. This checks that "
"all input and output edges are dangling and that there are no other "
"dangling edges (except any specified in `ignore_edges`). If not, an "
"exception is raised."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:1
msgid ""
"Contract the tensor network in place. This modifies the tensor network "
"representation of the operator (or vector, or scalar), reducing it to a "
"single tensor, without changing the value."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:5
msgid "Manually specify the axis ordering of the final tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:7
msgid "The present object."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.copy:1
msgid "The deep copy of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.copy:3
msgid "The new copy of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:1
msgid ""
"Contracts the tensor network in place and returns the final tensor. Note "
"that this modifies the tensor network representing the operator. The "
"default ordering for the axes of the final tensor is: `*out_edges, "
"*in_edges`. If there are any \"ignored\" edges, their axes come first: "
"`*ignored_edges, *out_edges, *in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:8
#: tensorcircuit.quantum.QuOperator.eval_matrix:6
msgid ""
"Manually specify the axis ordering of the final tensor. The default "
"ordering is determined by `out_edges` and `in_edges` (see above)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:11
#: tensorcircuit.quantum.QuOperator.eval_matrix:9
msgid "Node count '{}' > 1 after contraction!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:12
msgid "The final tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval_matrix:1
msgid ""
"Contracts the tensor network in place and returns the final tensor in two"
" dimentional matrix. The default ordering for the axes of the final "
"tensor is: (:math:`\\prod` dimension of out_edges, :math:`\\prod` "
"dimension of in_edges)"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval_matrix:10
msgid "The two-dimentional tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:1
msgid ""
"Construct a `QuAdjointVector` directly from a single tensor. This first "
"wraps the tensor in a `Node`, then constructs the `QuAdjointVector` from "
"that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:27
msgid "The tensor for constructing an QuAdjointVector."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:29
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (input edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:33
msgid "The new constructed QuAdjointVector give from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_adjoint_vector:1
msgid ""
"Returns a bool indicating if QuOperator is an adjoint vector. Examples "
"can be found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_scalar:1
msgid ""
"Returns a bool indicating if QuOperator is a scalar. Examples can be "
"found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_vector:1
msgid ""
"Returns a bool indicating if QuOperator is a vector. Examples can be "
"found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.nodes:1
#: tensorcircuit.quantum.QuOperator.nodes:1
#: tensorcircuit.quantum.QuScalar.nodes:1
#: tensorcircuit.quantum.QuVector.nodes:1
msgid "All tensor-network nodes involved in the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.norm:1
msgid ""
"The norm of the operator. This is the 2-norm (also known as the Frobenius"
" or Hilbert-Schmidt norm)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:1
msgid ""
"The partial trace of the operator. Subsystems to trace out are supplied "
"as indices, so that dangling edges are connected to each other as: "
"`out_edges[i] ^ in_edges[i] for i in subsystems_to_trace_out` This does "
"not modify the original network. The original ordering of the remaining "
"subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:16
#: tensorcircuit.quantum.QuOperator.partial_trace:8
#: tensorcircuit.quantum.QuVector.reduced_density:16
msgid "Indices of subsystems to trace out."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:10
msgid "A new QuOperator or QuScalar representing the result."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:1
#: tensorcircuit.quantum.QuVector.projector:1
msgid ""
"The projector of the operator. The operator, as a linear operator, on the"
" adjoint of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:4
msgid ""
"Set :math:`A` is the operator in matrix form, then the projector of "
"operator is defined as: :math:`A^\\dagger A`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:6
#: tensorcircuit.quantum.QuVector.projector:6
msgid "The projector of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:1
#: tensorcircuit.quantum.QuVector.reduced_density:1
msgid "The reduced density of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:3
#: tensorcircuit.quantum.QuVector.reduced_density:3
msgid ""
"Set :math:`A` is the matrix of the operator, then the reduced density is "
"defined as:"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:5
msgid "\\mathrm{Tr}_{subsystems}(A^\\dagger A)"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:9
#: tensorcircuit.quantum.QuVector.reduced_density:9
msgid ""
"Firstly, take the projector of the operator, then trace out the "
"subsystems to trace out are supplied as indices, so that dangling edges "
"are connected to each other as: `out_edges[i] ^ in_edges[i] for i in "
"subsystems_to_trace_out` This does not modify the original network. The "
"original ordering of the remaining subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:18
#: tensorcircuit.quantum.QuVector.reduced_density:18
msgid ""
"The QuOperator of the reduced density of the operator with given "
"subsystems."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:1
msgid ""
"Tensor product with another operator. Given two operators `A` and `B`, "
"produces a new operator `AB` representing :math:`A  B`. The `out_edges` "
"(`in_edges`) of `AB` is simply the concatenation of the `out_edges` "
"(`in_edges`) of `A.copy()` with that of `B.copy()`: `new_out_edges = "
"[*out_edges_A_copy, *out_edges_B_copy]` `new_in_edges = "
"[*in_edges_A_copy, *in_edges_B_copy]`"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:20
msgid "The other operator (`B`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:22
msgid "The result (`AB`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.trace:1
msgid "The trace of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator:1
msgid ""
"Represents a linear operator via a tensor network. To interpret a tensor "
"network as a linear operator, some of the dangling edges must be "
"designated as `out_edges` (output edges) and the rest as `in_edges` "
"(input edges). Considered as a matrix, the `out_edges` represent the row "
"index and the `in_edges` represent the column index. The (right) action "
"of the operator on another then consists of connecting the `in_edges` of "
"the first operator to the `out_edges` of the second. Can be used to do "
"simple linear algebra with tensor networks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:1
msgid ""
"Creates a new `QuOperator` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a linear operator. The "
"network is checked for consistency: All dangling edges must either be in "
"`out_edges`, `in_edges`, or `ignore_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:4
msgid "The edges of the network to be used as the output edges."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:15
msgid ""
"Optional collection of dangling edges to ignore when performing "
"consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:18
msgid ""
"At least one reference node is required to specify a scalar. None "
"provided!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:1
msgid ""
"Construct a `QuOperator` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuOperator` from that "
"`Node`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:28
msgid "The tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:30
msgid "The axis indices of `tensor` to use as `out_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:32
msgid "The axis indices of `tensor` to use as `in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:34
msgid "The new operator."
msgstr ""

#: of tensorcircuit.quantum.QuScalar:1
msgid "Represents a scalar via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:1
msgid ""
"Constructs a new `QuScalar` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a scalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:4
msgid ""
"Nodes used to refer to the tensor network (need not be exhaustive - one "
"node from each disconnected subnetwork is sufficient)."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:1
msgid ""
"Construct a `QuScalar` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuScalar` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:22
msgid "The tensor for constructing a new QuScalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:24
msgid "The new constructed QuScalar from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector:1
msgid "Represents a (column) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuVector.__init__:1
msgid ""
"Constructs a new `QuVector` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a (column) vector."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:1
msgid ""
"Construct a `QuVector` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuVector` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:28
msgid "The tensor for constructing a \"QuVector\"."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:30
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (output edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:34
msgid "The new constructed QuVector from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector.projector:4
msgid ""
"Set :math:`A` is the operator in matrix form, then the projector of "
"operator is defined as: :math:`A A^\\dagger`"
msgstr ""

#: of tensorcircuit.quantum.QuVector.reduced_density:5
msgid "\\mathrm{Tr}_{subsystems}(A A^\\dagger)"
msgstr ""

#: of tensorcircuit.quantum.check_spaces:1
msgid ""
"Check the vector spaces represented by two lists of edges are compatible."
" The number of edges must be the same and the dimensions of each pair of "
"edges must match. Otherwise, an exception is raised."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:5 tensorcircuit.quantum.check_spaces:7
msgid "List of edges representing a many-body Hilbert space."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:10
msgid ""
"Hilbert-space mismatch: \"Cannot connect {} subsystems with {} "
"subsystems\", or \"Input dimension {} != output dimension {}.\""
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:1
msgid ""
"Compute :math:`\\prod_{i\\in \\\\text{index}} s_i`, where the probability"
" for each bitstring is given as a vector ``results``. Results is in the "
"format of \"count_vector\""
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:13
#: tensorcircuit.quantum.correlation_from_samples:4
msgid "list of int, indicating the position in the bitstring"
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:15
msgid "probability vector of shape 2^n"
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:17
msgid "Correlation expectation from measurement shots."
msgstr ""

#: of tensorcircuit.quantum.correlation_from_samples:1
msgid ""
"Compute :math:`\\prod_{i\\in \\\\text{index}} s_i (s=\\pm 1)`, Results is"
" in the format of \"sample_int\" or \"sample_bin\""
msgstr ""

#: of tensorcircuit.quantum.correlation_from_samples:6
msgid "sample tensor"
msgstr ""

#: of tensorcircuit.quantum.correlation_from_samples:10
msgid "Correlation expectation from measurement shots"
msgstr ""

#: of tensorcircuit.quantum.count_d2s:1
msgid ""
"measurement shots results, dense representation to sparse tuple "
"representation non-jittable due to the non fixed return shape count_tuple"
" to count_vector"
msgstr ""

#: of tensorcircuit.quantum.count_d2s:12
msgid "cutoff to determine nonzero elements, defaults to 1e-7"
msgstr ""

#: of tensorcircuit.quantum.count_s2d:1
msgid ""
"measurement shots results, sparse tuple representation to dense "
"representation count_vector to count_tuple"
msgstr ""

#: of tensorcircuit.quantum.count_tuple2dict:1
msgid "count_tuple to count_dict_bin or count_dict_int"
msgstr ""

#: of tensorcircuit.quantum.count_tuple2dict:3
msgid "count_tuple format"
msgstr ""

#: of tensorcircuit.quantum.count_tuple2dict:7
#: tensorcircuit.quantum.count_vector2dict:7
msgid "can be \"int\" or \"bin\", defaults to \"bin\""
msgstr ""

#: of tensorcircuit.quantum.count_tuple2dict:9
msgid "count_dict"
msgstr ""

#: of tensorcircuit.quantum.count_vector2dict:1
msgid "convert_vector to count_dict_bin or count_dict_int"
msgstr ""

#: of tensorcircuit.quantum.count_vector2dict:3
msgid "tensor in shape [2**n]"
msgstr ""

#: of tensorcircuit.quantum.double_state:1
msgid "Compute the double state of the given Hamiltonian operator ``h``."
msgstr ""

#: of tensorcircuit.quantum.double_state:3 tensorcircuit.quantum.gibbs_state:3
#: tensorcircuit.quantum.truncated_free_energy:5
msgid "Hamiltonian operator in form of Tensor."
msgstr ""

#: of tensorcircuit.quantum.double_state:5 tensorcircuit.quantum.free_energy:17
#: tensorcircuit.quantum.gibbs_state:5
#: tensorcircuit.quantum.renyi_free_energy:16
#: tensorcircuit.quantum.truncated_free_energy:7
msgid "Constant for the optimization, default is 1."
msgstr ""

#: of tensorcircuit.quantum.double_state:7
msgid "The double state of ``h`` with the given ``beta``."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:1
msgid ""
"Eliminates any connected CopyNodes that are identity matrices. This will "
"modify the network represented by `nodes`. Only identities that are "
"connected to other nodes are eliminated."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:5
msgid "Collection of nodes to search."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:7
msgid ""
"The Dictionary mapping remaining Nodes to any replacements, Dictionary "
"specifying all dangling-edge replacements."
msgstr ""

#: of tensorcircuit.quantum.entropy:1
msgid "Compute the entropy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.entropy:30 tensorcircuit.quantum.free_energy:13
#: tensorcircuit.quantum.renyi_entropy:3
#: tensorcircuit.quantum.renyi_free_energy:12
msgid "The density matrix in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.entropy:32 tensorcircuit.quantum.free_energy:19
msgid "Epsilon, default is 1e-12."
msgstr ""

#: of tensorcircuit.quantum.entropy:34
msgid "Entropy on the given density matrix."
msgstr ""

#: of tensorcircuit.quantum.fidelity:1
msgid "Return fidelity scalar between two states rho and rho0."
msgstr ""

#: of tensorcircuit.quantum.fidelity:3
msgid "\\operatorname{Tr}(\\sqrt{\\sqrt{rho} rho_0 \\sqrt{rho}})"
msgstr ""

#: of tensorcircuit.quantum.fidelity:7 tensorcircuit.quantum.fidelity:9
#: tensorcircuit.quantum.mutual_information:3 tensorcircuit.quantum.taylorlnm:3
#: tensorcircuit.quantum.trace_distance:3
#: tensorcircuit.quantum.trace_distance:5
#: tensorcircuit.quantum.truncated_free_energy:3
msgid "The density matrix in form of Tensor."
msgstr ""

#: of tensorcircuit.quantum.fidelity:11
msgid "The sqrtm of a Hermitian matrix ``a``."
msgstr ""

#: of tensorcircuit.quantum.free_energy:1
msgid "Compute the free energy of the given density matrix."
msgstr ""

#: of tensorcircuit.quantum.free_energy:15
#: tensorcircuit.quantum.renyi_free_energy:14
msgid "Hamiltonian operator in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.free_energy:22
msgid "The free energy of the given density matrix with the Hamiltonian operator."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:1
msgid ""
"Generate a local Hamiltonian operator based on the given sequence of "
"Tensor. Note: further jit is recommended. For large Hilbert space, sparse"
" Hamiltonian is recommended"
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:5
msgid "A sequence of Tensor."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:7
msgid "Return Hamiltonian operator in form of matrix, defaults to True."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:9
msgid "The Hamiltonian operator in form of QuOperator or matrix."
msgstr ""

#: of tensorcircuit.quantum.gibbs_state:1
msgid "Compute the Gibbs state of the given Hamiltonian operator ``h``."
msgstr ""

#: of tensorcircuit.quantum.gibbs_state:7
msgid "The Gibbs state of ``h`` with the given ``beta``."
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:1
msgid ""
"Generate Heisenberg Hamiltonian with possible external fields. Currently "
"requires tensorflow installed"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:13
msgid "input circuit graph"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:15
msgid "zz coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:17
msgid "xx coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:19
msgid "yy coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:21
msgid "External field on z direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:23
msgid "External field on y direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:25
msgid "External field on x direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:27
msgid "Whether to return sparse Hamiltonian operator, default is True."
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:29
msgid "whether return the matrix in numpy or tensorflow form"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:32
msgid "Hamiltonian measurements"
msgstr ""

#: of tensorcircuit.quantum.identity:1
msgid ""
"Construct a 'QuOperator' representing the identity on a given space. "
"Internally, this is done by constructing 'CopyNode's for each edge, with "
"dimension according to 'space'."
msgstr ""

#: of tensorcircuit.quantum.identity:26
msgid ""
"A sequence of integers for the dimensions of the tensor product factors "
"of the space (the edges in the tensor network)."
msgstr ""

#: of tensorcircuit.quantum.identity:29
msgid ""
"The data type by np.* (for conversion to dense). defaults None to tc "
"dtype."
msgstr ""

#: of tensorcircuit.quantum.identity:31
msgid "The desired identity operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:1
msgid ""
"Simulate the measuring of each qubit of ``p`` in the computational basis,"
" thus producing output like that of ``qiskit``."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:4
msgid "Six formats of measurement counts results:"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:6
msgid "\"sample_int\": # np.array([0, 0])"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:8
msgid "\"sample_bin\": # [np.array([1, 0]), np.array([1, 0])]"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:10
msgid "\"count_vector\": # np.array([2, 0, 0, 0])"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:12
msgid "\"count_tuple\": # (np.array([0]), np.array([2]))"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:14
msgid "\"count_dict_bin\": # {\"00\": 2, \"01\": 0, \"10\": 0, \"11\": 0}"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:16
msgid "\"count_dict_int\": # {0: 2, 1: 0, 2: 0, 3: 0}"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:37
msgid ""
"The quantum state, assumed to be normalized, as either a ket or density "
"operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:39
msgid "The number of counts to perform."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:41
msgid "alias for the argument ``counts``"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:43
msgid "defaults to be \"direct\", see supported format above"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:47
msgid ""
"if True, the `state` is directly regarded as a probability list, defaults"
" to be False"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:55
msgid "if True, jax backend try using a jittable count, defaults to False"
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:57
msgid "The counts for each bit string measured."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:1
msgid "Mutual information between AB subsystem described by ``cut``."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:5
msgid "The AB subsystem."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:7
msgid "The mutual information between AB subsystem described by ``cut``."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:1
msgid ""
"Constructs an appropriately specialized QuOperator. If there are no "
"edges, creates a QuScalar. If the are only output (input) edges, creates "
"a QuVector (QuAdjointVector). Otherwise creates a QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:48
msgid "A list of output edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:50
msgid "A list of input edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:52
msgid ""
"Reference nodes for the tensor network (needed if there is a. scalar "
"component)."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:55
msgid "Edges to ignore when checking the dimensionality of the tensor network."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:58
msgid "The new created QuOperator object."
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:1
msgid "Convert MPO in Quimb package to QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:3
msgid "MPO in the form of Quimb package"
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:5 tensorcircuit.quantum.tn2qop:5
msgid "MPO in the form of QuOperator"
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:1
msgid "Compute the reduced density matrix from the quantum state ``state``."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:3
msgid "The quantum state in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:5
msgid ""
"the index list that is traced out, if cut is a int, it indicates [0, cut]"
" as the traced out region"
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:8
msgid "probability decoration, default is None."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:10
msgid "The reduced density matrix."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:1
msgid "Compute the Rnyi entropy of order :math:`k` by given density matrix."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:5
#: tensorcircuit.quantum.renyi_free_energy:18
msgid "The order of Rnyi entropy, default is 2."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:7
#: tensorcircuit.quantum.renyi_free_energy:20
msgid "The :math:`k` th order of Rnyi entropy."
msgstr ""

#: of tensorcircuit.quantum.renyi_free_energy:1
msgid ""
"Compute the Rnyi free energy of the corresponding density matrix and "
"Hamiltonian."
msgstr ""

#: of tensorcircuit.quantum.sample2all:1
msgid ""
"transform ``sample_int`` or ``sample_bin`` form results to other forms "
"specified by ``format``"
msgstr ""

#: of tensorcircuit.quantum.sample2all:3
msgid "measurement shots results in ``sample_int`` or ``sample_bin`` format"
msgstr ""

#: of tensorcircuit.quantum.sample2all:7
msgid ""
"see the doc in the doc in "
":py:meth:`tensorcircuit.quantum.measurement_results`, defaults to "
"\"count_vector\""
msgstr ""

#: of tensorcircuit.quantum.sample2all:12
msgid "only applicable to count transformation in jax backend, defaults to False"
msgstr ""

#: of tensorcircuit.quantum.sample2all:14
msgid "measurement results specified as ``format``"
msgstr ""

#: of tensorcircuit.quantum.sample2count:1
msgid "sample_int to count_tuple"
msgstr ""

#: of tensorcircuit.quantum.sample_bin2int:1
msgid "bin sample to int sample"
msgstr ""

#: of tensorcircuit.quantum.sample_bin2int:3
msgid "in shape [trials, n] of elements (0, 1)"
msgstr ""

#: of tensorcircuit.quantum.sample_bin2int:7
msgid "in shape [trials]"
msgstr ""

#: of tensorcircuit.quantum.sample_int2bin:1
msgid "int sample to bin sample"
msgstr ""

#: of tensorcircuit.quantum.sample_int2bin:3
msgid "in shape [trials] of int elements in the range [0, 2**n)"
msgstr ""

#: of tensorcircuit.quantum.sample_int2bin:7
msgid "in shape [trials, n] of element (0, 1)"
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:1
msgid ""
"Generate all n-bitstrings as an array, each row is a bitstring basis. "
"Return m-th col."
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:9
msgid "length of a bitstring"
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:11
msgid "m<n,"
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:13
msgid "the binary elements to generate, default is (1, -1)."
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:15
msgid ""
"The value for the m-th position in bitstring when going through all "
"bitstring basis."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:1
msgid "Taylor expansion of :math:`ln(x+1)`."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:5
msgid "The :math:`k` th order, default is 2."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:7
msgid "The :math:`k` th order of Taylor expansion of :math:`ln(x+1)`."
msgstr ""

#: of tensorcircuit.quantum.tn2qop:1
msgid "Convert MPO in TensorNetwork package to QuOperator."
msgstr ""

#: of tensorcircuit.quantum.tn2qop:3
msgid "MPO in the form of TensorNetwork package"
msgstr ""

#: of tensorcircuit.quantum.trace_distance:1
msgid ""
"Compute the trace distance between two density matrix ``rho`` and "
"``rho2``."
msgstr ""

#: of tensorcircuit.quantum.trace_distance:7
msgid "Epsilon, defaults to 1e-12"
msgstr ""

#: of tensorcircuit.quantum.trace_distance:9
msgid "The trace distance between two density matrix ``rho`` and ``rho2``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:1
msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:3
msgid "\\operatorname{Tr}(\\prod_i O_i)"
msgstr ""

#: of tensorcircuit.quantum.trace_product:22
msgid "The trace of several inputs."
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:1
msgid "Compute the truncated free energy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:9
msgid "The :math:`k` th order, defaults to 2"
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:11
msgid "The :math:`k` th order of the truncated free energy."
msgstr ""

#: ../../source/api/results.rst:2
msgid "tensorcircuit.results"
msgstr ""

#: ../../source/api/results/counts.rst:2
msgid "tensorcircuit.results.counts"
msgstr ""

#: of tensorcircuit.results.counts:1
msgid "dict related functionalities"
msgstr ""

#: of tensorcircuit.results.counts.expectation:1
msgid ""
"compute diagonal operator expectation value from bit string count "
"dictionary"
msgstr ""

#: of tensorcircuit.results.counts.expectation:3
msgid "count dict for bitstring histogram"
msgstr ""

#: of tensorcircuit.results.counts.expectation:5
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:5
msgid ""
"if defaults as None, then ``diagonal_op`` must be set a list of qubit "
"that we measure Z op on"
msgstr ""

#: of tensorcircuit.results.counts.expectation:8
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:8
msgid ""
"shape [n, 2], explicitly indicate the diagonal op on each qubit eg. [1, "
"-1] for z [1, 1] for I, etc."
msgstr ""

#: of tensorcircuit.results.counts.expectation:11
msgid "the expectation value"
msgstr ""

#: ../../source/api/results/readout_mitigation.rst:2
msgid "tensorcircuit.results.readout_mitigation"
msgstr ""

#: of tensorcircuit.results.readout_mitigation:1
msgid "readout error mitigation functionalities"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.__init__:1
msgid "The Class for readout error mitigation"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.__init__:3
msgid "execute function to run the cirucit"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.__init__:5
msgid "iteration threshold, defaults to 4096"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:1
msgid "Main readout mitigation program for all methods."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:3
msgid "raw count"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:5
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:6
msgid "user-defined logical qubits to show final mitted results"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:7
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:11
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:8
msgid "positional_logical_mapping, defaults to None."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:9
#: tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:13
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:10
msgid "logical_physical_mapping, defaults to None"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:11
msgid "defaults to None"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:13
msgid "mitigation method, defaults to \"square\""
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:15
msgid "defaults to 25"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:17
msgid "defaults to 1e-5"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.apply_correction:19
msgid ""
":param return_mitigation_overhead:defaults to False :type "
"return_mitigation_overhead: bool, optional :param details: defaults to "
"False :type details: bool, optional :return: mitigated count :rtype: ct"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation:1
msgid "Main readout mitigation program for method=\"inverse\" or \"square\""
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation:3
msgid "the raw count"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation:5
msgid "mitigation method, defaults to \"inverse\""
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.apply_readout_mitigation:7
msgid "mitigated count"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_api:1
msgid "Get local calibriation matrix from cloud API from tc supported providers"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_api:3
msgid "list of physical qubits to be calibriated"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_api:5
msgid "the device str to qurey for the info, defaults to None"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_system:1
msgid "Get calibrattion information from system."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_system:3
msgid "calibration qubit list (physical qubits on device)"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_system:5
msgid "shots used for runing the circuit, defaults to 8192"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.cals_from_system:7
msgid "calibration method, defaults to \"local\", it can also be \"global\""
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:1
msgid "Calculate expectation value after readout error mitigation"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:3
msgid "raw counts"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:15
msgid "readout mitigation method, defaults to \"constrained_least_square\""
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.expectation:17
msgid "expectation value after readout error mitigation"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix:1
msgid "Calculate cal_matrix according to use qubit list."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix:3
msgid "used qubit list, defaults to None"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.get_matrix:5
msgid "cal_matrix"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.global_miti_readout_circ:1
msgid "Generate circuits for global calibration."
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.global_miti_readout_circ:3
#: tensorcircuit.results.readout_mitigation.ReadoutMit.local_miti_readout_circ:3
msgid "circuit list"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.local_miti_readout_circ:1
msgid "Generate circuits for local calibration."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:1
msgid ""
"Preprocessing to deal with qubit mapping, including "
"positional_logical_mapping and logical_physical_mapping. Return "
"self.use_qubits(physical) and corresponding counts."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:4
msgid "raw_counts on positional_qubits"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.mapping_preprocess:12
msgid "counts on self.use_qubit(physical)"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability:1
msgid "Get the mitigated probability."
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability:3
msgid "probability of raw count"
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability:5
msgid "mitigation methods, defaults to \"inverse\", it can also be \"square\""
msgstr ""

#: of
#: tensorcircuit.results.readout_mitigation.ReadoutMit.mitigate_probability:7
msgid "mitigated probability"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.newrange:1
msgid "Rerange the order according to used qubit list."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.newrange:3
#: tensorcircuit.results.readout_mitigation.ReadoutMit.ubs:3
msgid "index"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.newrange:5
#: tensorcircuit.results.readout_mitigation.ReadoutMit.ubs:5
msgid "used qubit list"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.newrange:7
msgid "new index"
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.ubs:1
msgid "Help omit calibration results that not in used qubit list."
msgstr ""

#: of tensorcircuit.results.readout_mitigation.ReadoutMit.ubs:7
msgid "omitation related value"
msgstr ""

#: ../../source/api/simplify.rst:2
msgid "tensorcircuit.simplify"
msgstr ""

#: of tensorcircuit.simplify:1
msgid "Tensornetwork Simplification"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:1
msgid ""
"Get the new shape of two nodes, also supporting to return original shapes"
" of two nodes."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:13
msgid "node one"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:15
msgid "node two"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:17
msgid "Whether to include original shape of two nodes, default is True."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:19
msgid "The new shape of the two nodes."
msgstr ""

#: of tensorcircuit.simplify.pseudo_contract_between:1
msgid ""
"Contract between Node ``a`` and ``b``, with correct shape only and no "
"calculation"
msgstr ""

#: ../../source/api/templates.rst:2
msgid "tensorcircuit.templates"
msgstr ""

#: ../../source/api/templates/blocks.rst:2
msgid "tensorcircuit.templates.blocks"
msgstr ""

#: of tensorcircuit.templates.blocks:1 tensorcircuit.templates.measurements:1
msgid "Shortcuts for measurement patterns on circuit"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:1
msgid ""
"For each pair in links, the input product state |00> is transformed as "
"(01>-|10>)"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:3
#: tensorcircuit.templates.blocks.qft:3
msgid "Circuit in"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:5
msgid ""
"pairs indices for Bell pairs, defaults to None, corresponds to neighbor "
"links"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:7
msgid "Circuit out"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:1
msgid ""
"The circuit ansatz is firstly one layer of Hadamard gates and then we "
"have ``nlayers`` blocks of :math:`e^{i\\theta Z_iZ_{i+1}}` two-qubit gate"
" in ladder layout, following rx gate."
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:5
msgid "The circuit"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:7
msgid "paramter tensor with 2*nlayer*n elements"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:9
msgid "number of ZZ+RX blocks, defaults to 2"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:11
msgid "whether use SVD split to reduce ZZ gate bond dimension, defaults to False"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:14
msgid "The circuit with example ansatz attached"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:1
msgid ""
"This function applies quantum fourier transformation (QFT) to the "
"selected circuit lines"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:5
msgid "the indices of the circuit lines to apply QFT"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:7
msgid "Whether to include the final swaps in the QFT"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:9
msgid "If True, the inverse Fourier transform is constructed"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:11
msgid "If True, barriers are inserted as visualization improvement"
msgstr ""

#: of tensorcircuit.templates.blocks.qft:13
msgid "Circuit c"
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:1
msgid ""
"Function decorator wraps the function with the first input and output in "
"the format of circuit, the wrapped function has the first input and the "
"output as the state tensor."
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:4
msgid "Function with the fist input and the output as ``Circuit`` object."
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:6
msgid ""
"Wrapped function with the first input and the output as the state tensor "
"correspondingly."
msgstr ""

#: ../../source/api/templates/chems.rst:2
msgid "tensorcircuit.templates.chems"
msgstr ""

#: of tensorcircuit.templates.chems:1
msgid "Useful utilities for quantum chemistry related task"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:1
msgid ""
"Get Pauli string array and weights array for a qubit Hamiltonian as a sum"
" of Pauli strings defined in openfermion ``QubitOperator``."
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:4
msgid "``openfermion.ops.operators.qubit_operator.QubitOperator``"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:6
msgid "The number of qubits"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:8
msgid "Pauli String array and weights array"
msgstr ""

#: ../../source/api/templates/dataset.rst:2
msgid "tensorcircuit.templates.dataset"
msgstr ""

#: of tensorcircuit.templates.dataset:1
msgid "Quantum machine learning related data preprocessing and embedding"
msgstr ""

#: ../../source/api/templates/graphs.rst:2
msgid "tensorcircuit.templates.graphs"
msgstr ""

#: of tensorcircuit.templates.graphs:1
msgid "Some common graphs and lattices"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Two-dimensional grid lattice"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.__init__:1
msgid "number of rows"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.__init__:3
msgid "number of cols"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:1
msgid "return all col edge with 1d index encoding"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:3
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows:3
msgid ""
"whether to include pbc edges (periodic boundary condition), defaults to "
"False"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:6
msgid "list of col edge"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_rows:1
msgid "return all row edge with 1d index encoding"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_rows:6
msgid "list of row edge"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:1
msgid "Get the 2D grid lattice in ``nx.Graph`` format"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:3
msgid ""
"whether to include pbc edges (periodic boundary condition), defaults to "
"True"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:1
msgid "1D chain with ``n`` sites"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:5
#: tensorcircuit.templates.measurements.heisenberg_measurements:34
msgid "[description], defaults to True"
msgstr ""

#: ../../source/api/templates/measurements.rst:2
msgid "tensorcircuit.templates.measurements"
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:1
msgid ""
"This measurements pattern is specifically suitable for vmap. Parameterize"
" the local Pauli string to be measured."
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:19
#: tensorcircuit.templates.measurements.any_measurements:26
msgid "The circuit to be measured"
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:21
#: tensorcircuit.templates.measurements.any_measurements:28
msgid ""
"parameter tensors determines what Pauli string to be measured, shape is "
"[nwires, 4] if ``onehot`` is False and [nwires] if ``onehot`` is True."
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:24
#: tensorcircuit.templates.measurements.any_measurements:31
msgid ""
"defaults to False. If set to be True, structures will first go through "
"onehot procedure."
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:27
msgid ""
"reuse the wavefunction when computing the expectations, defaults to be "
"True"
msgstr ""

#: of tensorcircuit.templates.measurements.any_local_measurements:29
#: tensorcircuit.templates.measurements.any_measurements:36
msgid "The expectation value of given Pauli string by the tensor ``structures``."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:1
msgid ""
"This measurements pattern is specifically suitable for vmap. Parameterize"
" the Pauli string to be measured."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:34
msgid ""
"reuse the wavefunction when computing the expectations, defaults to be "
"False"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:1
msgid ""
"Evaluate Heisenberg energy expectation, whose Hamiltonian is defined on "
"the lattice graph ``g`` as follows: (e are edges in graph ``g`` where e1 "
"and e2 are two nodes for edge e and v are nodes in graph ``g``)"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:4
msgid ""
"H = \\sum_{e\\in g} w_e (h_{xx} X_{e1}X_{e2} + h_{yy} Y_{e1}Y_{e2} + "
"h_{zz} Z_{e1}Z_{e2})\n"
" + \\sum_{v\\in g} (h_x X_v + h_y Y_v + h_z Z_v)"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:18
msgid "Circuit to be measured"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:20
msgid "Lattice graph defining Heisenberg Hamiltonian"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:22
#: tensorcircuit.templates.measurements.heisenberg_measurements:24
#: tensorcircuit.templates.measurements.heisenberg_measurements:26
msgid "[description], defaults to 1.0"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:28
#: tensorcircuit.templates.measurements.heisenberg_measurements:30
#: tensorcircuit.templates.measurements.heisenberg_measurements:32
msgid "[description], defaults to 0.0"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:36
msgid "Value of Heisenberg energy"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:1
msgid ""
"Evaluate expectation of operator ``mpo`` defined in ``QuOperator`` MPO "
"format with the output quantum state from circuit ``c``."
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:4
msgid "The circuit for the output state"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:6
msgid "MPO operator"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:8
#: tensorcircuit.templates.measurements.operator_expectation:7
#: tensorcircuit.templates.measurements.sparse_expectation:7
msgid "a real and scalar tensor of shape [] as the expectation value"
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:1
msgid ""
"Evaluate Hamiltonian expectation where ``hamiltonian`` can be dense "
"matrix, sparse matrix or MPO."
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:3
#: tensorcircuit.templates.measurements.sparse_expectation:3
msgid "The circuit whose output state is used to evaluate the expectation"
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:5
#: tensorcircuit.templates.measurements.sparse_expectation:5
msgid "Hamiltonian matrix in COO_sparse_matrix form"
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:1
msgid ""
"Evaluate Hamiltonian expectation where ``hamiltonian`` is kept in sparse "
"matrix form to save space"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:1
msgid ""
"Compute spin glass energy defined on graph ``g`` expectation for output "
"state of the circuit ``c``. The Hamiltonian to be evaluated is defined as"
" (first term is determined by node weights while the second term is "
"determined by edge weights of the graph):"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:5
msgid "H = \\sum_{v\\in g} w_v Z_v + \\sum_{e\\in g} w_e Z_{e1} Z_{e2}"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:28
msgid "The quantum circuit"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:30
msgid "The graph for spin glass Hamiltonian definition"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:32
msgid ""
"Whether measure the circuit with reusing the wavefunction, defaults to "
"True"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:34
msgid "The spin glass energy expectation value"
msgstr ""

#: ../../source/api/torchnn.rst:2
msgid "tensorcircuit.torchnn"
msgstr ""

#: of tensorcircuit.torchnn:1
msgid "PyTorch nn Module wrapper for quantum function"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet:1
msgid "Bases: :py:class:`torch.nn.modules.module.Module`"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:1
msgid "PyTorch nn Module wrapper on quantum function ``f``."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:32
msgid "Quantum function with tensor in (input and weights) and tensor out."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:34
msgid ""
"list of shape tuple for different weights as the non-first parameters for"
" ``f``"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:36
msgid "function that gives the shape tuple returns torch tensor, defaults to None"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:38
msgid "whether apply vmap (batch input) on ``f``, defaults to True"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:40
msgid ""
"which position of input should be batched, need to be customized when "
"multiple inputs for the torch model, defaults to be 0."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:43
msgid "whether transform ``f`` with torch interface, defaults to True"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:45
msgid "whether jit ``f``, defaults to True"
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.__init__:47
msgid "whether enbale dlpack in interfaces, defaults to False"
msgstr ""

#: of torch.nn.modules.module.Module.add_module:1
msgid "Adds a child module to the current module."
msgstr ""

#: of torch.nn.modules.module.Module.add_module:3
msgid "The module can be accessed as an attribute using the given name."
msgstr ""

#: of torch.nn.modules.module.Module.add_module:5
msgid ""
"name of the child module. The child module can be accessed from this "
"module using the given name"
msgstr ""

#: of torch.nn.modules.module.Module.add_module:8
msgid "child module to be added to the module."
msgstr ""

#: of torch.nn.modules.module.Module.apply:1
msgid ""
"Applies ``fn`` recursively to every submodule (as returned by "
"``.children()``) as well as self. Typical use includes initializing the "
"parameters of a model (see also :ref:`nn-init-doc`)."
msgstr ""

#: of torch.nn.modules.module.Module.apply:5
msgid "function to be applied to each submodule"
msgstr ""

#: of torch.nn.modules.module.Module.apply:8
#: torch.nn.modules.module.Module.bfloat16:6
#: torch.nn.modules.module.Module.cpu:6 torch.nn.modules.module.Module.cuda:14
#: torch.nn.modules.module.Module.double:6
#: torch.nn.modules.module.Module.eval:13
#: torch.nn.modules.module.Module.float:6 torch.nn.modules.module.Module.half:6
#: torch.nn.modules.module.Module.requires_grad_:17
#: torch.nn.modules.module.Module.to:45
#: torch.nn.modules.module.Module.to_empty:7
#: torch.nn.modules.module.Module.train:12
#: torch.nn.modules.module.Module.type:9 torch.nn.modules.module.Module.xpu:14
msgid "self"
msgstr ""

#: of torch.nn.modules.module.Module.apply:11
#: torch.nn.modules.module.Module.buffers:10
#: torch.nn.modules.module.Module.modules:10
#: torch.nn.modules.module.Module.named_buffers:13
#: torch.nn.modules.module.Module.named_children:6
#: torch.nn.modules.module.Module.named_modules:16
#: torch.nn.modules.module.Module.named_parameters:13
#: torch.nn.modules.module.Module.parameters:12
#: torch.nn.modules.module.Module.register_buffer:25
#: torch.nn.modules.module.Module.state_dict:10
msgid "Example::"
msgstr ""

#: of torch.nn.modules.module.Module.bfloat16:1
msgid "Casts all floating point parameters and buffers to ``bfloat16`` datatype."
msgstr ""

#: of torch.nn.modules.module.Module.bfloat16:4
#: torch.nn.modules.module.Module.cpu:4 torch.nn.modules.module.Module.cuda:8
#: torch.nn.modules.module.Module.double:4
#: torch.nn.modules.module.Module.float:4 torch.nn.modules.module.Module.half:4
#: torch.nn.modules.module.Module.to:29 torch.nn.modules.module.Module.type:4
#: torch.nn.modules.module.Module.xpu:8
msgid "This method modifies the module in-place."
msgstr ""

#: of torch.nn.modules.module.Module.buffers:1
msgid "Returns an iterator over module buffers."
msgstr ""

#: of torch.nn.modules.module.Module.buffers:3
#: torch.nn.modules.module.Module.named_buffers:6
msgid ""
"if True, then yields buffers of this module and all submodules. "
"Otherwise, yields only buffers that are direct members of this module."
msgstr ""

#: of torch.nn.modules.module.Module.buffers
#: torch.nn.modules.module.Module.children
#: torch.nn.modules.module.Module.modules
#: torch.nn.modules.module.Module.named_buffers
#: torch.nn.modules.module.Module.named_children
#: torch.nn.modules.module.Module.named_modules
#: torch.nn.modules.module.Module.named_parameters
#: torch.nn.modules.module.Module.parameters
msgid "Yields"
msgstr ""

#: of torch.nn.modules.module.Module.buffers:8
msgid "*torch.Tensor* -- module buffer"
msgstr ""

#: of torch.nn.modules.module.Module.children:1
msgid "Returns an iterator over immediate children modules."
msgstr ""

#: of torch.nn.modules.module.Module.children:3
msgid "*Module* -- a child module"
msgstr ""

#: of torch.nn.modules.module.Module.cpu:1
msgid "Moves all model parameters and buffers to the CPU."
msgstr ""

#: of torch.nn.modules.module.Module.cuda:1
msgid "Moves all model parameters and buffers to the GPU."
msgstr ""

#: of torch.nn.modules.module.Module.cuda:3
msgid ""
"This also makes associated parameters and buffers different objects. So "
"it should be called before constructing optimizer if the module will live"
" on GPU while being optimized."
msgstr ""

#: of torch.nn.modules.module.Module.cuda:10
#: torch.nn.modules.module.Module.xpu:10
msgid "if specified, all parameters will be copied to that device"
msgstr ""

#: of torch.nn.modules.module.Module.double:1
msgid "Casts all floating point parameters and buffers to ``double`` datatype."
msgstr ""

#: ../../docstring of tensorcircuit.torchnn.QuantumNet.dump_patches:1
msgid ""
"This allows better BC support for :meth:`load_state_dict`. In "
":meth:`state_dict`, the version number will be saved as in the attribute "
"`_metadata` of the returned state dict, and thus pickled. `_metadata` is "
"a dictionary with keys that follow the naming convention of state dict. "
"See ``_load_from_state_dict`` on how to use this information in loading."
msgstr ""

#: ../../docstring of tensorcircuit.torchnn.QuantumNet.dump_patches:7
msgid ""
"If new parameters/buffers are added/removed from a module, this number "
"shall be bumped, and the module's `_load_from_state_dict` method can "
"compare the version number and do appropriate changes if the state dict "
"is from before the change."
msgstr ""

#: of torch.nn.modules.module.Module.eval:1
msgid "Sets the module in evaluation mode."
msgstr ""

#: of torch.nn.modules.module.Module.eval:3
#: torch.nn.modules.module.Module.train:3
msgid ""
"This has any effect only on certain modules. See documentations of "
"particular modules for details of their behaviors in training/evaluation "
"mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, "
"etc."
msgstr ""

#: of torch.nn.modules.module.Module.eval:8
msgid "This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`."
msgstr ""

#: of torch.nn.modules.module.Module.eval:10
msgid ""
"See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` "
"and several similar mechanisms that may be confused with it."
msgstr ""

#: of torch.nn.modules.module.Module.extra_repr:1
msgid "Set the extra representation of the module"
msgstr ""

#: of torch.nn.modules.module.Module.extra_repr:3
msgid ""
"To print customized extra information, you should re-implement this "
"method in your own modules. Both single-line and multi-line strings are "
"acceptable."
msgstr ""

#: of torch.nn.modules.module.Module.float:1
msgid "Casts all floating point parameters and buffers to ``float`` datatype."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.forward:1
msgid "Defines the computation performed at every call."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.forward:3
msgid "Should be overridden by all subclasses."
msgstr ""

#: of tensorcircuit.torchnn.QuantumNet.forward:6
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the registered hooks "
"while the latter silently ignores them."
msgstr ""

#: of torch.nn.modules.module.Module.get_buffer:1
msgid ""
"Returns the buffer given by ``target`` if it exists, otherwise throws an "
"error."
msgstr ""

#: of torch.nn.modules.module.Module.get_buffer:4
#: torch.nn.modules.module.Module.get_parameter:4
msgid ""
"See the docstring for ``get_submodule`` for a more detailed explanation "
"of this method's functionality as well as how to correctly specify "
"``target``."
msgstr ""

#: of torch.nn.modules.module.Module.get_buffer:8
msgid ""
"The fully-qualified string name of the buffer to look for. (See "
"``get_submodule`` for how to specify a fully-qualified string.)"
msgstr ""

#: of torch.nn.modules.module.Module.get_buffer:12
msgid "The buffer referenced by ``target``"
msgstr ""

#: of torch.nn.modules.module.Module.get_buffer:15
msgid ""
"If the target string references an invalid     path or resolves to "
"something that is not a     buffer"
msgstr ""

#: of torch.nn.modules.module.Module.get_extra_state:1
msgid ""
"Returns any extra state to include in the module's state_dict. Implement "
"this and a corresponding :func:`set_extra_state` for your module if you "
"need to store extra state. This function is called when building the "
"module's `state_dict()`."
msgstr ""

#: of torch.nn.modules.module.Module.get_extra_state:6
msgid ""
"Note that extra state should be pickleable to ensure working "
"serialization of the state_dict. We only provide provide backwards "
"compatibility guarantees for serializing Tensors; other objects may break"
" backwards compatibility if their serialized pickled form changes."
msgstr ""

#: of torch.nn.modules.module.Module.get_extra_state:11
msgid "Any extra state to store in the module's state_dict"
msgstr ""

#: of torch.nn.modules.module.Module.get_parameter:1
msgid ""
"Returns the parameter given by ``target`` if it exists, otherwise throws "
"an error."
msgstr ""

#: of torch.nn.modules.module.Module.get_parameter:8
msgid ""
"The fully-qualified string name of the Parameter to look for. (See "
"``get_submodule`` for how to specify a fully-qualified string.)"
msgstr ""

#: of torch.nn.modules.module.Module.get_parameter:12
msgid "The Parameter referenced by ``target``"
msgstr ""

#: of torch.nn.modules.module.Module.get_parameter:15
msgid ""
"If the target string references an invalid     path or resolves to "
"something that is not an     ``nn.Parameter``"
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:1
msgid ""
"Returns the submodule given by ``target`` if it exists, otherwise throws "
"an error."
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:4
msgid ""
"For example, let's say you have an ``nn.Module`` ``A`` that looks like "
"this:"
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:18
msgid ""
"(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule "
"``net_b``, which itself has two submodules ``net_c`` and ``linear``. "
"``net_c`` then has a submodule ``conv``.)"
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:22
msgid ""
"To check whether or not we have the ``linear`` submodule, we would call "
"``get_submodule(\"net_b.linear\")``. To check whether we have the "
"``conv`` submodule, we would call "
"``get_submodule(\"net_b.net_c.conv\")``."
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:27
msgid ""
"The runtime of ``get_submodule`` is bounded by the degree of module "
"nesting in ``target``. A query against ``named_modules`` achieves the "
"same result, but it is O(N) in the number of transitive modules. So, for "
"a simple check to see if some submodule exists, ``get_submodule`` should "
"always be used."
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:34
msgid ""
"The fully-qualified string name of the submodule to look for. (See above "
"example for how to specify a fully-qualified string.)"
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:38
msgid "The submodule referenced by ``target``"
msgstr ""

#: of torch.nn.modules.module.Module.get_submodule:41
msgid ""
"If the target string references an invalid     path or resolves to "
"something that is not an     ``nn.Module``"
msgstr ""

#: of torch.nn.modules.module.Module.half:1
msgid "Casts all floating point parameters and buffers to ``half`` datatype."
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:1
msgid ""
"Copies parameters and buffers from :attr:`state_dict` into this module "
"and its descendants. If :attr:`strict` is ``True``, then the keys of "
":attr:`state_dict` must exactly match the keys returned by this module's "
":meth:`~torch.nn.Module.state_dict` function."
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:6
msgid "a dict containing parameters and persistent buffers."
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:9
msgid ""
"whether to strictly enforce that the keys in :attr:`state_dict` match the"
" keys returned by this module's :meth:`~torch.nn.Module.state_dict` "
"function. Default: ``True``"
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:14
msgid ""
"* **missing_keys** is a list of str containing the missing keys * "
"**unexpected_keys** is a list of str containing the unexpected keys"
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:14
msgid "**missing_keys** is a list of str containing the missing keys"
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:15
msgid "**unexpected_keys** is a list of str containing the unexpected keys"
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:16
msgid "``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields"
msgstr ""

#: of torch.nn.modules.module.Module.load_state_dict:20
msgid ""
"If a parameter or buffer is registered as ``None`` and its corresponding "
"key exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a "
"``RuntimeError``."
msgstr ""

#: of torch.nn.modules.module.Module.modules:1
msgid "Returns an iterator over all modules in the network."
msgstr ""

#: of torch.nn.modules.module.Module.modules:3
msgid "*Module* -- a module in the network"
msgstr ""

#: of torch.nn.modules.module.Module.modules:7
#: torch.nn.modules.module.Module.named_modules:13
msgid ""
"Duplicate modules are returned only once. In the following example, ``l``"
" will be returned only once."
msgstr ""

#: of torch.nn.modules.module.Module.named_buffers:1
msgid ""
"Returns an iterator over module buffers, yielding both the name of the "
"buffer as well as the buffer itself."
msgstr ""

#: of torch.nn.modules.module.Module.named_buffers:4
msgid "prefix to prepend to all buffer names."
msgstr ""

#: of torch.nn.modules.module.Module.named_buffers:11
msgid "*(string, torch.Tensor)* -- Tuple containing the name and buffer"
msgstr ""

#: of torch.nn.modules.module.Module.named_children:1
msgid ""
"Returns an iterator over immediate children modules, yielding both the "
"name of the module as well as the module itself."
msgstr ""

#: of torch.nn.modules.module.Module.named_children:4
msgid "*(string, Module)* -- Tuple containing a name and child module"
msgstr ""

#: of torch.nn.modules.module.Module.named_modules:1
msgid ""
"Returns an iterator over all modules in the network, yielding both the "
"name of the module as well as the module itself."
msgstr ""

#: of torch.nn.modules.module.Module.named_modules:4
msgid "a memo to store the set of modules already added to the result"
msgstr ""

#: of torch.nn.modules.module.Module.named_modules:5
msgid "a prefix that will be added to the name of the module"
msgstr ""

#: of torch.nn.modules.module.Module.named_modules:6
msgid "whether to remove the duplicated module instances in the result or not"
msgstr ""

#: of torch.nn.modules.module.Module.named_modules:9
msgid "*(string, Module)* -- Tuple of name and module"
msgstr ""

#: of torch.nn.modules.module.Module.named_parameters:1
msgid ""
"Returns an iterator over module parameters, yielding both the name of the"
" parameter as well as the parameter itself."
msgstr ""

#: of torch.nn.modules.module.Module.named_parameters:4
msgid "prefix to prepend to all parameter names."
msgstr ""

#: of torch.nn.modules.module.Module.named_parameters:6
#: torch.nn.modules.module.Module.parameters:5
msgid ""
"if True, then yields parameters of this module and all submodules. "
"Otherwise, yields only parameters that are direct members of this module."
msgstr ""

#: of torch.nn.modules.module.Module.named_parameters:11
msgid "*(string, Parameter)* -- Tuple containing the name and parameter"
msgstr ""

#: of torch.nn.modules.module.Module.parameters:1
msgid "Returns an iterator over module parameters."
msgstr ""

#: of torch.nn.modules.module.Module.parameters:3
msgid "This is typically passed to an optimizer."
msgstr ""

#: of torch.nn.modules.module.Module.parameters:10
msgid "*Parameter* -- module parameter"
msgstr ""

#: of torch.nn.modules.module.Module.register_backward_hook:1
#: torch.nn.modules.module.Module.register_full_backward_hook:1
msgid "Registers a backward hook on the module."
msgstr ""

#: of torch.nn.modules.module.Module.register_backward_hook:3
msgid ""
"This function is deprecated in favor of "
":meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of "
"this function will change in future versions."
msgstr ""

#: of torch.nn.modules.module.Module.register_backward_hook:6
#: torch.nn.modules.module.Module.register_forward_hook:14
#: torch.nn.modules.module.Module.register_forward_pre_hook:14
#: torch.nn.modules.module.Module.register_full_backward_hook:25
msgid ""
"a handle that can be used to remove the added hook by calling "
"``handle.remove()``"
msgstr ""

#: of torch.nn.modules.module.Module.register_backward_hook:8
#: torch.nn.modules.module.Module.register_forward_hook:16
#: torch.nn.modules.module.Module.register_forward_pre_hook:16
#: torch.nn.modules.module.Module.register_full_backward_hook:27
msgid ":class:`torch.utils.hooks.RemovableHandle`"
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:1
msgid "Adds a buffer to the module."
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:3
msgid ""
"This is typically used to register a buffer that should not to be "
"considered a model parameter. For example, BatchNorm's ``running_mean`` "
"is not a parameter, but is part of the module's state. Buffers, by "
"default, are persistent and will be saved alongside parameters. This "
"behavior can be changed by setting :attr:`persistent` to ``False``. The "
"only difference between a persistent buffer and a non-persistent buffer "
"is that the latter will not be a part of this module's "
":attr:`state_dict`."
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:12
msgid "Buffers can be accessed as attributes using given names."
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:14
msgid ""
"name of the buffer. The buffer can be accessed from this module using the"
" given name"
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:17
msgid ""
"buffer to be registered. If ``None``, then operations that run on "
"buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is "
"**not** included in the module's :attr:`state_dict`."
msgstr ""

#: of torch.nn.modules.module.Module.register_buffer:21
msgid "whether the buffer is part of this module's :attr:`state_dict`."
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_hook:1
msgid "Registers a forward hook on the module."
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_hook:3
msgid ""
"The hook will be called every time after :func:`forward` has computed an "
"output. It should have the following signature::"
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_hook:8
msgid ""
"The input contains only the positional arguments given to the module. "
"Keyword arguments won't be passed to the hooks and only to the "
"``forward``. The hook can modify the output. It can modify the input "
"inplace but it will not have effect on forward since this is called after"
" :func:`forward` is called."
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_pre_hook:1
msgid "Registers a forward pre-hook on the module."
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_pre_hook:3
msgid ""
"The hook will be called every time before :func:`forward` is invoked. It "
"should have the following signature::"
msgstr ""

#: of torch.nn.modules.module.Module.register_forward_pre_hook:8
msgid ""
"The input contains only the positional arguments given to the module. "
"Keyword arguments won't be passed to the hooks and only to the "
"``forward``. The hook can modify the input. User can either return a "
"tuple or a single modified value in the hook. We will wrap the value into"
" a tuple if a single value is returned(unless that value is already a "
"tuple)."
msgstr ""

#: of torch.nn.modules.module.Module.register_full_backward_hook:3
msgid ""
"The hook will be called every time the gradients with respect to module "
"inputs are computed. The hook should have the following signature::"
msgstr ""

#: of torch.nn.modules.module.Module.register_full_backward_hook:8
msgid ""
"The :attr:`grad_input` and :attr:`grad_output` are tuples that contain "
"the gradients with respect to the inputs and outputs respectively. The "
"hook should not modify its arguments, but it can optionally return a new "
"gradient with respect to the input that will be used in place of "
":attr:`grad_input` in subsequent computations. :attr:`grad_input` will "
"only correspond to the inputs given as positional arguments and all kwarg"
" arguments are ignored. Entries in :attr:`grad_input` and "
":attr:`grad_output` will be ``None`` for all non-Tensor arguments."
msgstr ""

#: of torch.nn.modules.module.Module.register_full_backward_hook:17
msgid ""
"For technical reasons, when this hook is applied to a Module, its forward"
" function will receive a view of each Tensor passed to the Module. "
"Similarly the caller will receive a view of each Tensor returned by the "
"Module's forward function."
msgstr ""

#: of torch.nn.modules.module.Module.register_full_backward_hook:22
msgid ""
"Modifying inputs or outputs inplace is not allowed when using backward "
"hooks and will raise an error."
msgstr ""

#: of torch.nn.modules.module.Module.register_module:1
msgid "Alias for :func:`add_module`."
msgstr ""

#: of torch.nn.modules.module.Module.register_parameter:1
msgid "Adds a parameter to the module."
msgstr ""

#: of torch.nn.modules.module.Module.register_parameter:3
msgid "The parameter can be accessed as an attribute using given name."
msgstr ""

#: of torch.nn.modules.module.Module.register_parameter:5
msgid ""
"name of the parameter. The parameter can be accessed from this module "
"using the given name"
msgstr ""

#: of torch.nn.modules.module.Module.register_parameter:8
msgid ""
"parameter to be added to the module. If ``None``, then operations that "
"run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the "
"parameter is **not** included in the module's :attr:`state_dict`."
msgstr ""

#: of torch.nn.modules.module.Module.requires_grad_:1
msgid "Change if autograd should record operations on parameters in this module."
msgstr ""

#: of torch.nn.modules.module.Module.requires_grad_:4
msgid ""
"This method sets the parameters' :attr:`requires_grad` attributes in-"
"place."
msgstr ""

#: of torch.nn.modules.module.Module.requires_grad_:7
msgid ""
"This method is helpful for freezing part of the module for finetuning or "
"training parts of a model individually (e.g., GAN training)."
msgstr ""

#: of torch.nn.modules.module.Module.requires_grad_:10
msgid ""
"See :ref:`locally-disable-grad-doc` for a comparison between "
"`.requires_grad_()` and several similar mechanisms that may be confused "
"with it."
msgstr ""

#: of torch.nn.modules.module.Module.requires_grad_:13
msgid ""
"whether autograd should record operations on parameters in this module. "
"Default: ``True``."
msgstr ""

#: of torch.nn.modules.module.Module.set_extra_state:1
msgid ""
"This function is called from :func:`load_state_dict` to handle any extra "
"state found within the `state_dict`. Implement this function and a "
"corresponding :func:`get_extra_state` for your module if you need to "
"store extra state within its `state_dict`."
msgstr ""

#: of torch.nn.modules.module.Module.set_extra_state:6
msgid "Extra state from the `state_dict`"
msgstr ""

#: of torch.nn.modules.module.Module.share_memory:1
msgid "See :meth:`torch.Tensor.share_memory_`"
msgstr ""

#: of torch.nn.modules.module.Module.state_dict:1
msgid "Returns a dictionary containing a whole state of the module."
msgstr ""

#: of torch.nn.modules.module.Module.state_dict:3
msgid ""
"Both parameters and persistent buffers (e.g. running averages) are "
"included. Keys are corresponding parameter and buffer names. Parameters "
"and buffers set to ``None`` are not included."
msgstr ""

#: of torch.nn.modules.module.Module.state_dict:7
msgid "a dictionary containing a whole state of the module"
msgstr ""

#: of torch.nn.modules.module.Module.to:1
msgid "Moves and/or casts the parameters and buffers."
msgstr ""

#: of torch.nn.modules.module.Module.to:3
msgid "This can be called as"
msgstr ""

#: of torch.nn.modules.module.Module.to:17
msgid ""
"Its signature is similar to :meth:`torch.Tensor.to`, but only accepts "
"floating point or complex :attr:`dtype`\\ s. In addition, this method "
"will only cast the floating point or complex parameters and buffers to "
":attr:`dtype` (if given). The integral parameters and buffers will be "
"moved :attr:`device`, if that is given, but with dtypes unchanged. When "
":attr:`non_blocking` is set, it tries to convert/move asynchronously with"
" respect to the host if possible, e.g., moving CPU Tensors with pinned "
"memory to CUDA devices."
msgstr ""

#: of torch.nn.modules.module.Module.to:26
msgid "See below for examples."
msgstr ""

#: of torch.nn.modules.module.Module.to:31
msgid "the desired device of the parameters and buffers in this module"
msgstr ""

#: of torch.nn.modules.module.Module.to:34
msgid ""
"the desired floating point or complex dtype of the parameters and buffers"
" in this module"
msgstr ""

#: of torch.nn.modules.module.Module.to:37
msgid ""
"Tensor whose dtype and device are the desired dtype and device for all "
"parameters and buffers in this module"
msgstr ""

#: of torch.nn.modules.module.Module.to:40
msgid ""
"the desired memory format for 4D parameters and buffers in this module "
"(keyword only argument)"
msgstr ""

#: of torch.nn.modules.module.Module.to:48
msgid "Examples::"
msgstr ""

#: of torch.nn.modules.module.Module.to_empty:1
msgid ""
"Moves the parameters and buffers to the specified device without copying "
"storage."
msgstr ""

#: of torch.nn.modules.module.Module.to_empty:3
msgid "The desired device of the parameters and buffers in this module."
msgstr ""

#: of torch.nn.modules.module.Module.train:1
msgid "Sets the module in training mode."
msgstr ""

#: of torch.nn.modules.module.Module.train:8
msgid ""
"whether to set training mode (``True``) or evaluation mode (``False``). "
"Default: ``True``."
msgstr ""

#: of torch.nn.modules.module.Module.type:1
msgid "Casts all parameters and buffers to :attr:`dst_type`."
msgstr ""

#: of torch.nn.modules.module.Module.type:6
msgid "the desired type"
msgstr ""

#: of torch.nn.modules.module.Module.xpu:1
msgid "Moves all model parameters and buffers to the XPU."
msgstr ""

#: of torch.nn.modules.module.Module.xpu:3
msgid ""
"This also makes associated parameters and buffers different objects. So "
"it should be called before constructing optimizer if the module will live"
" on XPU while being optimized."
msgstr ""

#: of torch.nn.modules.module.Module.zero_grad:1
msgid ""
"Sets gradients of all model parameters to zero. See similar function "
"under :class:`torch.optim.Optimizer` for more context."
msgstr ""

#: of torch.nn.modules.module.Module.zero_grad:4
msgid ""
"instead of setting to zero, set the grads to None. See "
":meth:`torch.optim.Optimizer.zero_grad` for details."
msgstr ""

#: ../../source/api/translation.rst:2
msgid "tensorcircuit.translation"
msgstr ""

#: of tensorcircuit.translation:1
msgid "Circuit object translation in different packages"
msgstr ""

#: of tensorcircuit.translation.eqasm2tc:1
msgid "Translation qexe/eqasm instruction to tensorcircuit Circuit object"
msgstr ""

#: of tensorcircuit.translation.eqasm2tc:7
msgid "lines of ignored code at the head and the tail, defaults to (6, 1)"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:1
msgid ""
"Generate a permutation matrix P. Due to the different convention or "
"qubits' order in qiskit and tensorcircuit, the unitary represented by the"
" same circuit is different. They are related by this permutation matrix "
"P: P @ U_qiskit @ P = U_tc"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:7
#: tensorcircuit.translation.qir2cirq:15
#: tensorcircuit.translation.qir2qiskit:16
#: tensorcircuit.translation.qiskit2tc:14 tensorcircuit.vis.qir2tex:12
msgid "# of qubits"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:9
msgid "The permutation matrix P"
msgstr ""

#: of tensorcircuit.translation.qir2cirq:1
msgid ""
"Generate a cirq circuit using the quantum intermediate representation "
"(qir) in tensorcircuit."
msgstr ""

#: of tensorcircuit.translation.qir2cirq:17
#: tensorcircuit.translation.qir2qiskit:18
msgid ""
"The extra quantum IR of tc circuit including measure and reset on "
"hardware, defaults to None"
msgstr ""

#: of tensorcircuit.translation.qir2cirq:20
msgid "qiskit cirq object"
msgstr ""

#: of tensorcircuit.translation.qir2cirq:23
msgid ""
"#TODO(@erertertet): add default theta to iswap gate add more cirq built-"
"in gate instead of customized add unitary test with tolerance add support"
" of cirq built-in ControlledGate for multiplecontroll support more "
"element in qir, e.g. barrier, measure..."
msgstr ""

#: of tensorcircuit.translation.qir2json:1
msgid ""
"transform qir to json compatible list of dict where array is replaced by "
"real and imaginary list"
msgstr ""

#: of tensorcircuit.translation.qir2qiskit:1
msgid ""
"Generate a qiskit quantum circuit using the quantum intermediate "
"representation (qir) in tensorcircuit."
msgstr ""

#: of tensorcircuit.translation.qir2qiskit:21
msgid "qiskit QuantumCircuit object"
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:1
msgid "Generate a tensorcircuit circuit using the quantum circuit data in qiskit."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:12
msgid "Quantum circuit data from qiskit."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:16
msgid "Input state of the circuit. Default is None."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:18
msgid "``Circuit``, ``DMCircuit`` or ``MPSCircuit``"
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:26
msgid "A quantum circuit in tensorcircuit"
msgstr ""

#: of tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure:1
msgid ""
"qiskit ``from_qasm_str`` method cannot keep the order of measure as the "
"qasm file, we provide this alternative function in case the order of "
"measure instruction matters"
msgstr ""

#: of tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure:4
msgid "open qasm str"
msgstr ""

#: of tensorcircuit.translation.qiskit_from_qasm_str_ordered_measure:6
msgid "``qiskit.circuit.QuantumCircuit``"
msgstr ""

#: ../../source/api/utils.rst:2
msgid "tensorcircuit.utils"
msgstr ""

#: of tensorcircuit.utils:1
msgid "Helper functions"
msgstr ""

#: of tensorcircuit.utils.append:1
msgid "Functional programming paradigm to build function pipeline"
msgstr ""

#: of tensorcircuit.utils.append:9
msgid "The function which are attached with other functions"
msgstr ""

#: of tensorcircuit.utils.append:11
msgid "Function to be attached"
msgstr ""

#: of tensorcircuit.utils.append:13
msgid "The final results after function pipeline"
msgstr ""

#: of tensorcircuit.utils.arg_alias:1
msgid "function argument alias decorator with new docstring"
msgstr ""

#: of tensorcircuit.utils.arg_alias:7
msgid "whether to add doc for these new alias arguments, defaults True"
msgstr ""

#: of tensorcircuit.utils.arg_alias:9
msgid "the decorated function"
msgstr ""

#: of tensorcircuit.utils.benchmark:1
msgid "benchmark jittable function with staging time and running time"
msgstr ""

#: of tensorcircuit.utils.benchmark:5
msgid "_description_, defaults to 5"
msgstr ""

#: of tensorcircuit.utils.is_m1mac:1
msgid "check whether the running platform is MAC with M1 chip"
msgstr ""

#: of tensorcircuit.utils.is_m1mac:3
msgid "True for MAC M1 platform"
msgstr ""

#: of tensorcircuit.utils.return_partial:1
msgid ""
"Return a callable function for output ith parts of the original output "
"along the first axis. Original output supports List and Tensor."
msgstr ""

#: of tensorcircuit.utils.return_partial:20
msgid "The function to be applied this method"
msgstr ""

#: of tensorcircuit.utils.return_partial:22
msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
msgstr ""

#: of tensorcircuit.utils.return_partial:24
msgid "The modified callable function"
msgstr ""

#: ../../source/api/vis.rst:2
msgid "tensorcircuit.vis"
msgstr ""

#: of tensorcircuit.vis:1
msgid "Visualization on circuits"
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:1
msgid ""
"Translating from the gate name to gate information including the number "
"of control qubits and the reduced gate name."
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:10
msgid "String of gate name"
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:12
msgid "# of control qubits, reduced gate name"
msgstr ""

#: of tensorcircuit.vis.qir2tex:1
msgid ""
"Generate Tex code from 'qir' string to illustrate the circuit structure. "
"This visualization is based on quantikz package."
msgstr ""

#: of tensorcircuit.vis.qir2tex:10
msgid "The quantum intermediate representation of a circuit in tensorcircuit."
msgstr ""

#: of tensorcircuit.vis.qir2tex:14
msgid "Initial state, default is an all zero state '000...000'."
msgstr ""

#: of tensorcircuit.vis.qir2tex:16
msgid ""
"Measurement Basis, default is None which means no measurement in the end "
"of the circuit."
msgstr ""

#: of tensorcircuit.vis.qir2tex:19
msgid ""
"If true, a right compression of the circuit will be conducted. A right "
"compression means we will try to shift gates from right to left if "
"possible."
msgstr ""

#: of tensorcircuit.vis.qir2tex:21
msgid ""
"Default is false. :type rcompress: bool :param lcompress: If true, a left"
" compression of the circuit will be conducted."
msgstr ""

#: of tensorcircuit.vis.qir2tex:24
msgid ""
"A left compression means we will try to shift gates from left to right if"
" possible. Default is false."
msgstr ""

#: of tensorcircuit.vis.qir2tex:27
msgid ""
"If true, the tex code will be designed to generate a standalone document."
" Default is false which means the generated tex code is just a quantikz "
"code block."
msgstr ""

#: of tensorcircuit.vis.qir2tex:30
msgid ""
"If true, a string table of tex code will also be returned. Default is "
"false."
msgstr ""

#: of tensorcircuit.vis.qir2tex:33
msgid ""
"Tex code of circuit visualization based on quantikz package. If "
"return_string_table is true, a string table of tex code will also be "
"returned."
msgstr ""

#: of tensorcircuit.vis.render_pdf:1
msgid ""
"Generate the PDF file with given latex string and filename. Latex command"
" and file path can be specified. When notebook is True, convert the "
"output PDF file to image and return a Image object."
msgstr ""

#: of tensorcircuit.vis.render_pdf:15
msgid "String of latex content"
msgstr ""

#: of tensorcircuit.vis.render_pdf:17
msgid "File name, defaults to random UUID `str(uuid4())`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:19
msgid "Executable Latex command, defaults to `pdflatex`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:21
msgid "File path, defaults to current working place `os.getcwd()`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:23
msgid "[description], defaults to False"
msgstr ""

#: of tensorcircuit.vis.render_pdf:25
msgid "if notebook is True, return `Image` object; otherwise return `None`"
msgstr ""

#~ msgid ""
#~ "This is a method that implementers "
#~ "of subclasses of `Layer` or `Model` "
#~ "can override if they need a "
#~ "state-creation step in-between layer "
#~ "instantiation and layer call."
#~ msgstr ""

#~ msgid "This is typically used to create the weights of `Layer` subclasses."
#~ msgstr ""

#~ msgid ""
#~ "Note here that `call()` method in "
#~ "`tf.keras` is little bit different from"
#~ " `keras` API. In `keras` API, you "
#~ "can pass support masking for layers "
#~ "as additional arguments. Whereas `tf.keras`"
#~ " has `compute_mask()` method to support "
#~ "masking."
#~ msgstr ""

#~ msgid "Modules for DQAS framework"
#~ msgstr ""

#~ msgid "DQAS framework entrypoint"
#~ msgstr ""

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid ""
#~ "function with input of data instance,"
#~ " circuit parameters theta and structural"
#~ " paramter k, return tuple of "
#~ "objective value and gradient with "
#~ "respect to theta"
#~ msgstr ""

#~ msgid "data generator as dataset"
#~ msgstr ""

#~ msgid "list of operations as primitive operator pool"
#~ msgstr ""

#~ msgid "the default layer number of the circuit ansatz"
#~ msgstr ""

#~ msgid ""
#~ "shape of circuit parameter pool, in "
#~ "general p_stp*l, where l is the "
#~ "max number of circuit parameters for "
#~ "op in the operator pool"
#~ msgstr ""

#~ msgid "the same as p in the most times"
#~ msgstr ""

#~ msgid "batch size of one epoch"
#~ msgstr ""

#~ msgid "prethermal update times"
#~ msgstr ""

#~ msgid "training epochs"
#~ msgstr ""

#~ msgid "parallel thread number, 0 to disable multiprocessing model by default"
#~ msgstr ""

#~ msgid "set verbose log to print"
#~ msgstr ""

#~ msgid "function to output verbose information"
#~ msgstr ""

#~ msgid "function return intermiediate result for final history list"
#~ msgstr ""

#~ msgid "cutoff probability to avoid peak distribution"
#~ msgstr ""

#~ msgid ""
#~ "function accepting list of objective "
#~ "values and return the baseline value "
#~ "used in the next round"
#~ msgstr ""

#~ msgid "return noise with the same shape as circuit parameter pool"
#~ msgstr ""

#~ msgid "initial values for circuit parameter pool"
#~ msgstr ""

#~ msgid "initial values for probabilistic model parameters"
#~ msgstr ""

#~ msgid "optimizer for circuit parameters theta"
#~ msgstr ""

#~ msgid "optimizer for model parameters alpha"
#~ msgstr ""

#~ msgid "optimizer for circuit parameters in prethermal stage"
#~ msgstr ""

#~ msgid "fixed structural parameters for prethermal training"
#~ msgstr ""

#~ msgid "regularization function for model parameters alpha"
#~ msgstr ""

#~ msgid "regularization function for circuit parameters theta"
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr ""

#~ msgid ""
#~ "The probabilistic model based DQAS, can"
#~ " use extensively for DQAS case for"
#~ " ``NMF`` probabilistic model."
#~ msgstr ""

#~ msgid "vag func, return loss and nabla lnp"
#~ msgstr ""

#~ msgid "keras model"
#~ msgstr ""

#~ msgid "sample func of logic with keras model input"
#~ msgstr ""

#~ msgid "input data pipeline generator"
#~ msgstr ""

#~ msgid "operation pool"
#~ msgstr ""

#~ msgid "depth for DQAS"
#~ msgstr ""

#~ msgid "parallel kernels"
#~ msgstr ""

#~ msgid "final loss function in terms of average of sub loss for each circuit"
#~ msgstr ""

#~ msgid "derivative function for ``loss_func``"
#~ msgstr ""

#~ msgid ""
#~ "Call in customized functions and grab"
#~ " variables within DQAS framework function"
#~ " by var name str."
#~ msgstr ""

#~ msgid "The DQAS framework function"
#~ msgstr ""

#~ msgid "Variables within the DQAS framework"
#~ msgstr ""

#~ msgid "Return type"
#~ msgstr ""

#~ msgid ""
#~ "This function works only when nnp "
#~ "has the same shape as stp, i.e."
#~ " one parameter for each op."
#~ msgstr ""

#~ msgid "The kernel for multiprocess to run parallel in DQAS function/"
#~ msgstr ""

#~ msgid ""
#~ "parallel variational parameter training and"
#~ " search to avoid local minimum not"
#~ " limited to qaoa setup as the "
#~ "function name indicates, as long as "
#~ "you provided suitable `vag_func`"
#~ msgstr ""

#~ msgid "data input generator for vag_func"
#~ msgstr ""

#~ msgid "vag_kernel"
#~ msgstr ""

#~ msgid "number of tries"
#~ msgstr ""

#~ msgid ""
#~ "for optimization problem the input is"
#~ " in general fixed so batch is "
#~ "often 1"
#~ msgstr ""

#~ msgid "number of parallel jobs"
#~ msgstr ""

#~ msgid "mean value of normal distribution for nnp"
#~ msgstr ""

#~ msgid "std deviation of normal distribution for nnp"
#~ msgstr ""

#~ msgid "Doesn't support prob model DQAS search."
#~ msgstr ""

#~ msgid "Modules for graph instance data and more"
#~ msgstr ""

#~ msgid "```python d = nx.to_dict_of_dicts(g) ```"
#~ msgstr ""

#~ msgid "1D PBC chain with n sites."
#~ msgstr ""

#~ msgid "The number of nodes"
#~ msgstr ""

#~ msgid "The resulted graph g"
#~ msgstr ""

#~ msgid "all graphs with m edge out from g"
#~ msgstr ""

#~ msgid ""
#~ "Generate a reduced graph with given "
#~ "ratio of edges compared to the "
#~ "original graph g."
#~ msgstr ""

#~ msgid "The base graph"
#~ msgstr ""

#~ msgid "number of edges kept, default half of the edges"
#~ msgstr ""

#~ msgid "The resulted reduced graph"
#~ msgstr ""

#~ msgid "Split the graph in exactly ``split`` piece evenly."
#~ msgstr ""

#~ msgid "The mother graph"
#~ msgstr ""

#~ msgid "The number of the graph we want to divide into, defaults to 2"
#~ msgstr ""

#~ msgid "List of graph instance of size ``split``"
#~ msgstr ""

#~ msgid "Module for functions adding layers of circuits"
#~ msgstr ""

#~ msgid "Hlayer"
#~ msgstr ""

#~ msgid "anyrxlayer"
#~ msgstr ""

#~ msgid "anyrylayer"
#~ msgstr ""

#~ msgid "anyrzlayer"
#~ msgstr ""

#~ msgid "anyswaplayer"
#~ msgstr ""

#~ msgid "anyxxlayer"
#~ msgstr ""

#~ msgid "anyxylayer"
#~ msgstr ""

#~ msgid "anyxzlayer"
#~ msgstr ""

#~ msgid "anyyxlayer"
#~ msgstr ""

#~ msgid "anyyylayer"
#~ msgstr ""

#~ msgid "anyyzlayer"
#~ msgstr ""

#~ msgid "anyzxlayer"
#~ msgstr ""

#~ msgid "anyzylayer"
#~ msgstr ""

#~ msgid "anyzzlayer"
#~ msgstr ""

#~ msgid "cnotlayer"
#~ msgstr ""

#~ msgid "rxlayer"
#~ msgstr ""

#~ msgid "rylayer"
#~ msgstr ""

#~ msgid "rzlayer"
#~ msgstr ""

#~ msgid "swaplayer"
#~ msgstr ""

#~ msgid "xxgate"
#~ msgstr ""

#~ msgid "xxlayer"
#~ msgstr ""

#~ msgid "xygate"
#~ msgstr ""

#~ msgid "xylayer"
#~ msgstr ""

#~ msgid "xzgate"
#~ msgstr ""

#~ msgid "xzlayer"
#~ msgstr ""

#~ msgid "yxgate"
#~ msgstr ""

#~ msgid "yxlayer"
#~ msgstr ""

#~ msgid "yygate"
#~ msgstr ""

#~ msgid "yylayer"
#~ msgstr ""

#~ msgid "yzgate"
#~ msgstr ""

#~ msgid "yzlayer"
#~ msgstr ""

#~ msgid "zxgate"
#~ msgstr ""

#~ msgid "zxlayer"
#~ msgstr ""

#~ msgid "zygate"
#~ msgstr ""

#~ msgid "zylayer"
#~ msgstr ""

#~ msgid "zzgate"
#~ msgstr ""

#~ msgid "zzlayer"
#~ msgstr ""

#~ msgid "$$e^{-i     heta_i \\sigma}$$"
#~ msgstr ""

#~ msgid ""
#~ "The following function should be used"
#~ " to generate layers with special "
#~ "case. As its soundness depends on "
#~ "the nature of the task or problem,"
#~ " it doesn't always make sense."
#~ msgstr ""

#~ msgid "$$e^{-i heta \\sigma}$$"
#~ msgstr ""

#~ msgid "$$e^{-i     heta \\sigma}$$"
#~ msgstr ""

#~ msgid ""
#~ "A collection of useful function snippets"
#~ " that irrelevant with the main "
#~ "modules or await for furthere refactor"
#~ msgstr ""

#~ msgid "Bases: :py:class:`object`"
#~ msgstr ""

#~ msgid ""
#~ "color cirq circuit SVG for given "
#~ "gates, a small tool to hack the"
#~ " cirq SVG"
#~ msgstr ""

#~ msgid "integer coordinate which gate is colored"
#~ msgstr ""

#~ msgid "transform repr form of an array to real numpy array"
#~ msgstr ""

#~ msgid "DQAS application kernels as vag functions"
#~ msgstr ""

#~ msgid "1D array for full wavefunction, the basis is in lexcical order"
#~ msgstr ""

#~ msgid "nx.Graph"
#~ msgstr ""

#~ msgid "transformation functions before averaged"
#~ msgstr ""

#~ msgid "as f3"
#~ msgstr ""

#~ msgid "maxcut energy for n qubit wavefunction i-th basis"
#~ msgstr ""

#~ msgid "ranged from 0 to 2**n-1"
#~ msgstr ""

#~ msgid "number of qubits"
#~ msgstr ""

#~ msgid ""
#~ "deprecated as non tf and non "
#~ "flexible, use the combination of "
#~ "``reduced_density_matrix`` and ``entropy`` instead."
#~ msgstr ""

#~ msgid "deprecated, current version in tc.quantum"
#~ msgstr ""

#~ msgid ""
#~ "value and gradient, currently only "
#~ "tensorflow backend is supported jax and"
#~ " numpy seems to be slow in "
#~ "circuit simulation anyhow. *deprecated*"
#~ msgstr ""

#~ msgid "if lbd=0, take energy as objective"
#~ msgstr ""

#~ msgid "if as default 0, overlap will not compute in the process"
#~ msgstr ""

#~ msgid "Fill single qubit gates according to placeholder on circuit"
#~ msgstr ""

#~ msgid "Hamiltonian measurements for Heisenberg model on graph lattice g"
#~ msgstr ""

#~ msgid "short cut for ``cirq.LineQubit(i)``"
#~ msgstr ""

#~ msgid "QAOA block encoding kernel, support 2 params in one op"
#~ msgstr ""

#~ msgid ""
#~ "training QAOA with only optimizing "
#~ "circuit parameters, can be well replaced"
#~ " with more general function `DQAS_search`"
#~ msgstr ""

#~ msgid "multi parameter for one layer"
#~ msgstr ""

#~ msgid "kw arguments for measurements_func"
#~ msgstr ""

#~ msgid "loss function, gradient of nnp"
#~ msgstr ""

#~ msgid ""
#~ "tensorflow quantum backend compare to "
#~ "qaoa_vag which is tensorcircuit backend"
#~ msgstr ""

#~ msgid "Hamiltonian for tfim on lattice defined by graph g"
#~ msgstr ""

#~ msgid "cirq.PauliSum as operators for tfq expectation layer"
#~ msgstr ""

#~ msgid ""
#~ "generate random wavefunction from "
#~ "approximately Haar measure, reference:  "
#~ "https://doi.org/10.1063/1.4983266"
#~ msgstr ""

#~ msgid "repetition of the blocks"
#~ msgstr ""

#~ msgid "random Haar measure approximation"
#~ msgstr ""

#~ msgid "cirq.Circuit, empty circuit"
#~ msgstr ""

#~ msgid "# of qubit"
#~ msgstr ""

#~ msgid ""
#~ "One-hot variational autoregressive models "
#~ "for multiple categorical choices beyond "
#~ "binary"
#~ msgstr ""

#~ msgid "Bases: :py:class:`keras.engine.training.Model`"
#~ msgstr ""

#~ msgid "Calls the model on new inputs and returns the outputs as tensors."
#~ msgstr ""

#~ msgid ""
#~ "In this case `call()` just reapplies "
#~ "all ops in the graph to the "
#~ "new inputs (e.g. build a new "
#~ "computational graph from the provided "
#~ "inputs)."
#~ msgstr ""

#~ msgid ""
#~ "Note: This method should not be "
#~ "called directly. It is only meant "
#~ "to be overridden when subclassing "
#~ "`tf.keras.Model`. To call a model on "
#~ "an input, always use the `__call__()`"
#~ " method, i.e. `model(inputs)`, which relies"
#~ " on the underlying `call()` method."
#~ msgstr ""

#~ msgid "Args:"
#~ msgstr ""

#~ msgid ""
#~ "inputs: Input tensor, or dict/list/tuple "
#~ "of input tensors. training: Boolean or"
#~ " boolean scalar tensor, indicating whether"
#~ " to run"
#~ msgstr ""

#~ msgid "the `Network` in training mode or inference mode."
#~ msgstr ""

#~ msgid "mask: A mask or list of masks. A mask can be either a boolean tensor or"
#~ msgstr ""

#~ msgid "None (no mask). For more details, check the guide"
#~ msgstr ""

#~ msgid "[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
#~ msgstr ""

#~ msgid "Returns:"
#~ msgstr ""

#~ msgid ""
#~ "A tensor if there is a single "
#~ "output, or a list of tensors if"
#~ " there are more than one outputs."
#~ msgstr ""

#~ msgid "Bases: :py:class:`keras.engine.base_layer.Layer`"
#~ msgstr ""

#~ msgid ""
#~ "Creates the variables of the layer "
#~ "(optional, for subclass implementers)."
#~ msgstr ""

#~ msgid ""
#~ "This is a method that implementers "
#~ "of subclasses of `Layer` or `Model` "
#~ "can override if they need a "
#~ "state-creation step in-between layer "
#~ "instantiation and layer call. It is "
#~ "invoked automatically before the first "
#~ "execution of `call()`."
#~ msgstr ""

#~ msgid ""
#~ "This is typically used to create "
#~ "the weights of `Layer` subclasses (at"
#~ " the discretion of the subclass "
#~ "implementer)."
#~ msgstr ""

#~ msgid "input_shape: Instance of `TensorShape`, or list of instances of"
#~ msgstr ""

#~ msgid ""
#~ "`TensorShape` if the layer expects a "
#~ "list of inputs (one instance per "
#~ "input)."
#~ msgstr ""

#~ msgid "This is where the layer's logic lives."
#~ msgstr ""

#~ msgid ""
#~ "The `call()` method may not create "
#~ "state (except in its first invocation,"
#~ " wrapping the creation of variables "
#~ "or other resources in `tf.init_scope()`). "
#~ "It is recommended to create state "
#~ "in `__init__()`, or the `build()` method"
#~ " that is called automatically before "
#~ "`call()` executes the first time."
#~ msgstr ""

#~ msgid "inputs: Input tensor, or dict/list/tuple of input tensors."
#~ msgstr ""

#~ msgid ""
#~ "The first positional `inputs` argument "
#~ "is subject to special rules: - "
#~ "`inputs` must be explicitly passed. A"
#~ " layer cannot have zero"
#~ msgstr ""

#~ msgid ""
#~ "arguments, and `inputs` cannot be "
#~ "provided via the default value of "
#~ "a keyword argument."
#~ msgstr ""

#~ msgid "NumPy array or Python scalar values in `inputs` get cast as tensors."
#~ msgstr ""

#~ msgid "Keras mask metadata is only collected from `inputs`."
#~ msgstr ""

#~ msgid ""
#~ "Layers are built (`build(input_shape)` method)"
#~ " using shape info from `inputs` only."
#~ msgstr ""

#~ msgid "`input_spec` compatibility is only checked against `inputs`."
#~ msgstr ""

#~ msgid ""
#~ "Mixed precision input casting is only"
#~ " applied to `inputs`. If a layer "
#~ "has tensor arguments in `*args` or "
#~ "`**kwargs`, their casting behavior in "
#~ "mixed precision should be handled "
#~ "manually."
#~ msgstr ""

#~ msgid "The SavedModel input specification is generated using `inputs` only."
#~ msgstr ""

#~ msgid ""
#~ "Integration with various ecosystem packages"
#~ " like TFMOT, TFLite, TF.js, etc is"
#~ " only supported for `inputs` and not"
#~ " for tensors in positional and "
#~ "keyword arguments."
#~ msgstr ""

#~ msgid "*args: Additional positional arguments. May contain tensors, although"
#~ msgstr ""

#~ msgid "this is not recommended, for the reasons above."
#~ msgstr ""

#~ msgid "**kwargs: Additional keyword arguments. May contain tensors, although"
#~ msgstr ""

#~ msgid ""
#~ "this is not recommended, for the "
#~ "reasons above. The following optional "
#~ "keyword arguments are reserved: - "
#~ "`training`: Boolean scalar tensor of "
#~ "Python boolean indicating"
#~ msgstr ""

#~ msgid "whether the `call` is meant for training or inference."
#~ msgstr ""

#~ msgid ""
#~ "`mask`: Boolean input mask. If the "
#~ "layer's `call()` method takes a `mask`"
#~ " argument, its default value will be"
#~ " set to the mask generated for "
#~ "`inputs` by the previous layer (if "
#~ "`input` did come from a layer that"
#~ " generated a corresponding mask, i.e. "
#~ "if it came from a Keras layer "
#~ "with masking support)."
#~ msgstr ""

#~ msgid "A tensor or list/tuple of tensors."
#~ msgstr ""

#~ msgid "Relevant classes for VQNHE"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule`"
#~ msgstr ""

#~ msgid "Dense layer but with complex weights, used for building complex RBM"
#~ msgstr ""

#~ msgid "VQNHE"
#~ msgstr ""

#~ msgid "[description]"
#~ msgstr ""

#~ msgid "VQE"
#~ msgstr ""

#~ msgid "Backend register"
#~ msgstr ""

#~ msgid "Get the `tc.backend` object."
#~ msgstr ""

#~ msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
#~ msgstr ""

#~ msgid "Raises"
#~ msgstr ""

#~ msgid "Backend doesn't exist for `backend` argument."
#~ msgstr ""

#~ msgid "The `tc.backend` object that with all registered universal functions."
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: jax backend"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensornetwork.backends.jax.jax_backend.JaxBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "``jax backend``. "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
#~ msgstr ""

#~ msgid "Returns the elementwise absolute value of tensor. Args:"
#~ msgstr ""

#~ msgid "tensor: An input tensor."
#~ msgstr ""

#~ msgid "tensor: Its elementwise absolute value."
#~ msgstr ""

#~ msgid "Return the index of maximum of an array an axis."
#~ msgstr ""

#~ msgid "[description], defaults to 0, different behavior from numpy defaults!"
#~ msgstr ""

#~ msgid "Return the index of minimum of an array an axis."
#~ msgstr ""

#~ msgid "Cast the tensor dtype of a ``a``."
#~ msgstr ""

#~ msgid "tensor"
#~ msgstr ""

#~ msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
#~ msgstr ""

#~ msgid "``a`` of new dtype"
#~ msgstr ""

#~ msgid "Join a sequence of arrays along an existing axis."
#~ msgstr ""

#~ msgid "[description], defaults to 0"
#~ msgstr ""

#~ msgid ""
#~ "The native cond for XLA compiling, "
#~ "wrapper for ``tf.cond`` and limited "
#~ "functionality of ``jax.lax.cond``."
#~ msgstr ""

#~ msgid "Convert a np.array or a tensor to a tensor type for the backend."
#~ msgstr ""

#~ msgid ""
#~ "Generate the coo format sparse matrix"
#~ " from indices and values, which is"
#~ " the only sparse format supported in"
#~ " different ML backends."
#~ msgstr ""

#~ msgid "shape [n, 2] for n non zero values in the returned matrix"
#~ msgstr ""

#~ msgid "shape [n]"
#~ msgstr ""

#~ msgid "Tuple[int, ...]"
#~ msgstr ""

#~ msgid "Return the expm of ``a``, matrix exponential."
#~ msgstr ""

#~ msgid "tensor in matrix form"
#~ msgstr ""

#~ msgid "matrix exponential of matrix ``a``"
#~ msgstr ""

#~ msgid "Return the cosine of a tensor ``a``."
#~ msgstr ""

#~ msgid "cosine of ``a``"
#~ msgstr ""

#~ msgid "Return the cumulative sum of the elements along a given axis."
#~ msgstr ""

#~ msgid ""
#~ "The default behavior is the same "
#~ "as numpy, different from tf/torch as "
#~ "cumsum of the flatten 1D array, "
#~ "defaults to None"
#~ msgstr ""

#~ msgid "Return the copy of tensor ''a''."
#~ msgstr ""

#~ msgid "Return an identity matrix of dimension `dim`"
#~ msgstr ""

#~ msgid ""
#~ "Depending on specific backends, `dim` "
#~ "has to be either an int (numpy,"
#~ " torch, tensorflow) or a `ShapeType` "
#~ "object (for block-sparse backends). "
#~ "Block-sparse behavior is currently not "
#~ "supported"
#~ msgstr ""

#~ msgid ""
#~ "N (int): The dimension of the "
#~ "returned matrix. dtype: The dtype of "
#~ "the returned matrix. M (int): The "
#~ "dimension of the returned matrix."
#~ msgstr ""

#~ msgid "Return the function which is the grad function of input ``f``."
#~ msgstr ""

#~ msgid "Example"
#~ msgstr ""

#~ msgid "the function to be differentiated"
#~ msgstr ""

#~ msgid ""
#~ "the position of args in ``f`` that"
#~ " are to be differentiated, defaults "
#~ "to be 0"
#~ msgstr ""

#~ msgid "the grad function of ``f`` with the same set of arguments as ``f``"
#~ msgstr ""

#~ msgid "Return 1.j in as a tensor compatible with the backend."
#~ msgstr ""

#~ msgid "\"complex64\" or \"complex128\""
#~ msgstr ""

#~ msgid "1.j tensor"
#~ msgstr ""

#~ msgid "Return the elementwise imaginary value of a tensor ``a``."
#~ msgstr ""

#~ msgid "imaginary value of ``a``"
#~ msgstr ""

#~ msgid "[summary]"
#~ msgstr ""

#~ msgid "The possible options"
#~ msgstr ""

#~ msgid "Sampling output shape"
#~ msgstr ""

#~ msgid ""
#~ "probability for each option in a, "
#~ "defaults to None, as equal probability"
#~ " distribution"
#~ msgstr ""

#~ msgid ""
#~ "Call the random normal function with "
#~ "the random state management behind the"
#~ " scene."
#~ msgstr ""

#~ msgid "[description], defaults to 1"
#~ msgstr ""

#~ msgid "[description], defaults to \"32\""
#~ msgstr ""

#~ msgid "Determine whether the type of input ``a`` is  ``sparse``."
#~ msgstr ""

#~ msgid "input matrix ``a``"
#~ msgstr ""

#~ msgid "a bool indicating whether the matrix ``a`` is sparse"
#~ msgstr ""

#~ msgid "Return a boolean on whether ``a`` is a tensor in backend package."
#~ msgstr ""

#~ msgid "a tensor to be determined"
#~ msgstr ""

#~ msgid "whether ``a`` is a tensor"
#~ msgstr ""

#~ msgid "Return the jitted version of function ``f``."
#~ msgstr ""

#~ msgid "function to be jitted"
#~ msgstr ""

#~ msgid ""
#~ "index of args that doesn't regarded "
#~ "as tensor, only work for jax "
#~ "backend"
#~ msgstr ""

#~ msgid ""
#~ "whether open XLA compilation, only works"
#~ " for tensorflow backend, defaults False "
#~ "since several ops has no XLA "
#~ "correspondence"
#~ msgstr ""

#~ msgid "jitted version of ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Function that computes a (forward-mode)"
#~ " Jacobian-vector product of ``f``. "
#~ "Strictly speaking, this function is "
#~ "value_and_jvp."
#~ msgstr ""

#~ msgid "The function to compute jvp"
#~ msgstr ""

#~ msgid "input for ``f``"
#~ msgstr ""

#~ msgid "tangents"
#~ msgstr ""

#~ msgid ""
#~ "(``f(*inputs)``, jvp_tensor), where jvp_tensor "
#~ "is the same shape as the output"
#~ " of ``f``"
#~ msgstr ""

#~ msgid "Return the kronecker product of two matrices ``a`` and ``b``."
#~ msgstr ""

#~ msgid "kronecker product of ``a`` and ``b``"
#~ msgstr ""

#~ msgid "Return the maximum of an array or maximum along an axis."
#~ msgstr ""

#~ msgid "[description], defaults to None"
#~ msgstr ""

#~ msgid "Return the minimum of an array or minimum along an axis."
#~ msgstr ""

#~ msgid ""
#~ "Return the numpy array of a tensor"
#~ " ``a``, but may not work in a"
#~ " jitted function."
#~ msgstr ""

#~ msgid "numpy array of ``a``"
#~ msgstr ""

#~ msgid ""
#~ "One-hot encodes the given ``a``. "
#~ "Each index in the input ``a`` is"
#~ " encoded as a vector of zeros "
#~ "of length ``num`` with the element "
#~ "at index set to one:"
#~ msgstr ""

#~ msgid "input tensor"
#~ msgstr ""

#~ msgid "number of features in onehot dimension"
#~ msgstr ""

#~ msgid "onehot tensor with the last extra dimension"
#~ msgstr ""

#~ msgid ""
#~ "Return an ones-matrix of dimension "
#~ "`dim` Depending on specific backends, "
#~ "`dim` has to be either an int "
#~ "(numpy, torch, tensorflow) or a "
#~ "`ShapeType` object (for block-sparse "
#~ "backends). Block-sparse behavior is "
#~ "currently not supported Args:"
#~ msgstr ""

#~ msgid ""
#~ "shape (int): The dimension of the "
#~ "returned matrix. dtype: The dtype of "
#~ "the returned matrix."
#~ msgstr ""

#~ msgid ""
#~ "A jax like split API, but it "
#~ "doesn't split the key generator for "
#~ "other backends. It is just for a"
#~ " consistent interface of random code; "
#~ "make sure you know what the "
#~ "function actually does. This function is"
#~ " mainly a utility to write backend"
#~ " agnostic code instead of doing magic"
#~ " things."
#~ msgstr ""

#~ msgid "Return the elementwise real value of a tensor ``a``."
#~ msgstr ""

#~ msgid "real value of ``a``"
#~ msgstr ""

#~ msgid ""
#~ "Rectified linear unit activation function. "
#~ "Computes the element-wise function:"
#~ msgstr ""

#~ msgid "\\mathrm{relu}(x)=\\max(x,0)"
#~ msgstr ""

#~ msgid "Input tensor"
#~ msgstr ""

#~ msgid "Tensor after relu"
#~ msgstr ""

#~ msgid ""
#~ "Roughly equivalent to operand[indices] = "
#~ "updates, indices only support shape with"
#~ " rank 2 for now."
#~ msgstr ""

#~ msgid "Set the random state attached to the backend."
#~ msgstr ""

#~ msgid "the random seed, defaults to be None"
#~ msgstr ""

#~ msgid ""
#~ "If set to be true, only get "
#~ "the random state in return instead "
#~ "of setting the state on the "
#~ "backend"
#~ msgstr ""

#~ msgid "Return the  elementwise sine of a tensor ``a``."
#~ msgstr ""

#~ msgid "sine of ``a``"
#~ msgstr ""

#~ msgid "Return the total number of elements in ``a`` in tensor form."
#~ msgstr ""

#~ msgid "the total number of elements in ``a``"
#~ msgstr ""

#~ msgid ""
#~ "Softmax function. Computes the function "
#~ "which rescales elements to the range "
#~ "[0,1] such that the elements along "
#~ "axis sum to 1."
#~ msgstr ""

#~ msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
#~ msgstr ""

#~ msgid "Tensor"
#~ msgstr ""

#~ msgid ""
#~ "A dimension along which Softmax will "
#~ "be computed , defaults to None for"
#~ " all axis sum."
#~ msgstr ""

#~ msgid "concatenated tensor"
#~ msgstr ""

#~ msgid "Solve the linear system Ax=b and return the solution x."
#~ msgstr ""

#~ msgid "The multiplied matrix."
#~ msgstr ""

#~ msgid "The resulted matrix."
#~ msgstr ""

#~ msgid "The solution of the linear system."
#~ msgstr ""

#~ msgid "A sparse matrix multiplies a dense matrix."
#~ msgstr ""

#~ msgid "a sparse matrix"
#~ msgstr ""

#~ msgid "a dense matrix"
#~ msgstr ""

#~ msgid "dense matrix"
#~ msgstr ""

#~ msgid ""
#~ "Concatenates a sequence of tensors ``a``"
#~ " along a new dimension ``axis``."
#~ msgstr ""

#~ msgid "List of tensors in the same shape"
#~ msgstr ""

#~ msgid "the stack axis, defaults to 0"
#~ msgstr ""

#~ msgid "stateful register for each package"
#~ msgstr ""

#~ msgid "shape of output sampling tensor"
#~ msgstr ""

#~ msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
#~ msgstr ""

#~ msgid "Uniform random sampler from ``low`` to ``high``."
#~ msgstr ""

#~ msgid "shape of output sampling tensor, defaults to 1"
#~ msgstr ""

#~ msgid "Stop backpropagation from ``a``."
#~ msgstr ""

#~ msgid "``branches[index]()``"
#~ msgstr ""

#~ msgid "Constructs a tensor by tiling a given tensor."
#~ msgstr ""

#~ msgid "1d tensor with length the same as the rank of ``a``"
#~ msgstr ""

#~ msgid "Convert a sparse matrix to dense tensor."
#~ msgstr ""

#~ msgid "the resulted dense matrix"
#~ msgstr ""

#~ msgid ""
#~ "Find the unique elements and their "
#~ "corresponding counts of the given tensor"
#~ " ``a``."
#~ msgstr ""

#~ msgid "Unique elements, corresponding counts"
#~ msgstr ""

#~ msgid "Return the function which returns the value and grad of ``f``."
#~ msgstr ""

#~ msgid ""
#~ "the value and grad function of "
#~ "``f`` with the same set of "
#~ "arguments as ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Return the VVAG function of ``f``. "
#~ "The inputs for ``f`` is (args[0], "
#~ "args[1], args[2], ...), and the output"
#~ " of ``f`` is a scalar. Suppose "
#~ "VVAG(f) is a function with inputs "
#~ "in the form (vargs[0], args[1], args[2],"
#~ " ...), where vagrs[0] has one extra"
#~ " dimension than args[0] in the first"
#~ " axis and consistent with args[0] in"
#~ " shape for remaining dimensions, i.e. "
#~ "shape(vargs[0]) = [batch] + shape(args[0])."
#~ " (We only cover cases where "
#~ "``vectorized_argnums`` defaults to 0 here "
#~ "for demonstration). VVAG(f) returns a "
#~ "tuple as a value tensor with shape"
#~ " [batch, 1] and a gradient tuple "
#~ "with shape: ([batch]+shape(args[argnum]) for "
#~ "argnum in argnums). The gradient for "
#~ "argnums=k is defined as"
#~ msgstr ""

#~ msgid ""
#~ "g^k = \\frac{\\partial \\sum_{i\\in batch} "
#~ "f(vargs[0][i], args[1], ...)}{\\partial args[k]}"
#~ msgstr ""

#~ msgid "Therefore, if argnums=0, the gradient is reduced to"
#~ msgstr ""

#~ msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
#~ msgstr ""

#~ msgid ""
#~ ", which is specifically suitable for "
#~ "batched VQE optimization, where args[0] "
#~ "is the circuit parameters."
#~ msgstr ""

#~ msgid "And if argnums=1, the gradient is like"
#~ msgstr ""

#~ msgid ""
#~ "g^1_i = \\frac{\\partial \\sum_j "
#~ "f(vargs[0][j], args[1])}{\\partial args[1][i]}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ ", which is suitable for quantum "
#~ "machine learning scenarios, where ``f`` "
#~ "is the loss function, args[0] "
#~ "corresponds to the input data and "
#~ "args[1] corresponds to the weights in"
#~ " the QML model."
#~ msgstr ""

#~ msgid ""
#~ "the args to be vectorized, these "
#~ "arguments should share the same batch"
#~ " shape in the fist dimension"
#~ msgstr ""

#~ msgid ""
#~ "Function that computes the dot product"
#~ " between a vector v and the "
#~ "Jacobian of the given function at "
#~ "the point given by the inputs. "
#~ "(reverse mode AD relevant) Strictly "
#~ "speaking, this function is value_and_vjp."
#~ msgstr ""

#~ msgid "the function to carry out vjp calculation"
#~ msgstr ""

#~ msgid ""
#~ "value vector or gradient from downstream"
#~ " in reverse mode AD the same "
#~ "shape as return of function ``f``"
#~ msgstr ""

#~ msgid ""
#~ "(``f(*inputs)``, vjp_tensor), where vjp_tensor "
#~ "is the same shape as inputs"
#~ msgstr ""

#~ msgid ""
#~ "Return the vectorized map or batched "
#~ "version of ``f`` on the first "
#~ "extra axis. The general interface "
#~ "supports ``f`` with multiple arguments "
#~ "and broadcast in the fist dimension."
#~ msgstr ""

#~ msgid "function to be broadcasted."
#~ msgstr ""

#~ msgid "vmap version of ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Return a zeros-matrix of dimension "
#~ "`dim` Depending on specific backends, "
#~ "`dim` has to be either an int "
#~ "(numpy, torch, tensorflow) or a "
#~ "`ShapeType` object (for block-sparse "
#~ "backends)."
#~ msgstr ""

#~ msgid "Block-sparse behavior is currently not supported Args:"
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: numpy backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`"
#~ msgstr ""

#~ msgid ""
#~ "see the original backend API at "
#~ "`numpy backend "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: pytorch backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "``pytorch backend``. "
#~ "`<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
#~ msgstr ""

#~ msgid ""
#~ "Note the functionality provided by "
#~ "pytorch backend is incomplete, it "
#~ "currenly lacks native efficicent jit and"
#~ " vmap support."
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: tensorflow backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "`'tensorflow backend''. "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
#~ msgstr ""

#~ msgid "Some common noise quantum channels."
#~ msgstr ""

#~ msgid ""
#~ "Return an amplitude damping channel. "
#~ "Notice: Amplitude damping corrspondings to "
#~ "p = 1."
#~ msgstr ""

#~ msgid ""
#~ "\\sqrt{p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & \\sqrt{1-\\gamma}\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & \\sqrt{\\gamma}\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{1-p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    \\sqrt{1-\\gamma} & 0\\\\\n"
#~ "    0 & 1\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{1-p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 0\\\\\n"
#~ "    \\sqrt{\\gamma} & 0\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "the damping parameter of amplitude (:math:`\\gamma`)"
#~ msgstr ""

#~ msgid ":math:`p`"
#~ msgstr ""

#~ msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
#~ msgstr ""

#~ msgid "Return a Depolarizing Channel"
#~ msgstr ""

#~ msgid ""
#~ "\\sqrt{1-p_x-p_y-p_z}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & 1\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_x}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 1\\\\\n"
#~ "    1 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_y}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & -1j\\\\\n"
#~ "    1j & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_z}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & -1\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ":math:`p_x`"
#~ msgstr ""

#~ msgid ":math:`p_y`"
#~ msgstr ""

#~ msgid ":math:`p_z`"
#~ msgstr ""

#~ msgid "Sequences of Gates"
#~ msgstr ""

#~ msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
#~ msgstr ""

#~ msgid ""
#~ "\\sum_{k}^{} K_k \\otimes K_k^{\\dagger}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "A sequence of Gate"
#~ msgstr ""

#~ msgid "The corresponding Tensor of the list of Kraus operators"
#~ msgstr ""

#~ msgid "Return a phase damping channel with given :math:`\\gamma`"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & \\sqrt{1-\\gamma}\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 0\\\\\n"
#~ "    0 & \\sqrt{\\gamma}\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "The damping parameter of phase (:math:`\\gamma`)"
#~ msgstr ""

#~ msgid "A phase damping channel with given :math:`\\gamma`"
#~ msgstr ""

#~ msgid "Reset channel"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 1\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "Check identity of a single qubit Kraus operators."
#~ msgstr ""

#~ msgid "Examples:"
#~ msgstr ""

#~ msgid ""
#~ "\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
#~ "\n"
#~ msgstr ""

#~ msgid "List of Kraus operators."
#~ msgstr ""

#~ msgid "Quantum circuit: state simulator"
#~ msgstr ""

#~ msgid "``Circuit`` class. Simple usage demo below."
#~ msgstr ""

#~ msgid "Apply any gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Qubit number than the gate applies on."
#~ msgstr ""

#~ msgid "Parameters for the gate"
#~ msgstr ""

#~ msgid "Apply cnot gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 1.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Qubit number than the gate applies on. The matrix for the gate is"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply cr gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply crx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply cry gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply crz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply cy gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.-1.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+1.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.-1.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply cz gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & -1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply exp gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply exp1 gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply h gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & 0.70710677+0.j\\\\    "
#~ "0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.70710677+0.j & "
#~ "0.70710677+0.j\\\\    0.70710677+0.j & "
#~ "-0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply i gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply iswap gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+1.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j"
#~ " & 0.+1.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+1.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply r gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply rx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ry gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply rz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply s gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.+1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+1.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply sd gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.-1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.-1.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply swap gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j"
#~ " & 1.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply t gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677+0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1. & +0.j & 0. "
#~ "& +0.j\\\\    0. & +0.j & "
#~ "0.70710677+0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply td gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677-0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1. & +0.j & 0. "
#~ "& +0.j\\\\    0. & +0.j & "
#~ "0.70710677-0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply toffoli gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 1.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply wroot gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & -0.5 & -0.5j\\\\    0.5"
#~ " & -0.5j & 0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.70710677+0.j & -0.5 &"
#~ " -0.5j\\\\    0.5 & -0.5j & "
#~ "0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply x gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j\\\\    1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    "
#~ "1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply y gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j\\\\    0.+1.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    "
#~ "0.+1.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply z gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & -1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & -1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Circuit object based on state simulator."
#~ msgstr ""

#~ msgid "The number of qubits in the circuit."
#~ msgstr ""

#~ msgid ""
#~ "If not None, the initial state of"
#~ " the circuit is taken as ``inputs``"
#~ " instead of :math:`\\vert 0\\rangle^n` "
#~ "qubits, defaults to None"
#~ msgstr ""

#~ msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction"
#~ msgstr ""

#~ msgid ""
#~ "dict if two qubit gate is ready"
#~ " for split, including parameters for "
#~ "at least one of ``max_singular_values`` "
#~ "and ``max_truncation_err``."
#~ msgstr ""

#~ msgid ""
#~ "Monte Carlo trajectory simulation of "
#~ "general Kraus channel whose Kraus "
#~ "operators cannot be amplified to unitary"
#~ " operators. For unitary operators composed"
#~ " Kraus channel, :py:meth:`unitary_kraus` is "
#~ "much faster."
#~ msgstr ""

#~ msgid ""
#~ "This function is jittable in theory. "
#~ "But only jax+GPU combination is "
#~ "recommended for jit since the graph "
#~ "building time is too long for "
#~ "other backend options; though the "
#~ "running time of the function is "
#~ "very fast for every case."
#~ msgstr ""

#~ msgid "list of ``tn.Node`` for Kraus operators"
#~ msgstr ""

#~ msgid "the qubits index that Kraus channel is applied on"
#~ msgstr ""

#~ msgid ""
#~ "random tensor between 0 or 1, "
#~ "defaults to be None, the random "
#~ "number will be generated automatically"
#~ msgstr ""

#~ msgid "Compute the expectation of corresponding operators."
#~ msgstr ""

#~ msgid ""
#~ "operator and its position on the "
#~ "circuit, eg. ``(tc.gates.z(), [1, ]), "
#~ "(tc.gates.x(), [2, ])`` is for operator"
#~ " :math:`Z_1X_2`"
#~ msgstr ""

#~ msgid ""
#~ "if True, then the wavefunction tensor"
#~ " is cached for further expectation "
#~ "evaluation, defaults to be true"
#~ msgstr ""

#~ msgid "Tensor with one element"
#~ msgstr ""

#~ msgid "[WIP], check whether the circuit is legal."
#~ msgstr ""

#~ msgid "the bool indicating whether the circuit is legal"
#~ msgstr ""

#~ msgid "Take measurement to the given quantum lines."
#~ msgstr ""

#~ msgid "measure on which quantum line"
#~ msgstr ""

#~ msgid "if true, theoretical probability is also returned"
#~ msgstr ""

#~ msgid ""
#~ "Middle measurement in z-basis on the "
#~ "circuit, note the wavefunction output is"
#~ " not normalized with ``mid_measurement`` "
#~ "involved, one should normalize the state"
#~ " manually if needed."
#~ msgstr ""

#~ msgid "the index of qubit that the Z direction postselection applied on"
#~ msgstr ""

#~ msgid "0 for spin up, 1 for spin down, defaults to be 0"
#~ msgstr ""

#~ msgid "Reference: arXiv:1201.3974."
#~ msgstr ""

#~ msgid "sampled bit string and the corresponding theoretical probability"
#~ msgstr ""

#~ msgid "Replace the input state with the circuit structure unchanged."
#~ msgstr ""

#~ msgid "Input wavefunction."
#~ msgstr ""

#~ msgid ""
#~ "Replace the input state in MPS "
#~ "representation while keep the circuit "
#~ "structure unchanged."
#~ msgstr ""

#~ msgid "Compute the output wavefunction from the circuit."
#~ msgstr ""

#~ msgid "the str indicating the form of the output wavefunction"
#~ msgstr ""

#~ msgid "Tensor with the corresponding shape"
#~ msgstr ""

#~ msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`"
#~ msgstr ""

#~ msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
#~ msgstr ""

#~ msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
#~ msgstr ""

#~ msgid "[description], defaults to None, which is the same as ``ket``"
#~ msgstr ""

#~ msgid "[description], defaults to True"
#~ msgstr ""

#~ msgid "[description], defaults to False"
#~ msgstr ""

#~ msgid ""
#~ "Not an ideal visualization for quantum"
#~ " circuit, but reserve here as a "
#~ "general approch to show tensornetwork "
#~ "[Deperacted, use ``qir2tex instead``]"
#~ msgstr ""

#~ msgid "Constants and setups"
#~ msgstr ""

#~ msgid ""
#~ "To set runtime contractor of the "
#~ "tensornetwork for a better contraction "
#~ "path."
#~ msgstr ""

#~ msgid ""
#~ "\"auto\", \"greedy\", \"branch\", \"plain\", "
#~ "\"tng\", \"custom\", \"custom_stateful\". defaults"
#~ " to None (\"auto\")"
#~ msgstr ""

#~ msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "It is not very useful, as "
#~ "``memory_limit`` leads to ``branch`` "
#~ "contraction instead of ``greedy`` which "
#~ "is rather slow, defaults to None"
#~ msgstr ""

#~ msgid "Tensornetwork version is too low to support some of the contractors."
#~ msgstr ""

#~ msgid "Unknown method options."
#~ msgstr ""

#~ msgid "The new tensornetwork with its contractor set."
#~ msgstr ""

#~ msgid "To set the runtime numerical dtype of tensors."
#~ msgstr ""

#~ msgid ""
#~ "\"complex64\" or \"complex128\", defaults to"
#~ " None, which is equivalent to "
#~ "\"complex64\"."
#~ msgstr ""

#~ msgid "The naive state-vector simulator contraction path."
#~ msgstr ""

#~ msgid "The list of ``tn.Node``."
#~ msgstr ""

#~ msgid "The list of dangling node edges, defaults to be None."
#~ msgstr ""

#~ msgid "The ``tn.Node`` after contraction"
#~ msgstr ""

#~ msgid "To set the runtime backend of tensorcircuit."
#~ msgstr ""

#~ msgid ""
#~ "Note: ``tc.set_backend`` and "
#~ "``tc.cons.set_tensornetwork_backend`` are the same."
#~ msgstr ""

#~ msgid ""
#~ "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". "
#~ "defaults to None, which gives the "
#~ "same behavior as "
#~ "``tensornetwork.backend_contextmanager.get_default_backend()``."
#~ msgstr ""

#~ msgid "Whether the object should be set as global."
#~ msgstr ""

#~ msgid "Quantum circuit class but with density matrix simulator"
#~ msgstr ""

#~ msgid "Quantum circuit class but with density matrix simulator: v2"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.densitymatrix.DMCircuit`"
#~ msgstr ""

#~ msgid "Experimental features"
#~ msgstr ""

#~ msgid ""
#~ "Declarations of single-qubit and two-"
#~ "qubit gates and their corresponding "
#~ "matrix."
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensornetwork.network_components.Node`"
#~ msgstr ""

#~ msgid "Wrapper of tn.Node, quantum gate"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.gates.GateF`"
#~ msgstr ""

#~ msgid "Note one should provide the gate with properly reshaped."
#~ msgstr ""

#~ msgid "corresponding gate"
#~ msgstr ""

#~ msgid "The name of the gate."
#~ msgstr ""

#~ msgid "the resulted gate"
#~ msgstr ""

#~ msgid "Convert the inputs to Tensor with specified dtype."
#~ msgstr ""

#~ msgid "inputs"
#~ msgstr ""

#~ msgid "dtype of the output Tensors"
#~ msgstr ""

#~ msgid "List of Tensors"
#~ msgstr ""

#~ msgid "Returns a LaTeX bmatrix."
#~ msgstr ""

#~ msgid "Formatted Display:"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j \\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "2D numpy array"
#~ msgstr ""

#~ msgid "ValueError(\"bmatrix can at most display two dimensions\")"
#~ msgstr ""

#~ msgid "latex str for bmatrix of array a"
#~ msgstr ""

#~ msgid ""
#~ "Controlled rotation gate, when the "
#~ "control bit is 1, `rgate` is "
#~ "applied on the target gate."
#~ msgstr ""

#~ msgid "angle in radians"
#~ msgstr ""

#~ msgid "CR Gate"
#~ msgstr ""

#~ msgid ""
#~ "Faster exponential gate, directly implemented"
#~ " based on RHS, only work when: "
#~ ":math:`U^2` is identity matrix."
#~ msgstr ""

#~ msgid ""
#~ "\\rm{exp}(U) &= e^{-i \\theta U} \\\\\n"
#~ "        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
#~ "\n"
#~ msgstr ""

#~ msgid "input unitary (U)"
#~ msgstr ""

#~ msgid "suffix of Gate name"
#~ msgstr ""

#~ msgid "Exponential Gate"
#~ msgstr ""

#~ msgid "Exponential gate."
#~ msgstr ""

#~ msgid ""
#~ "\\rm{exp}(U) = e^{-i \\theta U}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "iSwap gate."
#~ msgstr ""

#~ msgid ""
#~ "iSwap(\\theta) =\n"
#~ "\\begin{pmatrix}\n"
#~ "    1 & 0 & 0 & 0\\\\\n"
#~ "    0 & \\cos(\\frac{\\pi}{2} \\theta )"
#~ " & j \\sin(\\frac{\\pi}{2} \\theta ) "
#~ "& 0\\\\\n"
#~ "    0 & j \\sin(\\frac{\\pi}{2} \\theta"
#~ " ) & \\cos(\\frac{\\pi}{2} \\theta ) "
#~ "& 0\\\\\n"
#~ "    0 & 0 & 0 & 1\\\\\n"
#~ "\\end{pmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "iSwap Gate"
#~ msgstr ""

#~ msgid "Convert Gate to numpy array."
#~ msgstr ""

#~ msgid "input Gate"
#~ msgstr ""

#~ msgid "corresponding Tensor"
#~ msgstr ""

#~ msgid ""
#~ "Inner helper function to generate gate"
#~ " functions, such as ``z()`` from "
#~ "``_z_matrix``"
#~ msgstr ""

#~ msgid "General single qubit rotation gate"
#~ msgstr ""

#~ msgid ""
#~ "R(\\theta, \\phi, \\alpha) = i \\cos(\\theta) I\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\sin(\\theta) \\cos(\\alpha) Z\n"
#~ "\n"
#~ msgstr ""

#~ msgid "R Gate"
#~ msgstr ""

#~ msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
#~ msgstr ""

#~ msgid "A random single qubit gate"
#~ msgstr ""

#~ msgid "Returns a random two-qubit gate."
#~ msgstr ""

#~ msgid "a random two-qubit gate"
#~ msgstr ""

#~ msgid ""
#~ "Rotation gate, which is in matrix "
#~ "exponential form, shall give the same"
#~ " result as `rgate`."
#~ msgstr ""

#~ msgid ""
#~ "mx = \\sin(\\alpha) \\cos(\\phi) X\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "my = \\sin(\\alpha) \\sin(\\phi) Y\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "mz = \\cos(\\alpha) Z\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "R(\\theta, \\alpha, \\phi) = e^{-i\\theta (mx+my+mz)}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "Rotation Gate"
#~ msgstr ""

#~ msgid "Rotation gate along X axis."
#~ msgstr ""

#~ msgid ""
#~ "RX(\\theta) = e^{-i\\frac{\\theta}{2}X}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RX Gate"
#~ msgstr ""

#~ msgid "Rotation gate along Y axis."
#~ msgstr ""

#~ msgid ""
#~ "RY(\\theta) = e^{-i\\frac{\\theta}{2}Y}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RY Gate"
#~ msgstr ""

#~ msgid "Rotation gate along Z axis."
#~ msgstr ""

#~ msgid ""
#~ "RZ(\\theta) = e^{-i\\frac{\\theta}{2}Z}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RZ Gate"
#~ msgstr ""

#~ msgid "Interfaces bridging different backends"
#~ msgstr ""

#~ msgid "Keras layer for tc quantum function"
#~ msgstr ""

#~ msgid ""
#~ "`QuantumLayer` wraps the quantum function "
#~ "`f` as a `keras.Layer` so that "
#~ "tensorcircuit is better integrated with "
#~ "tensorflow."
#~ msgstr ""

#~ msgid "[description], defaults to \"glorot_uniform\""
#~ msgstr ""

#~ msgid ""
#~ "Load function from the files in "
#~ "the ``tf.savedmodel`` format. We can "
#~ "load several functions at the same "
#~ "time, as they can be the same "
#~ "function of different input shapes."
#~ msgstr ""

#~ msgid ""
#~ "The fallback function when all functions"
#~ " loaded are failed, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "When there is not legal loaded "
#~ "function of the input shape and no"
#~ " fallback callable."
#~ msgstr ""

#~ msgid ""
#~ "A function that tries all loaded "
#~ "function against the input until the "
#~ "first success one."
#~ msgstr ""

#~ msgid ""
#~ "The keras loss function that directly"
#~ " taking the model output as the "
#~ "loss."
#~ msgstr ""

#~ msgid "Save tf function in the file (``tf.savedmodel`` format)."
#~ msgstr ""

#~ msgid "``tf.function`` ed function with graph building"
#~ msgstr ""

#~ msgid "the dir path to save the function"
#~ msgstr ""

#~ msgid "FiniteMPS from tensornetwork with bug fixed"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.matrixproductstates.finite_mps.FiniteMPS`"
#~ msgstr ""

#~ msgid ""
#~ "Apply a two-site gate to an "
#~ "MPS. This routine will in general "
#~ "destroy any canonical form of the "
#~ "state. If a canonical form is "
#~ "needed, the user can restore it "
#~ "using `FiniteMPS.position`."
#~ msgstr ""

#~ msgid "A two-body gate."
#~ msgstr ""

#~ msgid "The first site where the gate acts."
#~ msgstr ""

#~ msgid "The second site where the gate acts."
#~ msgstr ""

#~ msgid "The maximum number of singular values to keep."
#~ msgstr ""

#~ msgid "The maximum allowed truncation error."
#~ msgstr ""

#~ msgid ""
#~ "An optional value to choose the "
#~ "MPS tensor at `center_position` to be"
#~ " isometric after the application of "
#~ "the gate. Defaults to `site1`. If "
#~ "the MPS is canonical "
#~ "(i.e.`BaseMPS.center_position != None`), and "
#~ "if the orthogonality center coincides "
#~ "with either `site1` or `site2`,  the "
#~ "orthogonality center will be shifted to"
#~ " `center_position` (`site1` by default). If"
#~ " the orthogonality center does not "
#~ "coincide with `(site1, site2)` then "
#~ "`MPS.center_position` is set to `None`."
#~ msgstr ""

#~ msgid "Multiply `max_truncation_err` with the largest singular value."
#~ msgstr ""

#~ msgid ""
#~ "\"rank of gate is {} but has "
#~ "to be 4\", \"site1 = {} is "
#~ "not between 0 <= site < N -"
#~ " 1 = {}\", \"site2 = {} is "
#~ "not between 1 <= site < N ="
#~ " {}\",\"Found site2 ={}, site1={}. Only "
#~ "nearest neighbor gates are currently "
#~ "supported\", \"f center_position = "
#~ "{center_position} not  f in {(site1, "
#~ "site2)} \", or \"center_position = {},"
#~ " but gate is applied at sites "
#~ "{}, {}. Truncation should only be "
#~ "done if the gate is applied at "
#~ "the center position of the MPS.\""
#~ msgstr ""

#~ msgid "A scalar tensor containing the truncated weight of the truncation."
#~ msgstr ""

#~ msgid "Measure the expectation value of local operators `ops` site `sites`."
#~ msgstr ""

#~ msgid "A list Tensors of rank 2; the local operators to be measured."
#~ msgstr ""

#~ msgid "Sites where `ops` act."
#~ msgstr ""

#~ msgid "measurements :math:`\\langle` `ops[n]`:math:`\\rangle` for n in `sites`"
#~ msgstr ""

#~ msgid ""
#~ "Compute the correlator :math:`\\langle` "
#~ "`op1[site1], op2[s]`:math:`\\rangle` between `site1`"
#~ " and all sites `s` in `sites2`. "
#~ "If `s == site1`, `op2[s]` will be"
#~ " applied first."
#~ msgstr ""

#~ msgid "Tensor of rank 2; the local operator at `site1`."
#~ msgstr ""

#~ msgid "Tensor of rank 2; the local operator at `sites2`."
#~ msgstr ""

#~ msgid "The site where `op1`  acts"
#~ msgstr ""

#~ msgid "Sites where operator `op2` acts."
#~ msgstr ""

#~ msgid ""
#~ "Correlator :math:`\\langle` `op1[site1], "
#~ "op2[s]`:math:`\\rangle` for `s` :math:`\\in` "
#~ "`sites2`."
#~ msgstr ""

#~ msgid "Quantum circuit: MPS state simulator"
#~ msgstr ""

#~ msgid "``MPSCircuit`` class. Simple usage demo below."
#~ msgstr ""

#~ msgid "MPSCircuit object based on state simulator."
#~ msgstr ""

#~ msgid ""
#~ "If not None, the initial state of"
#~ " the circuit is taken as ``tensors``"
#~ " instead of :math:`\\vert 0\\rangle^n` "
#~ "qubits, defaults to None"
#~ msgstr ""

#~ msgid "The center position of MPS, default to 0"
#~ msgstr ""

#~ msgid "Apply a general qubit gate on MPS."
#~ msgstr ""

#~ msgid "The Gate to be applied"
#~ msgstr ""

#~ msgid "Qubit indices of the gate"
#~ msgstr ""

#~ msgid "\"MPS does not support application of gate on > 2 qubits.\""
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on "
#~ "adjacent qubits of Matrix Product States"
#~ " (MPS). Truncation rule is specified "
#~ "by `set_truncation_rule`."
#~ msgstr ""

#~ msgid "The first qubit index of the gate"
#~ msgstr ""

#~ msgid "The second qubit index of the gate"
#~ msgstr ""

#~ msgid "Center position of MPS, default is None"
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on MPS."
#~ " Truncation rule is specified by "
#~ "`set_truncation_rule`."
#~ msgstr ""

#~ msgid ""
#~ "Apply a single qubit gate on MPS,"
#~ " and the gate must be unitary; "
#~ "no truncation is needed."
#~ msgstr ""

#~ msgid "gate to be applied"
#~ msgstr ""

#~ msgid "Qubit index of the gate"
#~ msgstr ""

#~ msgid "Compute the conjugate of the current MPS."
#~ msgstr ""

#~ msgid "The constructed MPS"
#~ msgstr ""

#~ msgid "Copy the current MPS."
#~ msgstr ""

#~ msgid "Copy the current MPS without the tensors."
#~ msgstr ""

#~ msgid "Compute the expectation of the corresponding double qubit gate."
#~ msgstr ""

#~ msgid "qubit index of the gate"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of the "
#~ "corresponding single qubit gate in the"
#~ " form of tensor."
#~ msgstr ""

#~ msgid "Gate to be applied"
#~ msgstr ""

#~ msgid "The expectation of the corresponding single qubit gate"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of the direct"
#~ " product of the corresponding two "
#~ "gates."
#~ msgstr ""

#~ msgid "First gate to be applied"
#~ msgstr ""

#~ msgid "Second gate to be applied"
#~ msgstr ""

#~ msgid "Qubit index of the first gate"
#~ msgstr ""

#~ msgid "Qubit index of the second gate"
#~ msgstr ""

#~ msgid "The correlation of the corresponding two qubit gates"
#~ msgstr ""

#~ msgid "Construct the MPS from a given wavefunction."
#~ msgstr ""

#~ msgid "The given wavefunction (any shape is OK)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of corresponding "
#~ "operators in the form of tensor."
#~ msgstr ""

#~ msgid ""
#~ "Operator and its position on the "
#~ "circuit, eg. ``(gates.Z(), [1]), (gates.X(),"
#~ " [2])`` is for operator :math:`Z_1X_2`"
#~ msgstr ""

#~ msgid "The expectation of corresponding operators"
#~ msgstr ""

#~ msgid "Get the normalized Center Position."
#~ msgstr ""

#~ msgid "Normalized Center Position."
#~ msgstr ""

#~ msgid "Check whether the circuit is legal."
#~ msgstr ""

#~ msgid "Whether the circuit is legal."
#~ msgstr ""

#~ msgid "integer indicating the measure on which quantum line"
#~ msgstr ""

#~ msgid ""
#~ "Middle measurement in the z-basis on "
#~ "the circuit, note the wavefunction "
#~ "output is not normalized with "
#~ "``mid_measurement`` involved, one should "
#~ "normalized the state manually if needed."
#~ msgstr ""

#~ msgid "The index of qubit that the Z direction postselection applied on"
#~ msgstr ""

#~ msgid "0 for spin up, 1 for spin down, defaults to 0"
#~ msgstr ""

#~ msgid "Normalize MPS Circuit according to the center position."
#~ msgstr ""

#~ msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
#~ msgstr ""

#~ msgid "The orthogonality center"
#~ msgstr ""

#~ msgid "Compute the projection between `other` as bra and `self` as ket."
#~ msgstr ""

#~ msgid "ket of the other MPS, which will be converted to bra automatically"
#~ msgstr ""

#~ msgid "The projection in form of tensor"
#~ msgstr ""

#~ msgid ""
#~ "Set truncation rules when double qubit"
#~ " gates are applied. If nothing is "
#~ "specified, no truncation will take place"
#~ " and the bond dimension will keep "
#~ "growing. For more details, refer to "
#~ "`split_tensor`."
#~ msgstr ""

#~ msgid "Tensor with shape [1, -1]"
#~ msgstr ""

#~ msgid ""
#~ "Split the tensor by SVD or QR "
#~ "depends on whether a truncation is "
#~ "required."
#~ msgstr ""

#~ msgid "The input tensor to split."
#~ msgstr ""

#~ msgid ""
#~ "Determine the orthogonal center is on"
#~ " the left tensor or the right "
#~ "tensor."
#~ msgstr ""

#~ msgid "Two tensors after splitting"
#~ msgstr ""

#~ msgid "Quantum state and operator class backend by tensornetwork"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.quantum.QuOperator`"
#~ msgstr ""

#~ msgid "Represents an adjoint (row) vector via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuAdjointVector` from "
#~ "a tensor network. This encapsulates an"
#~ " existing tensor network, interpreting it"
#~ " as an adjoint vector (row vector)."
#~ msgstr ""

#~ msgid "The edges of the network to be used as the input edges."
#~ msgstr ""

#~ msgid ""
#~ "Nodes used to refer to parts of"
#~ " the tensor network that are not "
#~ "connected to any input or output "
#~ "edges (for example: a scalar factor)."
#~ msgstr ""

#~ msgid ""
#~ "Optional collection of edges to ignore"
#~ " when performing consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuAdjointVector` directly from"
#~ " a single tensor. This first wraps"
#~ " the tensor in a `Node`, then "
#~ "constructs the `QuAdjointVector` from that "
#~ "`Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing an QuAdjointVector."
#~ msgstr ""

#~ msgid ""
#~ "Sequence of integer indices specifying "
#~ "the order in which to interpret "
#~ "the axes as subsystems (input edges)."
#~ " If not specified, the axes are "
#~ "taken in ascending order."
#~ msgstr ""

#~ msgid "The new constructed QuAdjointVector give from the given tensor."
#~ msgstr ""

#~ msgid ""
#~ "Represents a linear operator via a "
#~ "tensor network. To interpret a tensor"
#~ " network as a linear operator, some"
#~ " of the dangling edges must be "
#~ "designated as `out_edges` (output edges) "
#~ "and the rest as `in_edges` (input "
#~ "edges). Considered as a matrix, the "
#~ "`out_edges` represent the row index and"
#~ " the `in_edges` represent the column "
#~ "index. The (right) action of the "
#~ "operator on another then consists of "
#~ "connecting the `in_edges` of the first"
#~ " operator to the `out_edges` of the"
#~ " second. Can be used to do "
#~ "simple linear algebra with tensor "
#~ "networks."
#~ msgstr ""

#~ msgid ""
#~ "Creates a new `QuOperator` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a linear operator. The network "
#~ "is checked for consistency: All dangling"
#~ " edges must either be in `out_edges`,"
#~ " `in_edges`, or `ignore_edges`."
#~ msgstr ""

#~ msgid "The edges of the network to be used as the output edges."
#~ msgstr ""

#~ msgid ""
#~ "Optional collection of dangling edges to"
#~ " ignore when performing consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "At least one reference node is "
#~ "required to specify a scalar. None "
#~ "provided!"
#~ msgstr ""

#~ msgid ""
#~ "The adjoint of the operator. This "
#~ "creates a new `QuOperator` with "
#~ "complex-conjugate copies of all tensors "
#~ "in the network and with the input"
#~ " and output edges switched."
#~ msgstr ""

#~ msgid ""
#~ "Check that the network has the "
#~ "expected dimensionality. This checks that "
#~ "all input and output edges are "
#~ "dangling and that there are no "
#~ "other dangling edges (except any "
#~ "specified in `ignore_edges`). If not, an"
#~ " exception is raised."
#~ msgstr ""

#~ msgid ""
#~ "Contract the tensor network in place."
#~ " This modifies the tensor network "
#~ "representation of the operator (or "
#~ "vector, or scalar), reducing it to "
#~ "a single tensor, without changing the"
#~ " value."
#~ msgstr ""

#~ msgid "Manually specify the axis ordering of the final tensor."
#~ msgstr ""

#~ msgid "The present object."
#~ msgstr ""

#~ msgid ""
#~ "Contracts the tensor network in place"
#~ " and returns the final tensor. Note"
#~ " that this modifies the tensor "
#~ "network representing the operator. The "
#~ "default ordering for the axes of "
#~ "the final tensor is: `*out_edges, "
#~ "*in_edges`. If there are any \"ignored\""
#~ " edges, their axes come first: "
#~ "`*ignored_edges, *out_edges, *in_edges`."
#~ msgstr ""

#~ msgid ""
#~ "Manually specify the axis ordering of"
#~ " the final tensor. The default "
#~ "ordering is determined by `out_edges` "
#~ "and `in_edges` (see above)."
#~ msgstr ""

#~ msgid "Node count '{}' > 1 after contraction!"
#~ msgstr ""

#~ msgid "The final tensor representing the operator."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuOperator` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuOperator` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor."
#~ msgstr ""

#~ msgid "The axis indices of `tensor` to use as `out_edges`."
#~ msgstr ""

#~ msgid "The axis indices of `tensor` to use as `in_edges`."
#~ msgstr ""

#~ msgid "The new operator."
#~ msgstr ""

#~ msgid "All tensor-network nodes involved in the operator."
#~ msgstr ""

#~ msgid ""
#~ "The norm of the operator. This is"
#~ " the 2-norm (also known as the "
#~ "Frobenius or Hilbert-Schmidt norm)."
#~ msgstr ""

#~ msgid ""
#~ "The partial trace of the operator. "
#~ "Subsystems to trace out are supplied "
#~ "as indices, so that dangling edges "
#~ "are connected to each other as: "
#~ "`out_edges[i] ^ in_edges[i] for i in "
#~ "subsystems_to_trace_out` This does not modify"
#~ " the original network. The original "
#~ "ordering of the remaining subsystems is"
#~ " maintained."
#~ msgstr ""

#~ msgid "Indices of subsystems to trace out."
#~ msgstr ""

#~ msgid "A new QuOperator or QuScalar representing the result."
#~ msgstr ""

#~ msgid ""
#~ "Tensor product with another operator. "
#~ "Given two operators `A` and `B`, "
#~ "produces a new operator `AB` "
#~ "representing `A`  `B`. The `out_edges`"
#~ " (`in_edges`) of `AB` is simply the"
#~ " concatenation of the `out_edges` "
#~ "(`in_edges`) of `A.copy()` with that of"
#~ " `B.copy()`: `new_out_edges = [*out_edges_A_copy,"
#~ " *out_edges_B_copy]` `new_in_edges = "
#~ "[*in_edges_A_copy, *in_edges_B_copy]`"
#~ msgstr ""

#~ msgid "The other operator (`B`)."
#~ msgstr ""

#~ msgid "The result (`AB`)."
#~ msgstr ""

#~ msgid "The trace of the operator."
#~ msgstr ""

#~ msgid "Represents a scalar via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuScalar` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a scalar."
#~ msgstr ""

#~ msgid ""
#~ "Nodes used to refer to the tensor"
#~ " network (need not be exhaustive -"
#~ " one node from each disconnected "
#~ "subnetwork is sufficient)."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuScalar` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuScalar` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing a new QuScalar."
#~ msgstr ""

#~ msgid "The new constructed QuScalar from the given tensor."
#~ msgstr ""

#~ msgid "Represents a (column) vector via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuVector` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a (column) vector."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuVector` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuVector` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing a \"QuVector\"."
#~ msgstr ""

#~ msgid ""
#~ "Sequence of integer indices specifying "
#~ "the order in which to interpret "
#~ "the axes as subsystems (output edges)."
#~ " If not specified, the axes are "
#~ "taken in ascending order."
#~ msgstr ""

#~ msgid "The new constructed QuVector from the given tensor."
#~ msgstr ""

#~ msgid ""
#~ "Check the vector spaces represented by"
#~ " two lists of edges are compatible."
#~ " The number of edges must be "
#~ "the same and the dimensions of "
#~ "each pair of edges must match. "
#~ "Otherwise, an exception is raised. "
#~ ":param edges_1: List of edges "
#~ "representing a many-body Hilbert space."
#~ " :type edges_1: Sequence[Edge] :param "
#~ "edges_2: List of edges representing a"
#~ " many-body Hilbert space. :type "
#~ "edges_2: Sequence[Edge]"
#~ msgstr ""

#~ msgid ""
#~ "Hilbert-space mismatch: \"Cannot connect "
#~ "{} subsystems with {} subsystems\", or"
#~ " \"Input dimension {} != output "
#~ "dimension {}.\""
#~ msgstr ""

#~ msgid ""
#~ "Eliminates any connected CopyNodes that "
#~ "are identity matrices. This will modify"
#~ " the network represented by `nodes`. "
#~ "Only identities that are connected to"
#~ " other nodes are eliminated."
#~ msgstr ""

#~ msgid "Collection of nodes to search."
#~ msgstr ""

#~ msgid ""
#~ "The Dictionary mapping remaining Nodes "
#~ "to any replacements, Dictionary specifying "
#~ "all dangling-edge replacements."
#~ msgstr ""

#~ msgid "Compute the entropy from the given density matrix ``rho``."
#~ msgstr ""

#~ msgid "[description], defaults to 1e-12"
#~ msgstr ""

#~ msgid ""
#~ "Note: further jit is recommended. For"
#~ " large Hilbert space, sparse Hamiltonian"
#~ " is recommended"
#~ msgstr ""

#~ msgid ""
#~ "Construct a 'QuOperator' representing the "
#~ "identity on a given space. Internally,"
#~ " this is done by constructing "
#~ "'CopyNode's for each edge, with "
#~ "dimension according to 'space'."
#~ msgstr ""

#~ msgid ""
#~ "A sequence of integers for the "
#~ "dimensions of the tensor product factors"
#~ " of the space (the edges in the"
#~ " tensor network)."
#~ msgstr ""

#~ msgid "The data type (for conversion to dense)."
#~ msgstr ""

#~ msgid "The desired identity operator."
#~ msgstr ""

#~ msgid ""
#~ "Simulate the measuring of each qubit "
#~ "of ``p`` in the computational basis, "
#~ "thus producing output like that of "
#~ "``qiskit``."
#~ msgstr ""

#~ msgid ""
#~ "The quantum state, assumed to be "
#~ "normalized, as either a ket or "
#~ "density operator."
#~ msgstr ""

#~ msgid "The number of counts to perform."
#~ msgstr ""

#~ msgid ""
#~ "Defaults True. The bool indicating "
#~ "whether the return form is in the"
#~ " form of two array or one of"
#~ " the same length as the ``state`` "
#~ "(if ``sparse=False``)."
#~ msgstr ""

#~ msgid "The counts for each bit string measured."
#~ msgstr ""

#~ msgid ""
#~ "Constructs an appropriately specialized "
#~ "QuOperator. If there are no edges, "
#~ "creates a QuScalar. If the are "
#~ "only output (input) edges, creates a "
#~ "QuVector (QuAdjointVector). Otherwise creates "
#~ "a QuOperator."
#~ msgstr ""

#~ msgid ""
#~ "op = qu.quantum_constructor([], [psi_node[0], "
#~ "psi_node[1]]) >>> show_attributes(op) op.is_scalar()"
#~ "          -> False op.is_vector()          -> "
#~ "False op.is_adjoint_vector()  -> True "
#~ "len(op.out_edges)       -> 0 len(op.in_edges)"
#~ "        -> 2 >>> # psi_node[0] -> "
#~ "op.in_edges[0] >>> # psi_node[1] -> "
#~ "op.in_edges[1]"
#~ msgstr ""

#~ msgid "output edges."
#~ msgstr ""

#~ msgid "in edges."
#~ msgstr ""

#~ msgid ""
#~ "reference nodes for the tensor network"
#~ " (needed if there is a scalar "
#~ "component)."
#~ msgstr ""

#~ msgid "edges to ignore when checking the dimensionality of the tensor network."
#~ msgstr ""

#~ msgid "The new created QuOperator object."
#~ msgstr ""

#~ msgid "Compute the reduced density matrix from the quantum state ``state``."
#~ msgstr ""

#~ msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
#~ msgstr ""

#~ msgid "\\mathrm{Tr}(\\prod_i O_i)"
#~ msgstr ""

#~ msgid "the trace of several inputs"
#~ msgstr ""

#~ msgid "Tensornetwork Simplification"
#~ msgstr ""

#~ msgid ""
#~ "Get the new shape of two nodes,"
#~ " also supporting to return original "
#~ "shapes of two nodes."
#~ msgstr ""

#~ msgid "node one"
#~ msgstr ""

#~ msgid "node two"
#~ msgstr ""

#~ msgid "Whether to include original shape of two nodes, default is True."
#~ msgstr ""

#~ msgid "The new shape of the two nodes."
#~ msgstr ""

#~ msgid ""
#~ "Contract between Node ``a`` and ``b``,"
#~ " with correct shape only and no "
#~ "calculation"
#~ msgstr ""

#~ msgid "Shortcuts for measurement patterns on circuit"
#~ msgstr ""

#~ msgid "Some common graphs and lattices"
#~ msgstr ""

#~ msgid "1D chain with ``n`` sites"
#~ msgstr ""

#~ msgid ""
#~ "This measurements pattern is specifically "
#~ "suitable for vmap. Parameterize the "
#~ "Pauli string to be measured."
#~ msgstr ""

#~ msgid ""
#~ "parameter tensors determines what Pauli "
#~ "string to be measured, shape is "
#~ "[nwires, 4] if onehot is False."
#~ msgstr ""

#~ msgid ""
#~ "[description], defaults to False. If set"
#~ " to be True, structures will first"
#~ " go through onehot procedure."
#~ msgstr ""

#~ msgid "COO_sparse_matrix"
#~ msgstr ""

#~ msgid "a real and scalar tensor of shape []"
#~ msgstr ""

#~ msgid "Helper functions"
#~ msgstr ""

#~ msgid ""
#~ "Return a callable function for output"
#~ " ith parts of the original output "
#~ "along the first axis. Original output"
#~ " supports List and Tensor."
#~ msgstr ""

#~ msgid "The function to be applied this method"
#~ msgstr ""

#~ msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
#~ msgstr ""

#~ msgid "The modified callable function"
#~ msgstr ""

#~ msgid "Visualization on circuits"
#~ msgstr ""

#~ msgid "# TODO(@YHPeter): add examples"
#~ msgstr ""

#~ msgid ""
#~ "Generate the PDF file with given "
#~ "latex string and filename. Latex command"
#~ " and file path can be specified. "
#~ "When notebook is True, convert the "
#~ "output PDF file to image and "
#~ "return a Image object."
#~ msgstr ""

#~ msgid "String of latex content"
#~ msgstr ""

#~ msgid "File name, defaults to random UUID `str(uuid4())`"
#~ msgstr ""

#~ msgid "Executable Latex command, defaults to `pdflatex`"
#~ msgstr ""

#~ msgid "File path, defaults to current working place `os.getcwd()`"
#~ msgstr ""

#~ msgid "if notebook is True, return `Image` object; otherwise return `None`"
#~ msgstr ""

#~ msgid "_summary_"
#~ msgstr ""

#~ msgid "_description_"
#~ msgstr ""

#~ msgid "[description], default is None."
#~ msgstr ""

#~ msgid "_description_, default is (1, -1)."
#~ msgstr ""

#~ msgid "Apply fredkin gate on the circuit."
#~ msgstr ""

#~ msgid "Apply orx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ory gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply orz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ox gate on the circuit."
#~ msgstr ""

#~ msgid "Apply oy gate on the circuit."
#~ msgstr ""

#~ msgid "Apply oz gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Random tensor between 0 or 1, "
#~ "defaults to be None, the random "
#~ "number will be generated automatically"
#~ msgstr ""

#~ msgid "The str indicating the form of the output wavefunction."
#~ msgstr ""

#~ msgid ""
#~ "A collection of useful function snippets"
#~ " that irrelevant with the main "
#~ "modules or await for further refactor"
#~ msgstr ""

#~ msgid "VQNHE application"
#~ msgstr ""

#~ msgid "Apply **ANY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CR** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **EXP** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **EXP1** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **FREDKIN** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **ISWAP** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **OX** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "1.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **OY** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+1.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **OZ** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & -1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **R** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **SD** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **TD** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **TOFFOLI** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Apply unitary gates in ``kraus`` "
#~ "randomly based on corresponding ``prob``."
#~ msgstr ""

#~ msgid "Get the eigenvalues of matrix ``a``."
#~ msgstr ""

#~ msgid "eigenvalues of ``a``"
#~ msgstr ""

#~ msgid ""
#~ "Apply **ANY** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.any_gate`."
#~ msgstr ""

#~ msgid "Qubit number that the gate applies on."
#~ msgstr ""

#~ msgid ""
#~ "Apply **CNOT** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.cnot_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 1.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Qubit number that the gate applies on. The matrix for the gate is"
#~ msgstr ""

#~ msgid ""
#~ "Apply **CR** gate with parameters on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.cr_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **CRX** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.crx_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **CRY** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.cry_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **CRZ** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.crz_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **CY** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.cy_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.-1.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+1.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **CZ** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.cz_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & -1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **EXP** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.exp_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **EXP1** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.exp1_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **FREDKIN** gate on the circuit."
#~ " See :py:meth:`tensorcircuit.gates.fredkin_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **H** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.h_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & 0.70710677+0.j\\\\    "
#~ "0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **I** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.i_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **ISWAP** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.iswap_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+1.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **ORX** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.orx_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **ORY** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.ory_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **ORZ** gate with parameters on"
#~ " the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.orz_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **OX** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.ox_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "1.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **OY** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.oy_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+1.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **OZ** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.oz_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & -1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **R** gate with parameters on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.r_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **RX** gate with parameters on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.rx_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **RY** gate with parameters on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.ry_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **RZ** gate with parameters on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.gates.rz_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Apply **S** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.s_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.+1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **SD** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.sd_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.-1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **SWAP** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.swap_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **T** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.t_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677+0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **TD** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.td_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677-0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **TOFFOLI** gate on the circuit."
#~ " See :py:meth:`tensorcircuit.gates.toffoli_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **WROOT** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.wroot_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & -0.5 & -0.5j\\\\    0.5"
#~ " & -0.5j & 0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **X** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.x_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j\\\\    1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **Y** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.y_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j\\\\    0.+1.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply **Z** gate on the circuit. "
#~ "See :py:meth:`tensorcircuit.gates.z_gate`."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number that the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & -1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "Apply unitary gates in ``kraus`` "
#~ "randomly based on corresponding ``prob``. "
#~ "If ``prob`` is ``None``, this is "
#~ "reduced to kraus channel language."
#~ msgstr ""

#~ msgid "The density matrix simulator based on tensornetwork engine."
#~ msgstr ""

#~ msgid "Number of qubits"
#~ msgstr ""

#~ msgid "if True, nothing initialized, only for internal use, defaults to False"
#~ msgstr ""

#~ msgid "the state input for the circuit, defaults to None"
#~ msgstr ""

#~ msgid "the density matrix input for the circuit, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "Apply amplitudedamping quantum channel on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.channels.amplitudedampingchannel`"
#~ msgstr ""

#~ msgid "Parameters for the channel."
#~ msgstr ""

#~ msgid "Return the output density matrix of the circuit."
#~ msgstr ""

#~ msgid ""
#~ "check whether the final return is "
#~ "a legal density matrix, defaults to "
#~ "False"
#~ msgstr ""

#~ msgid "whether to reuse previous results, defaults to True"
#~ msgstr ""

#~ msgid "The output densitymatrix in 2D shape tensor form"
#~ msgstr ""

#~ msgid ""
#~ "Apply depolarizing quantum channel on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.channels.depolarizingchannel`"
#~ msgstr ""

#~ msgid ""
#~ "Apply phasedamping quantum channel on "
#~ "the circuit. See "
#~ ":py:meth:`tensorcircuit.channels.phasedampingchannel`"
#~ msgstr ""

#~ msgid ""
#~ "Apply reset quantum channel on the "
#~ "circuit. See "
#~ ":py:meth:`tensorcircuit.channels.resetchannel`"
#~ msgstr ""

#~ msgid "Generate tensorflow sparse matrix from Pauli string sum"
#~ msgstr ""

#~ msgid ""
#~ "1D Tensor representing for a Pauli "
#~ "string, e.g. [1, 0, 0, 3, 2] "
#~ "is for :math:`X_0Z_3Y_4`"
#~ msgstr ""

#~ msgid ""
#~ "the weight for the Pauli string "
#~ "defaults to None (all Pauli strings "
#~ "weight 1.0)"
#~ msgstr ""

#~ msgid "the tensorflow sparse matrix"
#~ msgstr ""

#~ msgid ""
#~ "2D Tensor, each row is for a "
#~ "Pauli string, e.g. [1, 0, 0, 3,"
#~ " 2] is for :math:`X_0Z_3Y_4`"
#~ msgstr ""

#~ msgid ""
#~ "1D Tensor, each element corresponds the"
#~ " weight for each Pauli string "
#~ "defaults to None (all Pauli strings "
#~ "weight 1.0)"
#~ msgstr ""

#~ msgid "the tensorflow coo sparse matrix"
#~ msgstr ""

#~ msgid "Generate scipy sparse matrix from Pauli string sum"
#~ msgstr ""

#~ msgid "the scipy coo sparse matrix"
#~ msgstr ""

#~ msgid "Generate tensorflow dense matrix from Pauli string sum"
#~ msgstr ""

#~ msgid "the tensorflow dense matrix"
#~ msgstr ""

#~ msgid ""
#~ "The projector of the operator. The "
#~ "operator, as a linear operator, on "
#~ "the adjoint of the operator."
#~ msgstr ""

#~ msgid ""
#~ "Set :math:`A` is the operator in "
#~ "matrix form, then the projector of "
#~ "operator is defined as: :math:`A^\\dagger "
#~ "A`"
#~ msgstr ""

#~ msgid "The projector of the operator."
#~ msgstr ""

#~ msgid "The reduced density of the operator."
#~ msgstr ""

#~ msgid ""
#~ "Set :math:`A` is the matrix of the"
#~ " operator, then the reduced density "
#~ "is defined as:"
#~ msgstr ""

#~ msgid "\\mathrm{Tr}_{subsystems}(A^\\dagger A)"
#~ msgstr ""

#~ msgid ""
#~ "Firstly, take the projector of the "
#~ "operator, then trace out the subsystems"
#~ " to trace out are supplied as "
#~ "indices, so that dangling edges are "
#~ "connected to each other as: "
#~ "`out_edges[i] ^ in_edges[i] for i in "
#~ "subsystems_to_trace_out` This does not modify"
#~ " the original network. The original "
#~ "ordering of the remaining subsystems is"
#~ " maintained."
#~ msgstr ""

#~ msgid ""
#~ "The QuOperator of the reduced density"
#~ " of the operator with given "
#~ "subsystems."
#~ msgstr ""

#~ msgid ""
#~ "Contracts the tensor network in place"
#~ " and returns the final tensor in "
#~ "two dimentional matrix. The default "
#~ "ordering for the axes of the final"
#~ " tensor is: (:math:`\\prod` dimension of"
#~ " out_edges, :math:`\\prod` dimension of "
#~ "in_edges)"
#~ msgstr ""

#~ msgid "The two-dimentional tensor representing the operator."
#~ msgstr ""

#~ msgid ""
#~ "Returns a bool indicating if QuOperator"
#~ " is an adjoint vector. Examples can"
#~ " be found in the `QuOperator.from_tensor`."
#~ msgstr ""

#~ msgid ""
#~ "Returns a bool indicating if QuOperator"
#~ " is a scalar. Examples can be "
#~ "found in the `QuOperator.from_tensor`."
#~ msgstr ""

#~ msgid ""
#~ "Returns a bool indicating if QuOperator"
#~ " is a vector. Examples can be "
#~ "found in the `QuOperator.from_tensor`."
#~ msgstr ""

#~ msgid ""
#~ "Tensor product with another operator. "
#~ "Given two operators `A` and `B`, "
#~ "produces a new operator `AB` "
#~ "representing :math:`A  B`. The "
#~ "`out_edges` (`in_edges`) of `AB` is "
#~ "simply the concatenation of the "
#~ "`out_edges` (`in_edges`) of `A.copy()` with"
#~ " that of `B.copy()`: `new_out_edges = "
#~ "[*out_edges_A_copy, *out_edges_B_copy]` `new_in_edges "
#~ "= [*in_edges_A_copy, *in_edges_B_copy]`"
#~ msgstr ""

#~ msgid ""
#~ "Set :math:`A` is the operator in "
#~ "matrix form, then the projector of "
#~ "operator is defined as: :math:`A "
#~ "A^\\dagger`"
#~ msgstr ""

#~ msgid "\\mathrm{Tr}_{subsystems}(A A^\\dagger)"
#~ msgstr ""

#~ msgid "Compute the double state of the given Hamiltonian operator ``h``."
#~ msgstr ""

#~ msgid "Hamiltonian operator in form of Tensor."
#~ msgstr ""

#~ msgid "The double state of ``h`` with the given ``beta``."
#~ msgstr ""

#~ msgid "Return fidelity scalar between two states rho and rho0."
#~ msgstr ""

#~ msgid "\\operatorname{Tr}(\\sqrt{\\sqrt{rho} rho_0 \\sqrt{rho}})"
#~ msgstr ""

#~ msgid "The density matrix in form of Tensor."
#~ msgstr ""

#~ msgid "The sqrtm of a Hermitian matrix ``a``."
#~ msgstr ""

#~ msgid "Compute the Gibbs state of the given Hamiltonian operator ``h``."
#~ msgstr ""

#~ msgid "The Gibbs state of ``h`` with the given ``beta``."
#~ msgstr ""

#~ msgid "Mutual information between AB subsystem described by ``cut``."
#~ msgstr ""

#~ msgid "The AB subsystem."
#~ msgstr ""

#~ msgid "The mutual information between AB subsystem described by ``cut``."
#~ msgstr ""

#~ msgid "Taylor expansion of :math:`ln(x+1)`."
#~ msgstr ""

#~ msgid "The :math:`k` th order, default is 2."
#~ msgstr ""

#~ msgid "The :math:`k` th order of Taylor expansion of :math:`ln(x+1)`."
#~ msgstr ""

#~ msgid ""
#~ "Compute the trace distance between two"
#~ " density matrix ``rho`` and ``rho2``."
#~ msgstr ""

#~ msgid "Epsilon, defaults to 1e-12"
#~ msgstr ""

#~ msgid "The trace distance between two density matrix ``rho`` and ``rho2``."
#~ msgstr ""

#~ msgid "\\operatorname{Tr}(\\prod_i O_i)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the truncated free energy from"
#~ " the given density matrix ``rho``."
#~ msgstr ""

#~ msgid "The :math:`k` th order, defaults to 2"
#~ msgstr ""

#~ msgid "The :math:`k` th order of the truncated free energy."
#~ msgstr ""

#~ msgid ""
#~ "The circuit ansatz is firstly one "
#~ "layer of Hadamard gates and then "
#~ "we have ``nlayers`` blocks of "
#~ ":math:`e^{i\\theta Z_iZ_{i+1}}` two-qubit gate"
#~ " in ladder layout, following rx gate."
#~ msgstr ""

#~ msgid "The circuit"
#~ msgstr ""

#~ msgid "paramter tensor with 2*nlayer*n elements"
#~ msgstr ""

#~ msgid "number of ZZ+RX blocks, defaults to 2"
#~ msgstr ""

#~ msgid ""
#~ "whether use SVD split to reduce ZZ"
#~ " gate bond dimension, defaults to "
#~ "False"
#~ msgstr ""

#~ msgid "The circuit with example ansatz attached"
#~ msgstr ""

#~ msgid ""
#~ "Function decorator wraps the function "
#~ "with the first input and output in"
#~ " the format of circuit, the wrapped"
#~ " function has the first input and "
#~ "the output as the state tensor."
#~ msgstr ""

#~ msgid "Function with the fist input and the output as ``Circuit`` object."
#~ msgstr ""

#~ msgid ""
#~ "Wrapped function with the first input"
#~ " and the output as the state "
#~ "tensor correspondingly."
#~ msgstr ""

#~ msgid "Two-dimensional grid lattice"
#~ msgstr ""

#~ msgid "number of rows"
#~ msgstr ""

#~ msgid "number of cols"
#~ msgstr ""

#~ msgid "return all col edge with 1d index encoding"
#~ msgstr ""

#~ msgid ""
#~ "whether to include pbc edges (periodic"
#~ " boundary condition), defaults to False"
#~ msgstr ""

#~ msgid "list of col edge"
#~ msgstr ""

#~ msgid "return all row edge with 1d index encoding"
#~ msgstr ""

#~ msgid "list of row edge"
#~ msgstr ""

#~ msgid "Get the 2D grid lattice in ``nx.Graph`` format"
#~ msgstr ""

#~ msgid ""
#~ "whether to include pbc edges (periodic"
#~ " boundary condition), defaults to True"
#~ msgstr ""

#~ msgid ""
#~ "Generate a permutation matrix P. Due "
#~ "to the different convention or qubits'"
#~ " order in qiskit and tensorcircuit, "
#~ "the unitary represented by the same "
#~ "circuit is different. They are related"
#~ " by this permutation matrix P: P "
#~ "@ U_qiskit @ P = U_tc"
#~ msgstr ""

#~ msgid "# of qubits"
#~ msgstr ""

#~ msgid "The permutation matrix P"
#~ msgstr ""

#~ msgid ""
#~ "Generate a qiskit quantum circuit using"
#~ " the quantum intermediate representation "
#~ "(qir) in tensorcircuit."
#~ msgstr ""

#~ msgid "qiskit QuantumCircuit object"
#~ msgstr ""

#~ msgid ""
#~ "Generate a tensorcircuit circuit using "
#~ "the quantum circuit data in qiskit."
#~ msgstr ""

#~ msgid "Quantum circuit data from qiskit."
#~ msgstr ""

#~ msgid "Input state of the circuit. Default is None."
#~ msgstr ""

#~ msgid "A quantum circuit in tensorcircuit"
#~ msgstr ""

#~ msgid ""
#~ "Translating from the gate name to "
#~ "gate information including the number of"
#~ " control qubits and the reduced gate"
#~ " name."
#~ msgstr ""

#~ msgid "String of gate name"
#~ msgstr ""

#~ msgid "# of control qubits, reduced gate name"
#~ msgstr ""

#~ msgid ""
#~ "Generate Tex code from 'qir' string "
#~ "to illustrate the circuit structure. "
#~ "This visualization is based on quantikz"
#~ " package."
#~ msgstr ""

#~ msgid "The quantum intermediate representation of a circuit in tensorcircuit."
#~ msgstr ""

#~ msgid "Initial state, default is an all zero state '000...000'."
#~ msgstr ""

#~ msgid "Measurement Basis, default is None which means no"
#~ msgstr ""

#~ msgid ""
#~ "measurement in the end of the "
#~ "circuit. :type measure: Optional[List[str]] "
#~ ":param rcompress: If true, a right "
#~ "compression of the circuit will be "
#~ "conducted. A right compression means we"
#~ " will try to shift gates from "
#~ "right to left if possible. Default "
#~ "is false. :type rcompress: bool :param"
#~ " lcompress: If true, a left "
#~ "compression of the circuit will be "
#~ "conducted. A left compression means we"
#~ " will try to shift gates from "
#~ "left to right if possible. Default "
#~ "is false. :type lcompress: bool :param"
#~ " standalone: If true, the tex code"
#~ " will be designed to generate a "
#~ "standalone document. Default is false "
#~ "which means the generated tex code "
#~ "is just a quantikz code block. "
#~ ":type standalone: bool :param "
#~ "return_string_table: If true, a string "
#~ "table of tex code will also be "
#~ "returned. Default is false. :type "
#~ "return_string_table: bool :return: Tex code"
#~ " of circuit visualization based on "
#~ "quantikz package. If return_string_table is"
#~ " true, a string table of tex "
#~ "code will also be returned. :rtype: "
#~ "Union[str, Tuple[str, List[List[str]]]]"
#~ msgstr ""

#~ msgid ":math:`ket`."
#~ msgstr ""

#~ msgid ""
#~ "Get Pauli string array and weights "
#~ "array for a qubit Hamiltonian as a"
#~ " sum of Pauli strings defined in "
#~ "openfermion QubitOperator."
#~ msgstr ""

#~ msgid "Apply mpo gate in MPO format on the circuit."
#~ msgstr ""

#~ msgid "Apply multicontrol gate in MPO format on the circuit."
#~ msgstr ""

#~ msgid "Returns the amplitude of the circuit given the bitstring l."
#~ msgstr ""

#~ msgid "Apply the gate to two bits with given indexes."
#~ msgstr ""

#~ msgid "The Gate applied on bits."
#~ msgstr ""

#~ msgid "The index of the bit to apply the Gate."
#~ msgstr ""

#~ msgid "Apply the gate to the bit with the given index."
#~ msgstr ""

#~ msgid "The Gate applied on the bit."
#~ msgstr ""

#~ msgid ""
#~ "Return the list of nodes that "
#~ "consititues the expectation value just "
#~ "before the contraction."
#~ msgstr ""

#~ msgid "whether contract the output state firstly, defaults to True"
#~ msgstr ""

#~ msgid "The tensor network for the expectation"
#~ msgstr ""

#~ msgid ""
#~ "if true, we sample from the final"
#~ " state if memory allsows, True is "
#~ "prefered, defaults to False"
#~ msgstr ""

#~ msgid ""
#~ "List (if batch) of tuple (binary "
#~ "configuration tensor and correponding "
#~ "probability)"
#~ msgstr ""

#~ msgid "Sampling bistrings from the circuit output based on quantum amplitudes."
#~ msgstr ""

#~ msgid "tensorcircuit.densitymatrix2"
#~ msgstr ""

#~ msgid "Apply **CNOT** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **CY** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **CZ** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **H** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **I** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **S** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **SWAP** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **T** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **WROOT** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **X** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **Y** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **Z** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Compute :math:`\\prod_{i\\in \\text{index}} s_i`,"
#~ " where the probability for each "
#~ "bitstring is given as a vector "
#~ "``results``."
#~ msgstr ""

#~ msgid "Generate sparse tensor from Pauli string sum"
#~ msgstr ""

#~ msgid "Generate dense matrix from Pauli string sum"
#~ msgstr ""

#~ msgid "Generate Heisenberg Hamiltonian with possible external fields."
#~ msgstr ""

#~ msgid "calibration qubit list"
#~ msgstr ""

#~ msgid "tensorcircuit.cloud"
#~ msgstr ""

#~ msgid "tensorcircuit.cloud.config"
#~ msgstr ""


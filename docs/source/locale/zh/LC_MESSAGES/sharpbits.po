# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-06-27 20:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/sharpbits.rst:3
msgid "TensorCircuit: The Sharp Bits ğŸ”ª"
msgstr "TensorCircuit: å¸¸è§é”™è¯¯ ğŸ”ª"

#: ../../source/sharpbits.rst:5
msgid ""
"Be fast is never for free, though much cheaper in TensorCircuit, but you "
"have to be cautious especially in terms of AD, JIT compatibility. We will"
" go through the main sharp edges ğŸ”ª in this note."
msgstr ""
"è™½ç„¶åœ¨TensorCircuitä¸­é€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯ä½ å¿…é¡»å°å¿ƒï¼Œå°¤å…¶æ˜¯åœ¨ADå’ŒJITå…¼å®¹æ€§"
"æ–¹é¢ã€‚"

#: ../../source/sharpbits.rst:9
msgid "Jit Compatibility"
msgstr "Jit å…¼å®¹æ€§"

#: ../../source/sharpbits.rst:12
msgid "Non tensor input or varying shape tensor input"
msgstr "éå‘é‡è¾“å…¥æˆ–è€…å˜åŒ–å½¢çŠ¶çš„å‘é‡è¾“å…¥"

#: ../../source/sharpbits.rst:14
msgid ""
"The input must be in tensor form and the input tensor shape must be fixed"
" otherwise the recompilation is incurred which is time-consuming. "
"Therefore, if there are input args that are non-tensor or varying shape "
"tensors and frequently change, jit is not recommend."
msgstr ""
"è¾“å…¥å¿…é¡»æ˜¯å¼ é‡å½¢å¼ï¼Œä¸”è¾“å…¥å¼ é‡çš„å½¢çŠ¶å¿…é¡»å›ºå®šï¼Œå¦åˆ™ä¼šé‡æ–°ç¼–è¯‘ï¼Œè¿™æ˜¯éå¸¸è€—"
"æ—¶çš„ã€‚å› æ­¤ï¼Œå¦‚æœæœ‰è¾“å…¥å‚æ•°æ˜¯éå¼ é‡æˆ–è€…å˜åŒ–å½¢çŠ¶çš„å¼ é‡ï¼Œä¸”ç»å¸¸å˜åŒ–ï¼Œä¸å»ºè®®"
"ä½¿ç”¨jitã€‚"

#: ../../source/sharpbits.rst:38
msgid "Mix use of numpy and ML backend APIs"
msgstr "æ··åˆä½¿ç”¨numpyå’ŒMLåç«¯API"

#: ../../source/sharpbits.rst:40
msgid ""
"To make the function jittable and ad-aware, every ops in the function "
"should be called via ML backend (``tc.backend`` API or direct API for the"
" chosen backend ``tf`` or ``jax``). This is because the ML backend has to"
" create the computational graph to carry out AD and JIT transformation. "
"For numpy ops, they will be only called in jit staging time (the first "
"run)."
msgstr ""
"ä¸ºäº†ä½¿å‡½æ•°å¯jitå’Œå¯ADï¼Œå‡½æ•°ä¸­çš„æ¯ä¸ªæ“ä½œéƒ½åº”è¯¥é€šè¿‡MLåç«¯ï¼ˆ``tc.backend`` API"
"æˆ–è€…ç›´æ¥è°ƒç”¨åç«¯API ``tf`` æˆ–è€… ``jax``ï¼‰ã€‚è¿™æ˜¯å› ä¸ºMLåç«¯å¿…é¡»åˆ›å»ºè®¡ç®—"
"å›¾æ¥"è¿›è¡ŒADå’ŒJITè½¬æ¢ã€‚å¯¹äºnumpyæ“ä½œï¼Œå®ƒä»¬åªä¼šåœ¨jitç¼–è¯‘é˜¶æ®µè¢«è°ƒç”¨ï¼ˆç¬¬ä¸€"
"æ¬¡è¿è¡Œï¼‰ã€‚"

#: ../../source/sharpbits.rst:54
msgid ""
"Numpy call inside jitted function can be helpful if you are sure of the "
"behavior is what you expect."
msgstr ""
"å¦‚æœä½ ç¡®å®šnumpyè°ƒç”¨çš„è¡Œä¸ºæ˜¯ä½ æœŸæœ›çš„ï¼Œé‚£ä¹ˆåœ¨jitå‡½æ•°ä¸­è°ƒç”¨numpyæ˜¯æœ‰å¸®åŠ©çš„ã€‚"

#: ../../source/sharpbits.rst:83
msgid "list append under if"
msgstr "ifä¸‹çš„list append"

#: ../../source/sharpbits.rst:85
msgid ""
"Append something to a Python list within if whose condition is based on "
"tensor values will lead to wrong results. Actually values of both branch "
"will be attached to the list. See example below."
msgstr ""
"åœ¨ifæ¡ä»¶åŸºäºå¼ é‡å€¼çš„æƒ…å†µä¸‹ï¼Œå°†å†…å®¹é™„åŠ åˆ°Pythonåˆ—è¡¨ä¸­ä¼šå¯¼è‡´é”™è¯¯çš„ç»“æœã€‚å®é™…"
"ä¸Šï¼Œä¸¤ä¸ªåˆ†æ”¯çš„å€¼éƒ½ä¼šè¢«é™„åŠ åˆ°åˆ—è¡¨ä¸­ã€‚å‚è§ä¸‹é¢çš„ä¾‹å­ã€‚"

#: ../../source/sharpbits.rst:108
msgid ""
"The above code raise ``ConcretizationTypeError`` exception directly for "
"Jax backend since Jax jit doesn't support tensor value if condition."
msgstr ""
"ä¸Šé¢çš„ä»£ç ç›´æ¥ä¸ºJaxåç«¯å¼•å‘äº†``ConcretizationTypeError``å¼‚å¸¸ï¼Œå› ä¸ºJax "
"jitä¸æ”¯æŒå¼ é‡å€¼ifæ¡ä»¶ã€‚"

#: ../../source/sharpbits.rst:110
msgid "Similarly, conditional gate application must be takend carefully."
msgstr "ç±»ä¼¼åœ°ï¼Œå¿…é¡»å°å¿ƒåœ°åº”ç”¨æ¡ä»¶é—¨ã€‚"

#: ../../source/sharpbits.rst:145
msgid "AD Consistency"
msgstr "ADä¸€è‡´æ€§"

#: ../../source/sharpbits.rst:147
msgid ""
"TF and JAX backend manage the differentiation rules differently for "
"complex-valued function (actually up to a complex conjuagte). See issue "
"discussion `tensorflow issue "
"<https://github.com/tensorflow/tensorflow/issues/3348>`_."
msgstr ""
"TFå’ŒJAXåç«¯å¯¹å¤å€¼å‡½æ•°çš„å¾®åˆ†è§„åˆ™çš„ç®¡ç†æ–¹å¼ä¸åŒï¼ˆå®é™…ä¸Šæ˜¯å¤å…±è½­ï¼‰ã€‚å‚è§è®¨è®º "
"`tensorflow issue <https://github.com/tensorflow/tensorflow/issues/3348>`_ã€‚"

#: ../../source/sharpbits.rst:149
msgid ""
"In TensorCircuit, currently we make the difference in AD transparent, "
"namely, when switching the backend, the AD behavior and result for "
"complex valued function can be different and determined by the nature "
"behavior of the corresponding backend framework. All AD relevant ops such"
" as ``grad`` or ``jacrev`` may be affected. Therefore, the user must be "
"careful when dealing with AD on complex valued function in a backend "
"agnostic way in TensorCircuit."
msgstr ""
"åœ¨TensorCircuitä¸­ï¼Œæˆ‘ä»¬ç›®å‰ä½¿ADçš„å·®å¼‚é€æ˜ï¼Œå³åœ¨åˆ‡æ¢åç«¯æ—¶ï¼Œå¤å€¼å‡½æ•°çš„ADè¡Œä¸º"
"å’Œç»“æœå¯èƒ½ä¸åŒï¼Œå¹¶ç”±ç›¸åº”åç«¯æ¡†æ¶çš„æœ¬è´¨è¡Œä¸ºå†³å®šã€‚æ‰€æœ‰ä¸ADç›¸å…³çš„æ“ä½œï¼Œå¦‚ "
"``grad`` æˆ–è€… ``jacrev`` éƒ½å¯èƒ½å—åˆ°å½±å“ã€‚å› æ­¤ï¼Œç”¨æˆ·åœ¨TensorCircuitä¸­ä»¥åç«¯"
"æ— å…³çš„æ–¹å¼å¤„ç†å¤å€¼å‡½æ•°çš„ADæ—¶å¿…é¡»å°å¿ƒã€‚"

#: ../../source/sharpbits.rst:152
msgid ""
"See example script on computing Jacobian with different modes on "
"different backends: `jacobian_cal.py <https://github.com/tencent-quantum-"
"lab/tensorcircuit/blob/master/examples/jacobian_cal.py>`_. Also see the "
"code below for a reference:"
msgstr ""
"å‚è€ƒä¸åŒåç«¯çš„ä¸åŒæ¨¡å¼ä¸‹è®¡ç®—Jacobiançš„ç¤ºä¾‹è„šæœ¬ï¼š`jacobian_cal.py <https://github.com/tencent"
"-quantum-lab/tensorcircuit/blob/master/examples/jacobian_cal.py>`_ã€‚"
"å¦å¤–è¯·å‚è€ƒä¸‹é¢çš„ä»£ç :"

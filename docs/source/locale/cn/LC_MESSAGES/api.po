# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-09 17:10+0800\n"
"PO-Revision-Date: 2022-04-13 14:58+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: cn <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/api/applications.rst:2
msgid "tensorcircuit.applications"
msgstr ""

#: ../../source/api/applications/dqas.rst:2
msgid "tensorcircuit.applications.dqas"
msgstr ""

#: of tensorcircuit.applications.dqas:1
msgid "Modules for DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:1
msgid "DQAS framework entrypoint"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.concat
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.single_qubit_kraus_identity_check
#: tensorcircuit.circuit.Circuit.__init__
#: tensorcircuit.circuit.Circuit.amplitude
#: tensorcircuit.circuit.Circuit.append_from_qir
#: tensorcircuit.circuit.Circuit.apply_double_gate
#: tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.apply_single_gate
#: tensorcircuit.circuit.Circuit.cond_measurement
#: tensorcircuit.circuit.Circuit.depolarizing
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.expectation_before
#: tensorcircuit.circuit.Circuit.from_qir
#: tensorcircuit.circuit.Circuit.from_qiskit
#: tensorcircuit.circuit.Circuit.general_kraus
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.mid_measurement
#: tensorcircuit.circuit.Circuit.replace_inputs
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.circuit.Circuit.select_gate
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.wavefunction
#: tensorcircuit.circuit._expectation_ps tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.runtime_backend
#: tensorcircuit.cons.runtime_dtype tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.densitymatrix.DMCircuit.__init__
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.rgate_theoretical
#: tensorcircuit.gates.rx_gate tensorcircuit.gates.ry_gate
#: tensorcircuit.gates.rz_gate
#: tensorcircuit.interfaces.scipy_optimize_interface
#: tensorcircuit.interfaces.torch_interface
#: tensorcircuit.keras.QuantumLayer.__init__ tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss tensorcircuit.keras.save_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.__init__
#: tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.mid_measurement
#: tensorcircuit.mpscircuit.MPSCircuit.position
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.__init__
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.__init__
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.__init__
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.check_spaces
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.__init__
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.translation.perm_matrix tensorcircuit.translation.qir2qiskit
#: tensorcircuit.translation.qiskit2tc tensorcircuit.utils.append
#: tensorcircuit.utils.return_partial tensorcircuit.vis.gate_name_trans
#: tensorcircuit.vis.qir2tex tensorcircuit.vis.render_pdf
msgid "Parameters"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:3
msgid ""
"function with input of data instance, circuit parameters theta and "
"structural paramter k, return tuple of objective value and gradient with "
"respect to theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:5
msgid "data generator as dataset"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:6
msgid "list of operations as primitive operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:7
msgid "the default layer number of the circuit ansatz"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:8
msgid ""
"shape of circuit parameter pool, in general p_stp*l, where l is the max "
"number of circuit parameters for op in the operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:10
msgid "the same as p in the most times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:11
msgid "batch size of one epoch"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:12
msgid "prethermal update times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:13
msgid "training epochs"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:14
msgid "parallel thread number, 0 to disable multiprocessing model by default"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:15
msgid "set verbose log to print"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:16
msgid "function to output verbose information"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:17
msgid "function return intermiediate result for final history list"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:18
msgid "cutoff probability to avoid peak distribution"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:19
msgid ""
"function accepting list of objective values and return the baseline value"
" used in the next round"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:21
msgid "return noise with the same shape as circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:22
msgid "initial values for circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:23
msgid "initial values for probabilistic model parameters"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:24
msgid "optimizer for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:25
msgid "optimizer for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:26
msgid "optimizer for circuit parameters in prethermal stage"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:27
msgid "fixed structural parameters for prethermal training"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:28
msgid "regularization function for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:29
msgid "regularization function for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel tensorcircuit.circuit.Circuit.amplitude
#: tensorcircuit.circuit.Circuit.cond_measurement
#: tensorcircuit.circuit.Circuit.depolarizing
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.expectation_before
#: tensorcircuit.circuit.Circuit.from_qir
#: tensorcircuit.circuit.Circuit.from_qiskit
#: tensorcircuit.circuit.Circuit.get_quvector
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.matrix
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.to_qir tensorcircuit.circuit.Circuit.to_qiskit
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.vis_tex
#: tensorcircuit.circuit.Circuit.wavefunction
#: tensorcircuit.circuit._expectation_ps tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_contractor
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit
#: tensorcircuit.densitymatrix.DMCircuit.perfect_sampling
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.interfaces.scipy_optimize_interface
#: tensorcircuit.interfaces.torch_interface tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.projector
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.adjoint
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.copy tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.projector
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.trace_product
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.translation.perm_matrix tensorcircuit.translation.qir2qiskit
#: tensorcircuit.translation.qiskit2tc tensorcircuit.utils.append
#: tensorcircuit.utils.return_partial tensorcircuit.vis.gate_name_trans
#: tensorcircuit.vis.render_pdf
msgid "Returns"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:1
msgid ""
"The probabilistic model based DQAS, can use extensively for DQAS case for"
" ``NMF`` probabilistic model."
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:3
msgid "vag func, return loss and nabla lnp"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:4
msgid "keras model"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:5
msgid "sample func of logic with keras model input"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:6
msgid "input data pipeline generator"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:7
msgid "operation pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:8
msgid "depth for DQAS"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:12
msgid "parallel kernels"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:23
msgid "final loss function in terms of average of sub loss for each circuit"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:24
msgid "derivative function for ``loss_func``"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:1
msgid ""
"Call in customized functions and grab variables within DQAS framework "
"function by var name str."
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:3
msgid "The DQAS framework function"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:5
msgid "Variables within the DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.acos
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.asin
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh
#: tensorcircuit.backends.jax_backend.JaxBackend.atan
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.mean
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tan
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel tensorcircuit.circuit.Circuit.amplitude
#: tensorcircuit.circuit.Circuit.cond_measurement
#: tensorcircuit.circuit.Circuit.depolarizing
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.expectation_before
#: tensorcircuit.circuit.Circuit.from_qir
#: tensorcircuit.circuit.Circuit.from_qiskit
#: tensorcircuit.circuit.Circuit.get_quvector
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.matrix
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.to_qir
#: tensorcircuit.circuit.Circuit.unitary_kraus
#: tensorcircuit.circuit.Circuit.vis_tex
#: tensorcircuit.circuit.Circuit.wavefunction
#: tensorcircuit.circuit._expectation_ps tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.runtime_backend
#: tensorcircuit.cons.runtime_contractor tensorcircuit.cons.runtime_dtype
#: tensorcircuit.cons.set_contractor tensorcircuit.cons.set_dtype
#: tensorcircuit.cons.set_function_backend
#: tensorcircuit.cons.set_function_contractor
#: tensorcircuit.cons.set_function_dtype
#: tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.densitymatrix.DMCircuit.densitymatrix
#: tensorcircuit.densitymatrix.DMCircuit.expectation
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit
#: tensorcircuit.densitymatrix.DMCircuit.perfect_sampling
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.interfaces.scipy_optimize_interface
#: tensorcircuit.interfaces.torch_interface tensorcircuit.keras.load_func
#: tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor tensorcircuit.quantum.PauliString2COO
#: tensorcircuit.quantum.PauliStringSum2COO
#: tensorcircuit.quantum.PauliStringSum2COO_numpy
#: tensorcircuit.quantum.PauliStringSum2Dense
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuAdjointVector.projector
#: tensorcircuit.quantum.QuAdjointVector.reduced_density
#: tensorcircuit.quantum.QuOperator.adjoint
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.copy tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.QuVector.projector
#: tensorcircuit.quantum.QuVector.reduced_density
#: tensorcircuit.quantum.correlation_from_counts
#: tensorcircuit.quantum.double_state
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.fidelity tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.gibbs_state
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.mutual_information
#: tensorcircuit.quantum.quantum_constructor tensorcircuit.quantum.quimb2qop
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.renyi_entropy tensorcircuit.quantum.renyi_free_energy
#: tensorcircuit.quantum.spin_by_basis tensorcircuit.quantum.taylorlnm
#: tensorcircuit.quantum.tn2qop tensorcircuit.quantum.trace_distance
#: tensorcircuit.quantum.trace_product
#: tensorcircuit.quantum.truncated_free_energy
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.blocks.Bell_pair_block
#: tensorcircuit.templates.blocks.example_block
#: tensorcircuit.templates.blocks.state_centric
#: tensorcircuit.templates.chems.get_ps
#: tensorcircuit.templates.graphs.Grid2DCoord.all_cols
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.mpo_expectation
#: tensorcircuit.templates.measurements.operator_expectation
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.templates.measurements.spin_glass_measurements
#: tensorcircuit.translation.perm_matrix tensorcircuit.translation.qir2qiskit
#: tensorcircuit.translation.qiskit2tc tensorcircuit.utils.append
#: tensorcircuit.utils.return_partial tensorcircuit.vis.gate_name_trans
#: tensorcircuit.vis.render_pdf
msgid "Return type"
msgstr ""

#: of tensorcircuit.applications.dqas.get_weights:1
msgid ""
"This function works only when nnp has the same shape as stp, i.e. one "
"parameter for each op."
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_kernel:1
msgid "The kernel for multiprocess to run parallel in DQAS function/"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:1
msgid ""
"parallel variational parameter training and search to avoid local minimum"
" not limited to qaoa setup as the function name indicates, as long as you"
" provided suitable `vag_func`"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:6
msgid "data input generator for vag_func"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:7
msgid "vag_kernel"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:10
msgid "number of tries"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:11
msgid "for optimization problem the input is in general fixed so batch is often 1"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:12
msgid "number of parallel jobs"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:13
msgid "mean value of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:14
msgid "std deviation of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.verbose_output:1
msgid "Doesn't support prob model DQAS search."
msgstr ""

#: ../../source/api/applications/graphdata.rst:2
msgid "tensorcircuit.applications.graphdata"
msgstr ""

#: of tensorcircuit.applications.graphdata:1
msgid "Modules for graph instance data and more"
msgstr ""

#: of tensorcircuit.applications.graphdata.dict2graph:1
msgid "```python d = nx.to_dict_of_dicts(g) ```"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:1
msgid "1D PBC chain with n sites."
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:3
msgid "The number of nodes"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:5
msgid "The resulted graph g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduce_edges:3
msgid "all graphs with m edge out from g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:1
msgid ""
"Generate a reduced graph with given ratio of edges compared to the "
"original graph g."
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:3
msgid "The base graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:5
msgid "number of edges kept, default half of the edges"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:6
msgid "The resulted reduced graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:1
msgid "Split the graph in exactly ``split`` piece evenly."
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:3
msgid "The mother graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:5
msgid "The number of the graph we want to divide into, defaults to 2"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:7
msgid "List of graph instance of size ``split``"
msgstr ""

#: ../../source/api/applications/layers.rst:2
msgid "tensorcircuit.applications.layers"
msgstr ""

#: of tensorcircuit.applications.layers:1
msgid "Module for functions adding layers of circuits"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "Hlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyswaplayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "cnotlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "swaplayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_any_gate_layer:1
msgid "$$e^{-i     heta_i \\sigma}$$"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer:1
msgid ""
"The following function should be used to generate layers with special "
"case. As its soundness depends on the nature of the task or problem, it "
"doesn't always make sense."
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_gate_layer:1
#: tensorcircuit.applications.layers.generate_cirq_gate_layer:1
msgid "$$e^{-i heta \\sigma}$$"
msgstr ""

#: of tensorcircuit.applications.layers.generate_gate_layer:1
msgid "$$e^{-i     heta \\sigma}$$"
msgstr ""

#: ../../source/api/applications/utils.rst:2
msgid "tensorcircuit.applications.utils"
msgstr ""

#: of tensorcircuit.applications.utils:1
msgid ""
"A collection of useful function snippets that irrelevant with the main "
"modules or await for further refactor"
msgstr ""

#: of tensorcircuit.applications.utils.FakeModule:1
#: tensorcircuit.applications.vqes.VQNHE:1
#: tensorcircuit.backends.jax_backend.optax_optimizer:1
#: tensorcircuit.backends.tensorflow_backend.keras_optimizer:1
#: tensorcircuit.circuit.Circuit:1 tensorcircuit.densitymatrix.DMCircuit:1
#: tensorcircuit.gates.GateF:1 tensorcircuit.mpscircuit.MPSCircuit:1
#: tensorcircuit.quantum.QuOperator:1
#: tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Bases: :py:class:`object`"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:1
msgid "color cirq circuit SVG for given gates, a small tool to hack the cirq SVG"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:5
msgid "integer coordinate which gate is colored"
msgstr ""

#: of tensorcircuit.applications.utils.repr2array:1
msgid "transform repr form of an array to real numpy array"
msgstr ""

#: ../../source/api/applications/vags.rst:2
msgid "tensorcircuit.applications.vags"
msgstr ""

#: of tensorcircuit.applications.vags:1
msgid "DQAS application kernels as vag functions"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:1
msgid "1D array for full wavefunction, the basis is in lexcical order"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:2
#: tensorcircuit.applications.vags.energy:5
msgid "nx.Graph"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:3
msgid "transformation functions before averaged"
msgstr ""

#: of tensorcircuit.applications.vags.cvar:1
msgid "as f3"
msgstr ""

#: of tensorcircuit.applications.vags.energy:1
msgid "maxcut energy for n qubit wavefunction i-th basis"
msgstr ""

#: of tensorcircuit.applications.vags.energy:3
msgid "ranged from 0 to 2**n-1"
msgstr ""

#: of tensorcircuit.applications.vags.energy:4
#: tensorcircuit.applications.vags.unitary_design:4
msgid "number of qubits"
msgstr ""

#: of tensorcircuit.applications.vags.entanglement_entropy:1
msgid ""
"deprecated as non tf and non flexible, use the combination of "
"``reduced_density_matrix`` and ``entropy`` instead."
msgstr ""

#: of tensorcircuit.applications.vags.entropy:1
#: tensorcircuit.applications.vags.reduced_density_matrix:1
msgid "deprecated, current version in tc.quantum"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:1
msgid ""
"value and gradient, currently only tensorflow backend is supported jax "
"and numpy seems to be slow in circuit simulation anyhow. *deprecated*"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:8
msgid "if lbd=0, take energy as objective"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:9
msgid "if as default 0, overlap will not compute in the process"
msgstr ""

#: of tensorcircuit.applications.vags.gapfilling:1
msgid "Fill single qubit gates according to placeholder on circuit"
msgstr ""

#: of tensorcircuit.applications.vags.heisenberg_measurements:1
msgid "Hamiltonian measurements for Heisenberg model on graph lattice g"
msgstr ""

#: of tensorcircuit.applications.vags.q:1
msgid "short cut for ``cirq.LineQubit(i)``"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_block_vag:1
#: tensorcircuit.applications.vags.qaoa_block_vag_energy:1
msgid "QAOA block encoding kernel, support 2 params in one op"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_train:1
msgid ""
"training QAOA with only optimizing circuit parameters, can be well "
"replaced with more general function `DQAS_search`"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:1
msgid "multi parameter for one layer"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:7
#: tensorcircuit.applications.vags.quantum_qaoa_vag:7
msgid "kw arguments for measurements_func"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:8
msgid "loss function, gradient of nnp"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_qaoa_vag:1
msgid ""
"tensorflow quantum backend compare to qaoa_vag which is tensorcircuit "
"backend"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:1
msgid "Hamiltonian for tfim on lattice defined by graph g"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:8
msgid "cirq.PauliSum as operators for tfq expectation layer"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:1
msgid ""
"generate random wavefunction from approximately Haar measure, reference:"
"  https://doi.org/10.1063/1.4983266"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:5
msgid "repetition of the blocks"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:1
msgid "random Haar measure approximation"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:3
msgid "cirq.Circuit, empty circuit"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:4
msgid "# of qubit"
msgstr ""

#: ../../source/api/applications/van.rst:2
msgid "tensorcircuit.applications.van"
msgstr ""

#: of tensorcircuit.applications.van:1
msgid ""
"One-hot variational autoregressive models for multiple categorical "
"choices beyond binary"
msgstr ""

#: of tensorcircuit.applications.van.MADE:1
#: tensorcircuit.applications.van.NMF:1
#: tensorcircuit.applications.van.PixelCNN:1
msgid "Bases: :py:class:`keras.engine.training.Model`"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:1
#: tensorcircuit.applications.van.NMF.call:1
#: tensorcircuit.applications.van.PixelCNN.call:1
msgid "Calls the model on new inputs and returns the outputs as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:3
#: tensorcircuit.applications.van.NMF.call:3
#: tensorcircuit.applications.van.PixelCNN.call:3
msgid ""
"In this case `call()` just reapplies all ops in the graph to the new "
"inputs (e.g. build a new computational graph from the provided inputs)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:7
#: tensorcircuit.applications.van.NMF.call:7
#: tensorcircuit.applications.van.PixelCNN.call:7
msgid ""
"Note: This method should not be called directly. It is only meant to be "
"overridden when subclassing `tf.keras.Model`. To call a model on an "
"input, always use the `__call__()` method, i.e. `model(inputs)`, which "
"relies on the underlying `call()` method."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.applications.van.MaskedConv2D.call:37
#: tensorcircuit.applications.van.MaskedLinear.call:37
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
#: tensorcircuit.applications.van.ResidualBlock.call:37
#: tensorcircuit.applications.vqes.Linear.call:37
#: tensorcircuit.backends.jax_backend.JaxBackend.eye:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:8
#: tensorcircuit.keras.QuantumLayer.build:11
msgid "Args:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:13
#: tensorcircuit.applications.van.NMF.call:13
#: tensorcircuit.applications.van.PixelCNN.call:13
msgid ""
"inputs: Input tensor, or dict/list/tuple of input tensors. training: "
"Boolean or boolean scalar tensor, indicating whether to run"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:15
#: tensorcircuit.applications.van.NMF.call:15
#: tensorcircuit.applications.van.PixelCNN.call:15
msgid "the `Network` in training mode or inference mode."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "mask: A mask or list of masks. A mask can be either a boolean tensor or"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "None (no mask). For more details, check the guide"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:21
#: tensorcircuit.applications.van.MaskedConv2D.call:39
#: tensorcircuit.applications.van.MaskedLinear.call:39
#: tensorcircuit.applications.van.NMF.call:21
#: tensorcircuit.applications.van.PixelCNN.call:21
#: tensorcircuit.applications.van.ResidualBlock.call:39
#: tensorcircuit.applications.vqes.Linear.call:39
#: tensorcircuit.backends.jax_backend.JaxBackend.abs:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:4
msgid "Returns:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:21
#: tensorcircuit.applications.van.NMF.call:21
#: tensorcircuit.applications.van.PixelCNN.call:21
msgid ""
"A tensor if there is a single output, or a list of tensors if there are "
"more than one outputs."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D:1
#: tensorcircuit.applications.van.MaskedLinear:1
#: tensorcircuit.applications.van.ResidualBlock:1
#: tensorcircuit.applications.vqes.Linear:1 tensorcircuit.keras.QuantumLayer:1
msgid "Bases: :py:class:`keras.engine.base_layer.Layer`"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:1
#: tensorcircuit.keras.QuantumLayer.build:1
msgid "Creates the variables of the layer (optional, for subclass implementers)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:3
#: tensorcircuit.keras.QuantumLayer.build:3
msgid ""
"This is a method that implementers of subclasses of `Layer` or `Model` "
"can override if they need a state-creation step in-between layer "
"instantiation and layer call."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:7
#: tensorcircuit.keras.QuantumLayer.build:7
msgid "This is typically used to create the weights of `Layer` subclasses."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.keras.QuantumLayer.build:11
msgid "input_shape: Instance of `TensorShape`, or list of instances of"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.keras.QuantumLayer.build:11
msgid ""
"`TensorShape` if the layer expects a list of inputs (one instance per "
"input)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:1
#: tensorcircuit.applications.van.MaskedLinear.call:1
#: tensorcircuit.applications.van.ResidualBlock.call:1
#: tensorcircuit.applications.vqes.Linear.call:1
msgid "This is where the layer's logic lives."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:3
#: tensorcircuit.applications.van.MaskedLinear.call:3
#: tensorcircuit.applications.van.ResidualBlock.call:3
#: tensorcircuit.applications.vqes.Linear.call:3
msgid ""
"Note here that `call()` method in `tf.keras` is little bit different from"
" `keras` API. In `keras` API, you can pass support masking for layers as "
"additional arguments. Whereas `tf.keras` has `compute_mask()` method to "
"support masking."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:24
#: tensorcircuit.applications.van.MaskedLinear.call:24
#: tensorcircuit.applications.van.ResidualBlock.call:24
#: tensorcircuit.applications.vqes.Linear.call:24
msgid "inputs: Input tensor, or dict/list/tuple of input tensors."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:10
#: tensorcircuit.applications.van.MaskedLinear.call:10
#: tensorcircuit.applications.van.ResidualBlock.call:10
#: tensorcircuit.applications.vqes.Linear.call:10
msgid ""
"The first positional `inputs` argument is subject to special rules: - "
"`inputs` must be explicitly passed. A layer cannot have zero"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:12
#: tensorcircuit.applications.van.MaskedLinear.call:12
#: tensorcircuit.applications.van.ResidualBlock.call:12
#: tensorcircuit.applications.vqes.Linear.call:12
msgid ""
"arguments, and `inputs` cannot be provided via the default value of a "
"keyword argument."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:14
#: tensorcircuit.applications.van.MaskedLinear.call:14
#: tensorcircuit.applications.van.ResidualBlock.call:14
#: tensorcircuit.applications.vqes.Linear.call:14
msgid "NumPy array or Python scalar values in `inputs` get cast as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:15
#: tensorcircuit.applications.van.MaskedLinear.call:15
#: tensorcircuit.applications.van.ResidualBlock.call:15
#: tensorcircuit.applications.vqes.Linear.call:15
msgid "Keras mask metadata is only collected from `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:16
#: tensorcircuit.applications.van.MaskedLinear.call:16
#: tensorcircuit.applications.van.ResidualBlock.call:16
#: tensorcircuit.applications.vqes.Linear.call:16
msgid ""
"Layers are built (`build(input_shape)` method) using shape info from "
"`inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:18
#: tensorcircuit.applications.van.MaskedLinear.call:18
#: tensorcircuit.applications.van.ResidualBlock.call:18
#: tensorcircuit.applications.vqes.Linear.call:18
msgid "`input_spec` compatibility is only checked against `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:19
#: tensorcircuit.applications.van.MaskedLinear.call:19
#: tensorcircuit.applications.van.ResidualBlock.call:19
#: tensorcircuit.applications.vqes.Linear.call:19
msgid ""
"Mixed precision input casting is only applied to `inputs`. If a layer has"
" tensor arguments in `*args` or `**kwargs`, their casting behavior in "
"mixed precision should be handled manually."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:22
#: tensorcircuit.applications.van.MaskedLinear.call:22
#: tensorcircuit.applications.van.ResidualBlock.call:22
#: tensorcircuit.applications.vqes.Linear.call:22
msgid "The SavedModel input specification is generated using `inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:23
#: tensorcircuit.applications.van.MaskedLinear.call:23
#: tensorcircuit.applications.van.ResidualBlock.call:23
#: tensorcircuit.applications.vqes.Linear.call:23
msgid ""
"Integration with various ecosystem packages like TFMOT, TFLite, TF.js, "
"etc is only supported for `inputs` and not for tensors in positional and "
"keyword arguments."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:26
#: tensorcircuit.applications.van.MaskedLinear.call:26
#: tensorcircuit.applications.van.ResidualBlock.call:26
#: tensorcircuit.applications.vqes.Linear.call:26
msgid "*args: Additional positional arguments. May contain tensors, although"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:27
#: tensorcircuit.applications.van.MaskedLinear.call:27
#: tensorcircuit.applications.van.ResidualBlock.call:27
#: tensorcircuit.applications.vqes.Linear.call:27
msgid "this is not recommended, for the reasons above."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:37
#: tensorcircuit.applications.van.MaskedLinear.call:37
#: tensorcircuit.applications.van.ResidualBlock.call:37
#: tensorcircuit.applications.vqes.Linear.call:37
msgid "**kwargs: Additional keyword arguments. May contain tensors, although"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:29
#: tensorcircuit.applications.van.MaskedLinear.call:29
#: tensorcircuit.applications.van.ResidualBlock.call:29
#: tensorcircuit.applications.vqes.Linear.call:29
msgid ""
"this is not recommended, for the reasons above. The following optional "
"keyword arguments are reserved: - `training`: Boolean scalar tensor of "
"Python boolean indicating"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:32
#: tensorcircuit.applications.van.MaskedLinear.call:32
#: tensorcircuit.applications.van.ResidualBlock.call:32
#: tensorcircuit.applications.vqes.Linear.call:32
msgid "whether the `call` is meant for training or inference."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:33
#: tensorcircuit.applications.van.MaskedLinear.call:33
#: tensorcircuit.applications.van.ResidualBlock.call:33
#: tensorcircuit.applications.vqes.Linear.call:33
msgid ""
"`mask`: Boolean input mask. If the layer's `call()` method takes a `mask`"
" argument, its default value will be set to the mask generated for "
"`inputs` by the previous layer (if `input` did come from a layer that "
"generated a corresponding mask, i.e. if it came from a Keras layer with "
"masking support)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:40
#: tensorcircuit.applications.van.MaskedLinear.call:40
#: tensorcircuit.applications.van.ResidualBlock.call:40
#: tensorcircuit.applications.vqes.Linear.call:40
msgid "A tensor or list/tuple of tensors."
msgstr ""

#: ../../source/api/applications/vqes.rst:2
msgid "tensorcircuit.applications.vqes"
msgstr ""

#: of tensorcircuit.applications.vqes:1
msgid "VQNHE application"
msgstr ""

#: of tensorcircuit.applications.vqes.JointSchedule:1
msgid ""
"Bases: "
":py:class:`keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule`"
msgstr ""

#: of tensorcircuit.applications.vqes.Linear:1
msgid "Dense layer but with complex weights, used for building complex RBM"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:1
msgid "VQNHE"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.evaluation:5
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:7
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:7
#: tensorcircuit.backends.jax_backend.JaxBackend.concat:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:5
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:7
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:9
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:8
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:11
#: tensorcircuit.backends.jax_backend.JaxBackend.max:3
#: tensorcircuit.backends.jax_backend.JaxBackend.max:7
#: tensorcircuit.backends.jax_backend.JaxBackend.min:3
#: tensorcircuit.backends.jax_backend.JaxBackend.min:7
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:6
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:8
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:3
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:5
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:7
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:9
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:15
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:3
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:7
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:3
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:36
#: tensorcircuit.simplify.pseudo_contract_between:3
#: tensorcircuit.simplify.pseudo_contract_between:5
#: tensorcircuit.simplify.pseudo_contract_between:7
#: tensorcircuit.templates.graphs.Line1D:3
#: tensorcircuit.templates.graphs.Line1D:7
msgid "[description]"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.plain_evaluation:1
msgid "VQE"
msgstr ""

#: ../../source/api/backends.rst:2
msgid "tensorcircuit.backends"
msgstr ""

#: ../../source/api/backends/backend_factory.rst:2
msgid "tensorcircuit.backends.backend_factory"
msgstr ""

#: of tensorcircuit.backends.backend_factory:1
msgid "Backend register"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:1
msgid "Get the `tc.backend` object."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.circuit.Circuit.expectation tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.gates.bmatrix tensorcircuit.keras.load_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.eval_matrix
#: tensorcircuit.quantum.check_spaces
msgid "Raises"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:5
msgid "Backend doesn't exist for `backend` argument."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:6
#: tensorcircuit.cons.set_tensornetwork_backend:32
msgid "The `tc.backend` object that with all registered universal functions."
msgstr ""

#: ../../source/api/backends/jax_backend.rst:2
msgid "tensorcircuit.backends.jax_backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend:1
msgid "Backend magic inherited from tensornetwork: jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid "Bases: :py:class:`tensornetwork.backends.jax.jax_backend.JaxBackend`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid ""
"See the original backend API at `jax backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:1
msgid "Returns the elementwise absolute value of tensor. Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:3
msgid "tensor: An input tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:5
msgid "tensor: Its elementwise absolute value."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acos:1
#: tensorcircuit.backends.jax_backend.JaxBackend.asin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:1
msgid "Return the acos of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acos:3
#: tensorcircuit.backends.jax_backend.JaxBackend.acosh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.asin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.asinh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.atan:3
#: tensorcircuit.backends.jax_backend.JaxBackend.atan2:3
#: tensorcircuit.backends.jax_backend.JaxBackend.copy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cos:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cosh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:5
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sinh:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tan:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tanh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:3
msgid "tensor in matrix form"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acos:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acos:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acos:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acos:5
msgid "acos of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acosh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:1
msgid "Return the acosh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.acosh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.acosh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.acosh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.acosh:5
msgid "acosh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:1
msgid "Return the index of maximum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:5
msgid "[description], defaults to 0, different behavior from numpy defaults!"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:1
msgid "Return the index of minimum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asin:5
msgid "asin of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asinh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:1
msgid "Return the asinh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.asinh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.asinh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.asinh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.asinh:5
msgid "asinh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:1
msgid "Return the atan of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan:5
msgid "atan of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan2:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:1
msgid "Return the atan of a tensor ``y``/``x``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.atan2:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.atan2:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.atan2:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.atan2:5
msgid "atan2 of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:1
msgid "Cast the tensor dtype of a ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:3
#: tensorcircuit.backends.jax_backend.JaxBackend.imag:3
#: tensorcircuit.backends.jax_backend.JaxBackend.real:3
#: tensorcircuit.backends.jax_backend.JaxBackend.size:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:3
msgid "tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:5
msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:7
msgid "``a`` of new dtype"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:31
msgid "[description], defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cond:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:1
msgid ""
"The native cond for XLA compiling, wrapper for ``tf.cond`` and limited "
"functionality of ``jax.lax.cond``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.convert_to_tensor:1
msgid "Convert a np.array or a tensor to a tensor type for the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:1
msgid ""
"Generate the coo format sparse matrix from indices and values, which is "
"the only sparse format supported in different ML backends."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:4
msgid "shape [n, 2] for n non zero values in the returned matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:6
msgid "shape [n]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:8
msgid "Tuple[int, ...]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:1
msgid "Return the expm of ``a``, matrix exponential."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:5
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:5
msgid "matrix exponential of matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:1
msgid "Return the cosine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:5
msgid "cosine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cosh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:1
msgid "Return the cosh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cosh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cosh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cosh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cosh:5
msgid "cosh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:1
msgid "Return the cumulative sum of the elements along a given axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:5
msgid ""
"The default behavior is the same as numpy, different from tf/torch as "
"cumsum of the flatten 1D array, defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:1
msgid "Get the eigenvalues of matrix ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eigvalsh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eigvalsh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eigvalsh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eigvalsh:5
msgid "eigenvalues of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.expm:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:1
msgid "Return the copy of tensor ''a''."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:4
msgid "Return an identity matrix of dimension `dim`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:2
msgid ""
"Depending on specific backends, `dim` has to be either an int (numpy, "
"torch, tensorflow) or a `ShapeType` object (for block-sparse backends). "
"Block-sparse behavior is currently not supported"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:7
msgid ""
"N (int): The dimension of the returned matrix. dtype: The dtype of the "
"returned matrix. M (int): The dimension of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:1
msgid "Return the function which is the grad function of input ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel tensorcircuit.circuit.Circuit.amplitude
#: tensorcircuit.circuit.Circuit.append_from_qir
#: tensorcircuit.circuit.Circuit.apply_double_gate
#: tensorcircuit.circuit.Circuit.apply_single_gate
#: tensorcircuit.circuit.Circuit.cond_measurement
#: tensorcircuit.circuit.Circuit.draw tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.from_qir
#: tensorcircuit.circuit.Circuit.from_qiskit
#: tensorcircuit.circuit.Circuit.measure_reference
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.circuit.Circuit.to_qir tensorcircuit.circuit._expectation_ps
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.interfaces.scipy_optimize_interface
#: tensorcircuit.interfaces.torch_interface tensorcircuit.keras.load_func
#: tensorcircuit.keras.save_func
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.correlation_from_counts tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.free_energy
#: tensorcircuit.quantum.heisenberg_hamiltonian tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.renyi_free_energy tensorcircuit.quantum.spin_by_basis
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.translation.qir2qiskit tensorcircuit.translation.qiskit2tc
#: tensorcircuit.utils.append tensorcircuit.utils.return_partial
#: tensorcircuit.vis.gate_name_trans tensorcircuit.vis.qir2tex
#: tensorcircuit.vis.render_pdf
msgid "Example"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:13
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:13
msgid "the function to be differentiated"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:15
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:15
msgid ""
"the position of args in ``f`` that are to be differentiated, defaults to "
"be 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:17
msgid "the grad function of ``f`` with the same set of arguments as ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:1
msgid "Return 1.j in as a tensor compatible with the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:3
msgid "\"complex64\" or \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:5
msgid "1.j tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:1
msgid "Return the elementwise imaginary value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:5
msgid "imaginary value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:1
msgid "[summary]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:5
msgid "The possible options"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:7
msgid "Sampling output shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:9
msgid ""
"probability for each option in a, defaults to None, as equal probability "
"distribution"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:1
msgid ""
"Call the random normal function with the random state management behind "
"the scene."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:7
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:9
msgid "[description], defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:9
msgid "[description], defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:1
msgid "Determine whether the type of input ``a`` is  ``sparse``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:3
msgid "input matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:5
msgid "a bool indicating whether the matrix ``a`` is sparse"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:1
msgid "Return a boolean on whether ``a`` is a tensor in backend package."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:3
msgid "a tensor to be determined"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:5
msgid "whether ``a`` is a tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:1
msgid "Return the jitted version of function ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:3
msgid "function to be jitted"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:5
msgid "index of args that doesn't regarded as tensor, only work for jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:8
msgid ""
"whether open XLA compilation, only works for tensorflow backend, defaults"
" False since several ops has no XLA correspondence"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:11
msgid "jitted version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:1
msgid ""
"Function that computes a (forward-mode) Jacobian-vector product of ``f``."
" Strictly speaking, this function is value_and_jvp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:4
msgid "The function to compute jvp"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:6
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:7
msgid "input for ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:8
msgid "tangents"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:10
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:10
msgid ""
"(``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as the "
"output of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:1
msgid "Return the kronecker product of two matrices ``a`` and ``b``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:7
msgid "kronecker product of ``a`` and ``b``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:1
msgid "Return the maximum of an array or maximum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:5
#: tensorcircuit.backends.jax_backend.JaxBackend.min:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:5
#: tensorcircuit.keras.QuantumLayer.__init__:10
msgid "[description], defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:1
msgid "Compute the arithmetic mean for ``a`` along the specified ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:3
msgid "tensor to take average"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:5
msgid "the axis to take mean, defaults to None indicating sum over flatten array"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:7
msgid "_description_, defaults to False"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.mean:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.mean:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.mean:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.mean:9
#: tensorcircuit.cons.runtime_contractor:3
#: tensorcircuit.cons.set_function_contractor:3
#: tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:6
msgid "_description_"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.min:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:1
msgid "Return the minimum of an array or minimum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:1
msgid ""
"Return the numpy array of a tensor ``a``, but may not work in a jitted "
"function."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:5
msgid "numpy array of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:1
msgid ""
"One-hot encodes the given ``a``. Each index in the input ``a`` is encoded"
" as a vector of zeros of length ``num`` with the element at index set to "
"one:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:5
msgid "input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:7
msgid "number of features in onehot dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:9
msgid "onehot tensor with the last extra dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:1
msgid ""
"Return an ones-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends). Block-sparse behavior is currently "
"not supported Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:7
#: tensorcircuit.backends.jax_backend.JaxBackend.zeros:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:8
msgid ""
"shape (int): The dimension of the returned matrix. dtype: The dtype of "
"the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.random_split:1
msgid ""
"A jax like split API, but it doesn't split the key generator for other "
"backends. It is just for a consistent interface of random code; make sure"
" you know what the function actually does. This function is mainly a "
"utility to write backend agnostic code instead of doing magic things."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:1
msgid "Return the elementwise real value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:5
msgid "real value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:1
msgid ""
"Rectified linear unit activation function. Computes the element-wise "
"function:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:4
msgid "\\mathrm{relu}(x)=\\max(x,0)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:9
msgid "Input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:11
msgid "Tensor after relu"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.scatter:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:1
msgid ""
"Roughly equivalent to operand[indices] = updates, indices only support "
"shape with rank 2 for now."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:1
msgid "Set the random state attached to the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:3
msgid "the random seed, defaults to be None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:5
msgid ""
"If set to be true, only get the random state in return instead of setting"
" the state on the backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sigmoid:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sigmoid:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sigmoid:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sigmoid:1
msgid "Compute sigmoid of input ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:1
msgid "Return the  elementwise sine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:5
msgid "sine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sinh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:1
msgid "Return the sinh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sinh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sinh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sinh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sinh:5
msgid "sinh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:1
msgid "Return the total number of elements in ``a`` in tensor form."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:5
msgid "the total number of elements in ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:1
msgid ""
"Softmax function. Computes the function which rescales elements to the "
"range [0,1] such that the elements along axis sum to 1."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:4
msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:9
msgid "Tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:11
msgid ""
"A dimension along which Softmax will be computed , defaults to None for "
"all axis sum."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stack:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:7
msgid "concatenated tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:1
msgid "Solve the linear system Ax=b and return the solution x."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:3
msgid "The multiplied matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:5
msgid "The resulted matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:7
msgid "The solution of the linear system."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:1
msgid "A sparse matrix multiplies a dense matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:3
msgid "a sparse matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:5
msgid "a dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:7
msgid "dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:1
msgid "Concatenates a sequence of tensors ``a`` along a new dimension ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:3
msgid "List of tensors in the same shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:5
msgid "the stack axis, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:3
msgid "stateful register for each package"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:7
msgid "shape of output sampling tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:11
msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:1
msgid "Uniform random sampler from ``low`` to ``high``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:5
msgid "shape of output sampling tensor, defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:1
msgid "Stop backpropagation from ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.switch:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:1
msgid "``branches[index]()``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tan:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:1
msgid "Return the tan of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tan:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tan:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tan:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tan:5
msgid "tan of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tanh:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:1
msgid "Return the tanh of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tanh:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tanh:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tanh:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tanh:5
msgid "tanh of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:1
msgid "Constructs a tensor by tiling a given tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:5
msgid "1d tensor with length the same as the rank of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.to_dense:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:1
msgid "Convert a sparse matrix to dense tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.to_dense:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:5
msgid "the resulted dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:1
msgid ""
"Find the unique elements and their corresponding counts of the given "
"tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:5
msgid "Unique elements, corresponding counts"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:1
msgid "Return the function which returns the value and grad of ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:17
msgid ""
"the value and grad function of ``f`` with the same set of arguments as "
"``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:1
msgid ""
"Return the VVAG function of ``f``. The inputs for ``f`` is (args[0], "
"args[1], args[2], ...), and the output of ``f`` is a scalar. Suppose "
"VVAG(f) is a function with inputs in the form (vargs[0], args[1], "
"args[2], ...), where vagrs[0] has one extra dimension than args[0] in the"
" first axis and consistent with args[0] in shape for remaining "
"dimensions, i.e. shape(vargs[0]) = [batch] + shape(args[0]). (We only "
"cover cases where ``vectorized_argnums`` defaults to 0 here for "
"demonstration). VVAG(f) returns a tuple as a value tensor with shape "
"[batch, 1] and a gradient tuple with shape: ([batch]+shape(args[argnum]) "
"for argnum in argnums). The gradient for argnums=k is defined as"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:9
msgid ""
"g^k = \\frac{\\partial \\sum_{i\\in batch} f(vargs[0][i], args[1], "
"...)}{\\partial args[k]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:13
msgid "Therefore, if argnums=0, the gradient is reduced to"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:15
msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:19
msgid ""
", which is specifically suitable for batched VQE optimization, where "
"args[0] is the circuit parameters."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:21
msgid "And if argnums=1, the gradient is like"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:23
msgid ""
"g^1_i = \\frac{\\partial \\sum_j f(vargs[0][j], args[1])}{\\partial "
"args[1][i]}\n"
"\n"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:26
msgid ""
", which is suitable for quantum machine learning scenarios, where ``f`` "
"is the loss function, args[0] corresponds to the input data and args[1] "
"corresponds to the weights in the QML model."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:6
msgid ""
"the args to be vectorized, these arguments should share the same batch "
"shape in the fist dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:1
msgid ""
"Function that computes the dot product between a vector v and the "
"Jacobian of the given function at the point given by the inputs. (reverse"
" mode AD relevant) Strictly speaking, this function is value_and_vjp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:5
msgid "the function to carry out vjp calculation"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:9
msgid ""
"value vector or gradient from downstream in reverse mode AD the same "
"shape as return of function ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:12
msgid "(``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:1
msgid ""
"Return the vectorized map or batched version of ``f`` on the first extra "
"axis. The general interface supports ``f`` with multiple arguments and "
"broadcast in the fist dimension."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:4
msgid "function to be broadcasted."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:9
msgid "vmap version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:1
msgid ""
"Return a zeros-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:5
msgid "Block-sparse behavior is currently not supported Args:"
msgstr ""

#: ../../source/api/backends/numpy_backend.rst:2
msgid "tensorcircuit.backends.numpy_backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend:1
msgid "Backend magic inherited from tensornetwork: numpy backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid "Bases: :py:class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid ""
"see the original backend API at `numpy backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
msgstr ""

#: ../../source/api/backends/pytorch_backend.rst:2
msgid "tensorcircuit.backends.pytorch_backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend:1
msgid "Backend magic inherited from tensornetwork: pytorch backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"See the original backend API at `pytorch backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:4
msgid ""
"Note the functionality provided by pytorch backend is incomplete, it "
"currenly lacks native efficicent jit and vmap support."
msgstr ""

#: ../../source/api/backends/tensorflow_backend.rst:2
msgid "tensorcircuit.backends.tensorflow_backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend:1
msgid "Backend magic inherited from tensornetwork: tensorflow backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"See the original backend API at `tensorflow backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:1
msgid ""
"Return ``operand[indices]``, both ``operand`` and ``indices`` are rank-1 "
"tensor."
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:3
msgid "rank-1 tensor"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:5
msgid "rank-1 tensor with int dtype"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.gather1d:7
msgid "``operand[indices]``"
msgstr ""

#: ../../source/api/channels.rst:2
msgid "tensorcircuit.channels"
msgstr ""

#: of tensorcircuit.channels:1
msgid "Some common noise quantum channels."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:1
msgid ""
"Return an amplitude damping channel. Notice: Amplitude damping "
"corrspondings to p = 1."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:4
msgid ""
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    \\sqrt{1-\\gamma} & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    \\sqrt{\\gamma} & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:31
msgid "the damping parameter of amplitude (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:33
msgid ":math:`p`"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:35
msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:1
msgid "Return a Depolarizing Channel"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:3
msgid ""
"\\sqrt{1-p_x-p_y-p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_x}\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    1 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_y}\n"
"\\begin{bmatrix}\n"
"    0 & -1j\\\\\n"
"    1j & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & -1\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:30
msgid ":math:`p_x`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:32
msgid ":math:`p_y`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:34
msgid ":math:`p_z`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:36
msgid "Sequences of Gates"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:1
msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:3
msgid ""
"\\sum_{k}^{} K_k \\otimes K_k^{\\dagger}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:6
msgid "A sequence of Gate"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:8
msgid "The corresponding Tensor of the list of Kraus operators"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:1
msgid "Return a phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:18
msgid "The damping parameter of phase (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:20
msgid "A phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.resetchannel:1
#: tensorcircuit.channels.resetchannel:18
msgid "Reset channel"
msgstr ""

#: of tensorcircuit.channels.resetchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:1
msgid "Check identity of a single qubit Kraus operators."
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check
msgid "Examples"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:8
msgid ""
"\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:11
msgid "List of Kraus operators."
msgstr ""

#: ../../source/api/circuit.rst:2
msgid "tensorcircuit.circuit"
msgstr ""

#: of tensorcircuit.circuit:1
msgid "Quantum circuit: the state simulator"
msgstr ""

#: of tensorcircuit.circuit.Circuit:1
msgid "``Circuit`` class. Simple usage demo below."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ANY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.any_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:3
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:4
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:4
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:3
msgid "Qubit number that the gate applies on."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:7
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:7
msgid "Parameters for the gate."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CNOT** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cnot_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid "Qubit number that the gate applies on. The matrix for the gate is"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CR** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cr_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.crx_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.cry_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CRZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.crz_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CY** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cy_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"0.-1.j\\\\    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.-1.j\\\\    "
"0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **CZ** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.cz_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **EXP** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.exp_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **EXP1** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.exp1_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **FREDKIN** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.fredkin_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **H** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.h_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    "
"0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    0.70710677+0.j"
" & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **I** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.i_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ISWAP** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.iswap_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+1.j & 0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply mpo gate in MPO format on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply multicontrol gate in MPO format on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.orx_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.ory_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **ORZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.orz_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OX** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.ox_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\"
"    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    1.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OY** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.oy_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 0.-1.j & 0.+0.j & 0.+0.j\\\\"
"    0.+1.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    0.+0.j & 0.-1.j & 0.+0.j & 0.+0.j\\\\    0.+1.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **OZ** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.oz_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & -1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"-1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **R** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.r_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RX** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rx_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RY** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.ry_gate`."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid ""
"Apply **RZ** gate with parameters on the circuit. See "
":py:meth:`tensorcircuit.gates.rz_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **S** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.s_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **SD** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.sd_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **SWAP** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.swap_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **T** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.t_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **TD** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.td_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **TOFFOLI** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.toffoli_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **WROOT** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.wroot_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    "
"0.5 & -0.5j & 0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    0.5 & -0.5j & "
"0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **X** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.x_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **Y** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.y_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid ""
"Apply **Z** gate on the circuit. See "
":py:meth:`tensorcircuit.gates.z_gate`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number that the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_gate_delayed.<locals>.apply:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:1
msgid "Circuit object based on state simulator."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:3
#: tensorcircuit.mpscircuit.MPSCircuit.__init__:3
msgid "The number of qubits in the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``inputs`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:8
#: tensorcircuit.circuit.Circuit.replace_mps_inputs:18
msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:10
msgid ""
"dict if two qubit gate is ready for split, including parameters for at "
"least one of ``max_singular_values`` and ``max_truncation_err``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.amplitude:1
msgid "Returns the amplitude of the circuit given the bitstring l."
msgstr ""

#: of tensorcircuit.circuit.Circuit.amplitude:13
msgid "The bitstring of 0 and 1s."
msgstr ""

#: of tensorcircuit.circuit.Circuit.amplitude:15
msgid "The amplitude of the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.append_from_qir:1
msgid ""
"Apply the ciurict in form of quantum intermediate representation after "
"the current cirucit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.append_from_qir:18
msgid "The quantum intermediate representation."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_double_gate:1
msgid "Apply the gate to two bits with given indexes."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_double_gate:11
msgid "The Gate applied on bits."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_double_gate:13
#: tensorcircuit.circuit.Circuit.apply_double_gate:15
#: tensorcircuit.circuit.Circuit.apply_single_gate:13
msgid "The index of the bit to apply the Gate."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:1
msgid ""
"Monte Carlo trajectory simulation of general Kraus channel whose Kraus "
"operators cannot be amplified to unitary operators. For unitary operators"
" composed Kraus channel, :py:meth:`unitary_kraus` is much faster."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:5
msgid ""
"This function is jittable in theory. But only jax+GPU combination is "
"recommended for jit since the graph building time is too long for other "
"backend options; though the running time of the function is very fast for"
" every case."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:9
msgid "A list of ``tn.Node`` for Kraus operators."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:11
msgid "The qubits index that Kraus channel is applied on."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:13
msgid ""
"Random tensor uniformly between 0 or 1, defaults to be None, when the "
"random number will be generated automatically"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_single_gate:1
msgid "Apply the gate to the bit with the given index."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_single_gate:11
msgid "The Gate applied on the bit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.cond_measurement:1
msgid ""
"Measurement on z basis at ``index`` qubit based on quantum amplitude (not"
" post-selection). The highlight is that this method can return the "
"measured result as a int Tensor and thus maintained a jittable pipeline."
msgstr ""

#: of tensorcircuit.circuit.Circuit.cond_measurement:14
msgid "the qubit for the z-basis measurement"
msgstr ""

#: of tensorcircuit.circuit.Circuit.cond_measurement:16
msgid "0 or 1 for z measurement on up and down freedom"
msgstr ""

#: of tensorcircuit.circuit.Circuit.select_gate:1
msgid "Apply ``which``-th gate from ``kraus`` list, i.e. apply kraus[which]"
msgstr ""

#: of tensorcircuit.circuit.Circuit.select_gate:3
msgid "Tensor of shape [] and dtype int"
msgstr ""

#: of tensorcircuit.circuit.Circuit.select_gate:5
msgid "A list of gate in the form of ``tc.gate`` or Tensor"
msgstr ""

#: of tensorcircuit.circuit.Circuit.select_gate:7
msgid "the qubit lines the gate applied on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:1
msgid ""
"Apply depolarizing channel in a Monte Carlo way, i.e. for each call of "
"this method, one of gates from X, Y, Z, I are applied on the circuit "
"based on the probability indicated by ``px``, ``py``, ``pz``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:6
msgid "The qubit that depolarizing channel is on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:8
msgid "probability for X noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:10
msgid "probability for Y noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:12
msgid "probability for Z noise"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:14
msgid "random seed uniformly from 0 to 1, defaults to None (generated implicitly)"
msgstr ""

#: of tensorcircuit.circuit.Circuit.depolarizing:16
msgid "int Tensor, the element lookup: [0: x, 1: y, 2: z, 3: I]"
msgstr ""

#: of tensorcircuit.circuit.Circuit.draw:1
msgid ""
"Visualise the circuit. This method recevies the keywords as same as "
"qiskit.circuit.QuantumCircuit.draw. More details can be found here: "
"https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.draw.html."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:1
#: tensorcircuit.densitymatrix.DMCircuit.expectation:1
msgid "Compute the expectation of corresponding operators."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:10
#: tensorcircuit.densitymatrix.DMCircuit.expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(tc.gates.z(), [1, ]), "
"(tc.gates.x(), [2, ])`` is for operator :math:`Z_1X_2`."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:13
msgid ""
"If True, then the wavefunction tensor is cached for further expectation "
"evaluation, defaults to be true."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:16
#: tensorcircuit.circuit.expectation:50
msgid "\"Cannot measure two operators in one index\""
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:17
#: tensorcircuit.densitymatrix.DMCircuit.expectation:6
msgid "Tensor with one element"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation_before:1
msgid ""
"Return the list of nodes that consititues the expectation value just "
"before the contraction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation_before:3
msgid "whether contract the output state firstly, defaults to True"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation_before:5
msgid "The tensor network for the expectation"
msgstr ""

#: of tensorcircuit.circuit._expectation_ps:1
msgid ""
"Shortcut for Pauli string expectation. x, y, z list are for X, Y, Z "
"positions"
msgstr ""

#: of tensorcircuit.circuit._expectation_ps:12
#: tensorcircuit.circuit._expectation_ps:14
#: tensorcircuit.circuit._expectation_ps:16
msgid "_description_, defaults to None"
msgstr ""

#: of tensorcircuit.circuit._expectation_ps:18
msgid "whether to cache and reuse the wavefunction, defaults to True"
msgstr ""

#: of tensorcircuit.circuit._expectation_ps:20
msgid "Expectation value"
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qir:1
msgid "Restore the circuit from the quantum intermediate representation."
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qir:21
#: tensorcircuit.translation.qir2qiskit:14
msgid "The quantum intermediate representation of a circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qir:23
msgid "Extra circuit parameters."
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qir:25
msgid "The circuit have same gates in the qir."
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qiskit:1
msgid "Import Qiskit QuantumCircuit object as a ``tc.Circuit`` object."
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qiskit:12
msgid "Qiskit Circuit object"
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qiskit:14
msgid "The number of qubits for the circuit"
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qiskit:16
msgid "possible input wavefunction for ``tc.Circuit``, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.from_qiskit:18
msgid "The same circuit but as tensorcircuit object"
msgstr ""

#: of tensorcircuit.circuit.Circuit.get_quvector:1
msgid ""
"Get the representation of the output state in the form of ``QuVector`` "
"while maintaining the circuit uncomputed"
msgstr ""

#: of tensorcircuit.circuit.Circuit.get_quvector:4
msgid "``QuVector`` representation of the output state from the circuit"
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:1
msgid "[WIP], check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:3
msgid "The bool indicating whether the circuit is legal"
msgstr ""

#: of tensorcircuit.circuit.Circuit.matrix:1
msgid ""
"Get the unitary matrix for the circuit irrespective with the circuit "
"input state."
msgstr ""

#: of tensorcircuit.circuit.Circuit.matrix:3
msgid "The circuit unitary matrix"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_jit:1
msgid ""
"Take measurement to the given quantum lines. This method is jittable is "
"and about 100 times faster than unjit version!"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_jit:4
#: tensorcircuit.circuit.Circuit.measure_reference:16
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit:3
msgid "Measure on which quantum line."
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_jit:6
#: tensorcircuit.circuit.Circuit.measure_reference:17
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit:5
msgid "If true, theoretical probability is also returned."
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_jit:8
#: tensorcircuit.circuit.Circuit.measure_reference:18
#: tensorcircuit.densitymatrix.DMCircuit.measure_jit:7
msgid "The sample output and probability (optional) of the quantum line."
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure_reference:1
msgid "Take measurement on the given quantum lines by ``index``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:1
msgid ""
"Middle measurement in z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalize the state manually if needed. This is a post-selection method "
"as keep is provided as a prior."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:5
msgid "The index of qubit that the Z direction postselection applied on."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:7
msgid "0 for spin up, 1 for spin down, defaults to be 0."
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:1
msgid ""
"Sampling bistrings from the circuit output based on quantum amplitudes. "
"Reference: arXiv:1201.3974."
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:4
#: tensorcircuit.densitymatrix.DMCircuit.perfect_sampling:3
msgid "Sampled bit string and the corresponding theoretical probability."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:1
msgid "Replace the input state with the circuit structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:3
msgid "Input wavefunction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_mps_inputs:1
msgid ""
"Replace the input state in MPS representation while keep the circuit "
"structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:1
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:1
msgid "Compute the output wavefunction from the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:3
msgid ""
"The str indicating the form of the output wavefunction. \"default\": "
"[-1], \"ket\": [-1, 1], \"bra\": [1, -1]"
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:6
msgid "Tensor with the corresponding shape."
msgstr ""

#: of tensorcircuit.circuit.Circuit.vis_tex:1
msgid "Generate latex string based on quantikz latex package"
msgstr ""

#: of tensorcircuit.circuit.Circuit.vis_tex:3
msgid "Latex string that can be directly compiled via, e.g. latexit"
msgstr ""

#: of tensorcircuit.circuit.Circuit.to_qir:1
msgid "Return the quantum intermediate representation of the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.to_qir:32
msgid "The quantum intermediate representation of the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.to_qiskit:1
msgid "Translate ``tc.Circuit`` to a qiskit QuantumCircuit object."
msgstr ""

#: of tensorcircuit.circuit.Circuit.to_qiskit:3
msgid "A qiskit object of this circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:1
msgid ""
"Apply unitary gates in ``kraus`` randomly based on corresponding "
"``prob``. If ``prob`` is ``None``, this is reduced to kraus channel "
"language."
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:4
msgid "List of ``tc.gates.Gate`` or just Tensors"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:6
msgid "prob list with the same size as ``kraus``, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:8
msgid "random seed between 0 to 1, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.unitary_kraus:10
msgid "shape [] int dtype tensor indicates which kraus gate is actually applied"
msgstr ""

#: of tensorcircuit.circuit.expectation:1
msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`."
msgstr ""

#: of tensorcircuit.circuit.expectation:3
msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:24
msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:42
msgid ":math:`ket`."
msgstr ""

#: of tensorcircuit.circuit.expectation:44
msgid ":math:`bra`, defaults to None, which is the same as ``ket``."
msgstr ""

#: of tensorcircuit.circuit.expectation:46
msgid ""
":math:`bra` changes to the adjoint matrix of :math:`bra`, defaults to "
"True."
msgstr ""

#: of tensorcircuit.circuit.expectation:48
msgid "Normalize the :math:`ket` and :math:`bra`, defaults to False."
msgstr ""

#: of tensorcircuit.circuit.expectation:51
msgid "The result of :math:`\\langle bra\\vert ops \\vert ket\\rangle`."
msgstr ""

#: of tensorcircuit.circuit.to_graphviz:1
msgid ""
"Not an ideal visualization for quantum circuit, but reserve here as a "
"general approach to show the tensornetwork [Deprecated, use "
"``Circuit.vis_tex`` or ``Circuit.draw`` instead]"
msgstr ""

#: ../../source/api/cons.rst:2
msgid "tensorcircuit.cons"
msgstr ""

#: of tensorcircuit.cons:1
msgid "Constants and setups"
msgstr ""

#: of tensorcircuit.cons.get_contractor:1 tensorcircuit.cons.set_contractor:1
msgid ""
"To set runtime contractor of the tensornetwork for a better contraction "
"path. For more information on the usage of contractor, please refer to "
"independent tutorial."
msgstr ""

#: of tensorcircuit.cons.get_contractor:4 tensorcircuit.cons.set_contractor:4
msgid ""
"\"auto\", \"greedy\", \"branch\", \"plain\", \"tng\", \"custom\", "
"\"custom_stateful\". defaults to None (\"auto\")"
msgstr ""

#: of tensorcircuit.cons.get_contractor:6 tensorcircuit.cons.set_contractor:6
msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:8 tensorcircuit.cons.set_contractor:8
msgid ""
"It is not very useful, as ``memory_limit`` leads to ``branch`` "
"contraction instead of ``greedy`` which is rather slow, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:11 tensorcircuit.cons.set_contractor:11
msgid "Tensornetwork version is too low to support some of the contractors."
msgstr ""

#: of tensorcircuit.cons.get_contractor:12 tensorcircuit.cons.set_contractor:12
msgid "Unknown method options."
msgstr ""

#: of tensorcircuit.cons.get_contractor:13 tensorcircuit.cons.set_contractor:13
msgid "The new tensornetwork with its contractor set."
msgstr ""

#: of tensorcircuit.cons.get_dtype:1 tensorcircuit.cons.set_dtype:1
msgid "Set the global runtime numerical dtype of tensors."
msgstr ""

#: of tensorcircuit.cons.get_dtype:3 tensorcircuit.cons.set_dtype:3
msgid ""
"\"complex64\" or \"complex128\", defaults to None, which is equivalent to"
" \"complex64\"."
msgstr ""

#: of tensorcircuit.cons.get_dtype:5 tensorcircuit.cons.set_dtype:5
msgid "complex dtype str and the corresponding real dtype str"
msgstr ""

#: of tensorcircuit.cons.plain_contractor:1
msgid "The naive state-vector simulator contraction path."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:3
msgid "The list of ``tn.Node``."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:5
msgid "The list of dangling node edges, defaults to be None."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:7
msgid "The ``tn.Node`` after contraction"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:1
msgid "Context manager to set with-level runtime backend"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:3
#: tensorcircuit.cons.set_function_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\", defaults to None"
msgstr ""

#: of tensorcircuit.cons.runtime_backend tensorcircuit.cons.runtime_contractor
#: tensorcircuit.cons.runtime_dtype
msgid "yield"
msgstr ""

#: of tensorcircuit.cons.runtime_backend:5
msgid "the backend object"
msgstr ""

#: of tensorcircuit.cons.runtime_contractor:1
msgid "Context manager to change with-levek contractor"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:1
msgid "Context manager to set with-level runtime dtype"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:3
msgid "\"complex64\" or \"complex128\", defaults to None (\"complex64\")"
msgstr ""

#: of tensorcircuit.cons.runtime_dtype:5
msgid "complex dtype str and real dtype str"
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:1
msgid "To set the runtime backend of tensorcircuit."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:3
msgid ""
"Note: ``tc.set_backend`` and ``tc.cons.set_tensornetwork_backend`` are "
"the same."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:27
msgid ""
"\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". defaults to None, which "
"gives the same behavior as "
"``tensornetwork.backend_contextmanager.get_default_backend()``."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:30
msgid "Whether the object should be set as global."
msgstr ""

#: of tensorcircuit.cons.set_function_backend:1
msgid "Function decorator to set function-level runtime backend"
msgstr ""

#: of tensorcircuit.cons.set_function_backend:5
msgid "Decorated function"
msgstr ""

#: of tensorcircuit.cons.set_function_contractor:1
msgid "Function decorate to change function-level contractor"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:1
msgid "Function decorator to set function-level numerical dtype"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:3
msgid "\"complex64\" or \"complex128\", defaults to None"
msgstr ""

#: of tensorcircuit.cons.set_function_dtype:5
msgid "The decorated function"
msgstr ""

#: ../../source/api/densitymatrix.rst:2
msgid "tensorcircuit.densitymatrix"
msgstr ""

#: of tensorcircuit.densitymatrix:1
msgid "Quantum circuit class but with density matrix simulator"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:1
msgid "The density matrix simulator based on tensornetwork engine."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:3
msgid "Number of qubits"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:5
msgid "if True, nothing initialized, only for internal use, defaults to False"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:7
msgid "the state input for the circuit, defaults to None"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.__init__:9
msgid "the density matrix input for the circuit, defaults to None"
msgstr ""

#: of
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply amplitudedamping quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.amplitudedampingchannel`"
msgstr ""

#: of
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:6
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:6
msgid "Parameters for the channel."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:1
msgid "Return the output density matrix of the circuit."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:3
msgid ""
"check whether the final return is a legal density matrix, defaults to "
"False"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:5
msgid "whether to reuse previous results, defaults to True"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.densitymatrix:7
msgid "The output densitymatrix in 2D shape tensor form"
msgstr ""

#: of
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply depolarizing quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.depolarizingchannel`"
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.measure_jit:1
msgid "Take measurement to the given quantum lines."
msgstr ""

#: of tensorcircuit.densitymatrix.DMCircuit.perfect_sampling:1
msgid "Sampling bistrings from the circuit output based on quantum amplitudes."
msgstr ""

#: of
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply phasedamping quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.phasedampingchannel`"
msgstr ""

#: of
#: tensorcircuit.densitymatrix.DMCircuit.apply_general_kraus_delayed.<locals>.apply:1
#: tensorcircuit.densitymatrix2.DMCircuit2.apply_general_kraus_delayed.<locals>.apply:1
msgid ""
"Apply reset quantum channel on the circuit. See "
":py:meth:`tensorcircuit.channels.resetchannel`"
msgstr ""

#: ../../source/api/densitymatrix2.rst:2
msgid "tensorcircuit.densitymatrix2"
msgstr ""

#: of tensorcircuit.densitymatrix2:1
msgid "Quantum circuit class but with density matrix simulator: v2"
msgstr ""

#: of tensorcircuit.densitymatrix2.DMCircuit2:1
msgid "Bases: :py:class:`tensorcircuit.densitymatrix.DMCircuit`"
msgstr ""

#: ../../source/api/experimental.rst:2
msgid "tensorcircuit.experimental"
msgstr ""

#: of tensorcircuit.experimental:1
msgid "Experimental features"
msgstr ""

#: ../../source/api/gates.rst:2
msgid "tensorcircuit.gates"
msgstr ""

#: of tensorcircuit.gates:1
msgid ""
"Declarations of single-qubit and two-qubit gates and their corresponding "
"matrix."
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Bases: :py:class:`tensornetwork.network_components.Node`"
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Wrapper of tn.Node, quantum gate"
msgstr ""

#: of tensorcircuit.gates.GateVF:1
msgid "Bases: :py:class:`tensorcircuit.gates.GateF`"
msgstr ""

#: of tensorcircuit.gates.any_gate:1
msgid "Note one should provide the gate with properly reshaped."
msgstr ""

#: of tensorcircuit.gates.any_gate:3
msgid "corresponding gate"
msgstr ""

#: of tensorcircuit.gates.any_gate:5
msgid "The name of the gate."
msgstr ""

#: of tensorcircuit.gates.any_gate:7
msgid "the resulted gate"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:1
msgid "Convert the inputs to Tensor with specified dtype."
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:35
msgid "inputs"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:37
msgid "dtype of the output Tensors"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:39
msgid "List of Tensors"
msgstr ""

#: of tensorcircuit.gates.bmatrix:1
msgid "Returns a :math:`\\LaTeX` bmatrix."
msgstr ""

#: of tensorcircuit.gates.bmatrix:13
msgid "Formatted Display:"
msgstr ""

#: of tensorcircuit.gates.bmatrix:15
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
"\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.bmatrix:18
msgid "2D numpy array"
msgstr ""

#: of tensorcircuit.gates.bmatrix:20
msgid "ValueError(\"bmatrix can at most display two dimensions\")"
msgstr ""

#: of tensorcircuit.gates.bmatrix:21
msgid ":math:`\\LaTeX`-formatted string for bmatrix of the array a"
msgstr ""

#: of tensorcircuit.gates.cr_gate:1
msgid ""
"Controlled rotation gate. When the control qubit is 1, `rgate` is applied"
" to the target qubit."
msgstr ""

#: of tensorcircuit.gates.cr_gate:3 tensorcircuit.gates.cr_gate:5
#: tensorcircuit.gates.cr_gate:7 tensorcircuit.gates.exponential_gate:8
#: tensorcircuit.gates.exponential_gate_unity:9
#: tensorcircuit.gates.iswap_gate:12 tensorcircuit.gates.r_gate:9
#: tensorcircuit.gates.r_gate:11 tensorcircuit.gates.r_gate:13
#: tensorcircuit.gates.rgate_theoretical:8
#: tensorcircuit.gates.rgate_theoretical:10
#: tensorcircuit.gates.rgate_theoretical:12 tensorcircuit.gates.rx_gate:6
#: tensorcircuit.gates.ry_gate:6 tensorcircuit.gates.rz_gate:6
msgid "angle in radians"
msgstr ""

#: of tensorcircuit.gates.cr_gate:10
msgid "CR Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:1
msgid ""
"Faster exponential gate directly implemented based on RHS. Only works "
"when :math:`U^2 = I` is an identity matrix."
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:3
msgid ""
"\\textrm{exp}(U) &= e^{-j \\theta U} \\\\\n"
"        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:6
#: tensorcircuit.gates.exponential_gate_unity:7
msgid "input unitary :math:`U`"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:10
#: tensorcircuit.gates.exponential_gate_unity:11
msgid "suffix of Gate name"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:11
#: tensorcircuit.gates.exponential_gate_unity:13
msgid "Exponential Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:1
msgid "Exponential gate."
msgstr ""

#: of tensorcircuit.gates.exponential_gate:3
msgid ""
"\\textrm{exp}(U) = e^{-j \\theta U}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:1
msgid "iSwap gate."
msgstr ""

#: of tensorcircuit.gates.iswap_gate:3
msgid ""
"\\textrm{iSwap}(\\theta) =\n"
"\\begin{pmatrix}\n"
"    1 & 0 & 0 & 0\\\\\n"
"    0 & \\cos(\\frac{\\pi}{2} \\theta ) & j \\sin(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & j \\sin(\\frac{\\pi}{2} \\theta ) & \\cos(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & 0 & 0 & 1\\\\\n"
"\\end{pmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:14
msgid "iSwap Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:1
msgid "Convert Gate to numpy array."
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:10
msgid "input Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:12
msgid "Corresponding Tensor"
msgstr ""

#: of tensorcircuit.gates.meta_gate:1
msgid ""
"Inner helper function to generate gate functions, such as ``z()`` from "
"``_z_matrix``"
msgstr ""

#: of tensorcircuit.gates.r_gate:1
msgid "General single qubit rotation gate"
msgstr ""

#: of tensorcircuit.gates.r_gate:3
msgid ""
"R(\\theta, \\alpha, \\phi) = j \\cos(\\theta) I\n"
"- j \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
"- j \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
"- j \\sin(\\theta) \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:16
msgid "R Gate"
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:1
msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:3
msgid "A random single-qubit gate"
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:1
msgid "Returns a random two-qubit gate."
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:3
msgid "A random two-qubit gate"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:1
msgid ""
"Rotation gate implemented by matrix exponential. The output is the same "
"as `rgate`."
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:3
msgid ""
"R(\\theta, \\alpha, \\phi) = e^{-j \\theta \\left[\\sin(\\alpha) "
"\\cos(\\phi) X\n"
"                                           + \\sin(\\alpha) \\sin(\\phi) "
"Y\n"
"                                           + \\cos(\\alpha) Z\\right]}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:14
msgid "Rotation Gate"
msgstr ""

#: of tensorcircuit.gates.rx_gate:1
msgid "Rotation gate along :math:`x` axis."
msgstr ""

#: of tensorcircuit.gates.rx_gate:3
msgid ""
"RX(\\theta) = e^{-j\\frac{\\theta}{2}X}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rx_gate:8
msgid "RX Gate"
msgstr ""

#: of tensorcircuit.gates.ry_gate:1
msgid "Rotation gate along :math:`y` axis."
msgstr ""

#: of tensorcircuit.gates.ry_gate:3
msgid ""
"RY(\\theta) = e^{-j\\frac{\\theta}{2}Y}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.ry_gate:8
msgid "RY Gate"
msgstr ""

#: of tensorcircuit.gates.rz_gate:1
msgid "Rotation gate along :math:`z` axis."
msgstr ""

#: of tensorcircuit.gates.rz_gate:3
msgid ""
"RZ(\\theta) = e^{-j\\frac{\\theta}{2}Z}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rz_gate:8
msgid "RZ Gate"
msgstr ""

#: ../../source/api/interfaces.rst:2
msgid "tensorcircuit.interfaces"
msgstr ""

#: of tensorcircuit.interfaces:1
msgid "Interfaces bridging different backends"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:1
msgid "Convert ``fun`` into a scipy optimize interface compatible version"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:35
msgid "The quantum function with scalar out that to be optimized"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:37
msgid "the shape of parameters that ``fun`` accepts, defaults to None"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:39
msgid "whether to jit ``fun``, defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:41
msgid ""
"whether using gradient-based or gradient free scipy optimize interface, "
"defaults to True"
msgstr ""

#: of tensorcircuit.interfaces.scipy_optimize_interface:44
msgid "The scipy interface compatible version of ``fun``"
msgstr ""

#: of tensorcircuit.interfaces.torch_interface:1
msgid "Wrap a quantum function on different ML backend with a pytorch interface."
msgstr ""

#: of tensorcircuit.interfaces.torch_interface:28
msgid "The quantum function with tensor in and tensor out"
msgstr ""

#: of tensorcircuit.interfaces.torch_interface:30
msgid "whether to jit ``fun``, defaults to False"
msgstr ""

#: of tensorcircuit.interfaces.torch_interface:32
msgid ""
"The same quantum function but now with torch tensor in and torch tensor "
"out while AD is also supported"
msgstr ""

#: ../../source/api/keras.rst:2
msgid "tensorcircuit.keras"
msgstr ""

#: of tensorcircuit.keras:1
msgid "Keras layer for tc quantum function"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:1
msgid ""
"`QuantumLayer` wraps the quantum function `f` as a `keras.Layer` so that "
"tensorcircuit is better integrated with tensorflow."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:4
msgid "Callabel function."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:6
msgid "The shape of the weights."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:8
msgid "The initializer of the weights, defaults to \"glorot_uniform\""
msgstr ""

#: of tensorcircuit.keras.load_func:1
msgid ""
"Load function from the files in the ``tf.savedmodel`` format. We can load"
" several functions at the same time, as they can be the same function of "
"different input shapes."
msgstr ""

#: of tensorcircuit.keras.load_func:24
msgid ""
"The fallback function when all functions loaded are failed, defaults to "
"None"
msgstr ""

#: of tensorcircuit.keras.load_func:26
msgid ""
"When there is not legal loaded function of the input shape and no "
"fallback callable."
msgstr ""

#: of tensorcircuit.keras.load_func:27
msgid ""
"A function that tries all loaded function against the input until the "
"first success one."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:1
msgid "The keras loss function that directly taking the model output as the loss."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:3
msgid "Ignoring this parameter."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:5
msgid "Model output."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:7
msgid "Model output, which is y_pred."
msgstr ""

#: of tensorcircuit.keras.save_func:1
msgid "Save tf function in the file (``tf.savedmodel`` format)."
msgstr ""

#: of tensorcircuit.keras.save_func:30
msgid "``tf.function`` ed function with graph building"
msgstr ""

#: of tensorcircuit.keras.save_func:32
msgid "the dir path to save the function"
msgstr ""

#: ../../source/api/mps_base.rst:2
msgid "tensorcircuit.mps_base"
msgstr ""

#: of tensorcircuit.mps_base:1
msgid "FiniteMPS from tensornetwork with bug fixed"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS:1
msgid "Bases: :py:class:`tensornetwork.matrixproductstates.finite_mps.FiniteMPS`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:1
msgid ""
"Apply a two-site gate to an MPS. This routine will in general destroy any"
" canonical form of the state. If a canonical form is needed, the user can"
" restore it using `FiniteMPS.position`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:5
msgid "A two-body gate."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:7
msgid "The first site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:9
msgid "The second site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:11
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:5
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:5
#: tensorcircuit.mpscircuit.split_tensor:7
msgid "The maximum number of singular values to keep."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:13
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:7
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:7
#: tensorcircuit.mpscircuit.split_tensor:9
msgid "The maximum allowed truncation error."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:15
msgid ""
"An optional value to choose the MPS tensor at `center_position` to be "
"isometric after the application of the gate. Defaults to `site1`. If the "
"MPS is canonical (i.e.`BaseMPS.center_position != None`), and if the "
"orthogonality center coincides with either `site1` or `site2`,  the "
"orthogonality center will be shifted to `center_position` (`site1` by "
"default). If the orthogonality center does not coincide with `(site1, "
"site2)` then `MPS.center_position` is set to `None`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:24
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:9
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:9
#: tensorcircuit.mpscircuit.split_tensor:11
msgid "Multiply `max_truncation_err` with the largest singular value."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:26
msgid ""
"\"rank of gate is {} but has to be 4\", \"site1 = {} is not between 0 <= "
"site < N - 1 = {}\", \"site2 = {} is not between 1 <= site < N = "
"{}\",\"Found site2 ={}, site1={}. Only nearest neighbor gates are "
"currently supported\", \"f center_position = {center_position} not  f in "
"{(site1, site2)} \", or \"center_position = {}, but gate is applied at "
"sites {}, {}. Truncation should only be done if the gate is applied at "
"the center position of the MPS.\""
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:32
msgid "A scalar tensor containing the truncated weight of the truncation."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:1
msgid "Measure the expectation value of local operators `ops` site `sites`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:3
msgid "A list Tensors of rank 2; the local operators to be measured."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:5
msgid "Sites where `ops` act."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:7
msgid "measurements :math:`\\langle` `ops[n]`:math:`\\rangle` for n in `sites`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:1
msgid ""
"Compute the correlator :math:`\\langle` `op1[site1], "
"op2[s]`:math:`\\rangle` between `site1` and all sites `s` in `sites2`. If"
" `s == site1`, `op2[s]` will be applied first."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:6
msgid "Tensor of rank 2; the local operator at `site1`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:8
msgid "Tensor of rank 2; the local operator at `sites2`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:10
msgid "The site where `op1`  acts"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:12
msgid "Sites where operator `op2` acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:14
msgid ""
"Correlator :math:`\\langle` `op1[site1], op2[s]`:math:`\\rangle` for `s` "
":math:`\\in` `sites2`."
msgstr ""

#: ../../source/api/mpscircuit.rst:2
msgid "tensorcircuit.mpscircuit"
msgstr ""

#: of tensorcircuit.mpscircuit:1
msgid "Quantum circuit: MPS state simulator"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit:1
msgid "``MPSCircuit`` class. Simple usage demo below."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply any gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:5
msgid "Parameters for the gate"
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **CNOT** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply cr gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **CY** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **CZ** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp1 gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **H** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **I** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply r gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rx gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply ry gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rz gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **S** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **SWAP** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **T** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **WROOT** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **X** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **Y** gate on the circuit."
msgstr ""

#: of
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply **Z** gate on the circuit."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:1
msgid "MPSCircuit object based on state simulator."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``tensors`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:8
msgid "The center position of MPS, default to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:1
msgid "Apply a general qubit gate on MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:4
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:3
msgid "The Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:6
msgid "Qubit indices of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:5
msgid "\"MPS does not support application of gate on > 2 qubits.\""
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:1
msgid ""
"Apply a double qubit gate on adjacent qubits of Matrix Product States "
"(MPS). Truncation rule is specified by `set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:5
msgid "The first qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:7
msgid "The second qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:10
msgid "Center position of MPS, default is None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:1
msgid ""
"Apply a double qubit gate on MPS. Truncation rule is specified by "
"`set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:1
msgid ""
"Apply a single qubit gate on MPS, and the gate must be unitary; no "
"truncation is needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:3
msgid "gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:5
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:5
msgid "Qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:1
msgid "Compute the conjugate of the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:3
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:11
msgid "The constructed MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy:1
msgid "Copy the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:1
msgid "Copy the current MPS without the tensors."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:1
msgid "Compute the expectation of the corresponding double qubit gate."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:5
msgid "qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:1
msgid ""
"Compute the expectation of the corresponding single qubit gate in the "
"form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:3
msgid "Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:7
msgid "The expectation of the corresponding single qubit gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:1
msgid ""
"Compute the expectation of the direct product of the corresponding two "
"gates."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:3
msgid "First gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:5
msgid "Second gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:7
msgid "Qubit index of the first gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:9
msgid "Qubit index of the second gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:11
msgid "The correlation of the corresponding two qubit gates"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:1
msgid "Construct the MPS from a given wavefunction."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:3
msgid "The given wavefunction (any shape is OK)"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:1
msgid "Compute the expectation of corresponding operators in the form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(gates.Z(), [1]), "
"(gates.X(), [2])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:6
msgid "The expectation of corresponding operators"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:1
msgid "Get the normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:3
msgid "Normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:1
msgid "Check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:3
msgid "Whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.measure:1
msgid "integer indicating the measure on which quantum line"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.measure:2
msgid "if true, theoretical probability is also returned"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:1
msgid ""
"Middle measurement in the z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalized the state manually if needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:4
msgid "The index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.normalize:1
msgid "Normalize MPS Circuit according to the center position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:1
msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:4
msgid "The orthogonality center"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:1
msgid "Compute the projection between `other` as bra and `self` as ket."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:3
msgid "ket of the other MPS, which will be converted to bra automatically"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:5
msgid "The projection in form of tensor"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:1
msgid ""
"Set truncation rules when double qubit gates are applied. If nothing is "
"specified, no truncation will take place and the bond dimension will keep"
" growing. For more details, refer to `split_tensor`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:3
msgid "the str indicating the form of the output wavefunction"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:5
msgid "Tensor with shape [1, -1]"
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:1
msgid "Split the tensor by SVD or QR depends on whether a truncation is required."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:3
msgid "The input tensor to split."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:5
msgid "Determine the orthogonal center is on the left tensor or the right tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:13
msgid "Two tensors after splitting"
msgstr ""

#: ../../source/api/quantum.rst:2
msgid "tensorcircuit.quantum"
msgstr ""

#: of tensorcircuit.quantum:1
msgid "Quantum state and operator class backend by tensornetwork"
msgstr ""

#: of tensorcircuit.quantum
msgid "IMPORT"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:1
#: tensorcircuit.quantum.PauliStringSum2COO:1
msgid "Generate tensorflow sparse matrix from Pauli string sum"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:3
msgid ""
"1D Tensor representing for a Pauli string, e.g. [1, 0, 0, 3, 2] is for "
":math:`X_0Z_3Y_4`"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:6
msgid ""
"the weight for the Pauli string defaults to None (all Pauli strings "
"weight 1.0)"
msgstr ""

#: of tensorcircuit.quantum.PauliString2COO:9
msgid "the tensorflow sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:3
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:3
#: tensorcircuit.quantum.PauliStringSum2Dense:3
msgid ""
"2D Tensor, each row is for a Pauli string, e.g. [1, 0, 0, 3, 2] is for "
":math:`X_0Z_3Y_4`"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:6
#: tensorcircuit.quantum.PauliStringSum2COO_numpy:6
#: tensorcircuit.quantum.PauliStringSum2Dense:6
msgid ""
"1D Tensor, each element corresponds the weight for each Pauli string "
"defaults to None (all Pauli strings weight 1.0)"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO:9
msgid "the tensorflow coo sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO_numpy:1
msgid "Generate scipy sparse matrix from Pauli string sum"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2COO_numpy:9
msgid "the scipy coo sparse matrix"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2Dense:1
msgid "Generate tensorflow dense matrix from Pauli string sum"
msgstr ""

#: of tensorcircuit.quantum.PauliStringSum2Dense:9
msgid "the tensorflow dense matrix"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1 tensorcircuit.quantum.QuScalar:1
#: tensorcircuit.quantum.QuVector:1
msgid "Bases: :py:class:`tensorcircuit.quantum.QuOperator`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1
msgid "Represents an adjoint (row) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:1
msgid ""
"Constructs a new `QuAdjointVector` from a tensor network. This "
"encapsulates an existing tensor network, interpreting it as an adjoint "
"vector (row vector)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:5
#: tensorcircuit.quantum.QuOperator.__init__:9
msgid "The edges of the network to be used as the input edges."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:7
#: tensorcircuit.quantum.QuOperator.__init__:11
#: tensorcircuit.quantum.QuVector.__init__:6
msgid ""
"Nodes used to refer to parts of the tensor network that are not connected"
" to any input or output edges (for example: a scalar factor)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:10
#: tensorcircuit.quantum.QuScalar.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:9
msgid "Optional collection of edges to ignore when performing consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:1
msgid ""
"Construct a `QuAdjointVector` directly from a single tensor. This first "
"wraps the tensor in a `Node`, then constructs the `QuAdjointVector` from "
"that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:27
msgid "The tensor for constructing an QuAdjointVector."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:29
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (input edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:33
msgid "The new constructed QuAdjointVector give from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:1
#: tensorcircuit.quantum.QuVector.projector:1
msgid ""
"The projector of the operator. The operator, as a linear operator, on the"
" adjoint of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:4
msgid ""
"Set :math:`A` is the operator in matrix form, then the projector of "
"operator is defined as: :math:`A^\\dagger A`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.projector:6
#: tensorcircuit.quantum.QuVector.projector:6
msgid "The projector of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:1
#: tensorcircuit.quantum.QuVector.reduced_density:1
msgid "The reduced density of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:3
#: tensorcircuit.quantum.QuVector.reduced_density:3
msgid ""
"Set :math:`A` is the matrix of the operator, then the reduced density is "
"defined as:"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:5
msgid "\\mathrm{Tr}_{subsystems}(A^\\dagger A)"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:9
#: tensorcircuit.quantum.QuVector.reduced_density:9
msgid ""
"Firstly, take the projector of the operator, then trace out the "
"subsystems to trace out are supplied as indices, so that dangling edges "
"are connected to each other as: `out_edges[i] ^ in_edges[i] for i in "
"subsystems_to_trace_out` This does not modify the original network. The "
"original ordering of the remaining subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:16
#: tensorcircuit.quantum.QuOperator.partial_trace:8
#: tensorcircuit.quantum.QuVector.reduced_density:16
msgid "Indices of subsystems to trace out."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.reduced_density:18
#: tensorcircuit.quantum.QuVector.reduced_density:18
msgid ""
"The QuOperator of the reduced density of the operator with given "
"subsystems."
msgstr ""

#: of tensorcircuit.quantum.QuOperator:1
msgid ""
"Represents a linear operator via a tensor network. To interpret a tensor "
"network as a linear operator, some of the dangling edges must be "
"designated as `out_edges` (output edges) and the rest as `in_edges` "
"(input edges). Considered as a matrix, the `out_edges` represent the row "
"index and the `in_edges` represent the column index. The (right) action "
"of the operator on another then consists of connecting the `in_edges` of "
"the first operator to the `out_edges` of the second. Can be used to do "
"simple linear algebra with tensor networks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:1
msgid ""
"Creates a new `QuOperator` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a linear operator. The "
"network is checked for consistency: All dangling edges must either be in "
"`out_edges`, `in_edges`, or `ignore_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:4
msgid "The edges of the network to be used as the output edges."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:15
msgid ""
"Optional collection of dangling edges to ignore when performing "
"consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:18
msgid ""
"At least one reference node is required to specify a scalar. None "
"provided!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:1
msgid ""
"The adjoint of the operator. This creates a new `QuOperator` with "
"complex-conjugate copies of all tensors in the network and with the input"
" and output edges switched."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:5
msgid "The adjoint of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.check_network:1
msgid ""
"Check that the network has the expected dimensionality. This checks that "
"all input and output edges are dangling and that there are no other "
"dangling edges (except any specified in `ignore_edges`). If not, an "
"exception is raised."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:1
msgid ""
"Contract the tensor network in place. This modifies the tensor network "
"representation of the operator (or vector, or scalar), reducing it to a "
"single tensor, without changing the value."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:5
msgid "Manually specify the axis ordering of the final tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:7
msgid "The present object."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.copy:1
msgid "The deep copy of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.copy:3
msgid "The new copy of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:1
msgid ""
"Contracts the tensor network in place and returns the final tensor. Note "
"that this modifies the tensor network representing the operator. The "
"default ordering for the axes of the final tensor is: `*out_edges, "
"*in_edges`. If there are any \"ignored\" edges, their axes come first: "
"`*ignored_edges, *out_edges, *in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:8
#: tensorcircuit.quantum.QuOperator.eval_matrix:6
msgid ""
"Manually specify the axis ordering of the final tensor. The default "
"ordering is determined by `out_edges` and `in_edges` (see above)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:11
#: tensorcircuit.quantum.QuOperator.eval_matrix:9
msgid "Node count '{}' > 1 after contraction!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:12
msgid "The final tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval_matrix:1
msgid ""
"Contracts the tensor network in place and returns the final tensor in two"
" dimentional matrix. The default ordering for the axes of the final "
"tensor is: (:math:`\\prod` dimension of out_edges, :math:`\\prod` "
"dimension of in_edges)"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval_matrix:10
msgid "The two-dimentional tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:1
msgid ""
"Construct a `QuOperator` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuOperator` from that "
"`Node`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:28
msgid "The tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:30
msgid "The axis indices of `tensor` to use as `out_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:32
msgid "The axis indices of `tensor` to use as `in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:34
msgid "The new operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_adjoint_vector:1
msgid ""
"Returns a bool indicating if QuOperator is an adjoint vector. Examples "
"can be found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_scalar:1
msgid ""
"Returns a bool indicating if QuOperator is a scalar. Examples can be "
"found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.is_vector:1
msgid ""
"Returns a bool indicating if QuOperator is a vector. Examples can be "
"found in the `QuOperator.from_tensor`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.nodes:1
msgid "All tensor-network nodes involved in the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.norm:1
msgid ""
"The norm of the operator. This is the 2-norm (also known as the Frobenius"
" or Hilbert-Schmidt norm)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:1
msgid ""
"The partial trace of the operator. Subsystems to trace out are supplied "
"as indices, so that dangling edges are connected to each other as: "
"`out_edges[i] ^ in_edges[i] for i in subsystems_to_trace_out` This does "
"not modify the original network. The original ordering of the remaining "
"subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:10
msgid "A new QuOperator or QuScalar representing the result."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:1
msgid ""
"Tensor product with another operator. Given two operators `A` and `B`, "
"produces a new operator `AB` representing :math:`A  B`. The `out_edges` "
"(`in_edges`) of `AB` is simply the concatenation of the `out_edges` "
"(`in_edges`) of `A.copy()` with that of `B.copy()`: `new_out_edges = "
"[*out_edges_A_copy, *out_edges_B_copy]` `new_in_edges = "
"[*in_edges_A_copy, *in_edges_B_copy]`"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:20
msgid "The other operator (`B`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:22
msgid "The result (`AB`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.trace:1
msgid "The trace of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuScalar:1
msgid "Represents a scalar via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:1
msgid ""
"Constructs a new `QuScalar` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a scalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:4
msgid ""
"Nodes used to refer to the tensor network (need not be exhaustive - one "
"node from each disconnected subnetwork is sufficient)."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:1
msgid ""
"Construct a `QuScalar` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuScalar` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:22
msgid "The tensor for constructing a new QuScalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:24
msgid "The new constructed QuScalar from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector:1
msgid "Represents a (column) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuVector.__init__:1
msgid ""
"Constructs a new `QuVector` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a (column) vector."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:1
msgid ""
"Construct a `QuVector` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuVector` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:28
msgid "The tensor for constructing a \"QuVector\"."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:30
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (output edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:34
msgid "The new constructed QuVector from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector.projector:4
msgid ""
"Set :math:`A` is the operator in matrix form, then the projector of "
"operator is defined as: :math:`A A^\\dagger`"
msgstr ""

#: of tensorcircuit.quantum.QuVector.reduced_density:5
msgid "\\mathrm{Tr}_{subsystems}(A A^\\dagger)"
msgstr ""

#: of tensorcircuit.quantum.check_spaces:1
msgid ""
"Check the vector spaces represented by two lists of edges are compatible."
" The number of edges must be the same and the dimensions of each pair of "
"edges must match. Otherwise, an exception is raised."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:5 tensorcircuit.quantum.check_spaces:7
msgid "List of edges representing a many-body Hilbert space."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:10
msgid ""
"Hilbert-space mismatch: \"Cannot connect {} subsystems with {} "
"subsystems\", or \"Input dimension {} != output dimension {}.\""
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:1
msgid ""
"Compute :math:`\\prod_{i\\in \\text{index}} s_i`, where the probability "
"for each bitstring is given as a vector ``results``."
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:12
msgid "list of int, indicating the position in the bitstring"
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:14
msgid "probability vector of shape 2^n"
msgstr ""

#: of tensorcircuit.quantum.correlation_from_counts:16
msgid "Correlation expectation from measurement shots."
msgstr ""

#: of tensorcircuit.quantum.double_state:1
msgid "Compute the double state of the given Hamiltonian operator ``h``."
msgstr ""

#: of tensorcircuit.quantum.double_state:3 tensorcircuit.quantum.gibbs_state:3
#: tensorcircuit.quantum.truncated_free_energy:5
msgid "Hamiltonian operator in form of Tensor."
msgstr ""

#: of tensorcircuit.quantum.double_state:5 tensorcircuit.quantum.free_energy:17
#: tensorcircuit.quantum.gibbs_state:5
#: tensorcircuit.quantum.renyi_free_energy:16
#: tensorcircuit.quantum.truncated_free_energy:7
msgid "Constant for the optimization, default is 1."
msgstr ""

#: of tensorcircuit.quantum.double_state:7
msgid "The double state of ``h`` with the given ``beta``."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:1
msgid ""
"Eliminates any connected CopyNodes that are identity matrices. This will "
"modify the network represented by `nodes`. Only identities that are "
"connected to other nodes are eliminated."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:5
msgid "Collection of nodes to search."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:7
msgid ""
"The Dictionary mapping remaining Nodes to any replacements, Dictionary "
"specifying all dangling-edge replacements."
msgstr ""

#: of tensorcircuit.quantum.entropy:1
msgid "Compute the entropy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.entropy:30 tensorcircuit.quantum.free_energy:13
#: tensorcircuit.quantum.renyi_entropy:3
#: tensorcircuit.quantum.renyi_free_energy:12
msgid "The density matrix in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.entropy:32 tensorcircuit.quantum.free_energy:19
msgid "Epsilon, default is 1e-12."
msgstr ""

#: of tensorcircuit.quantum.entropy:34
msgid "Entropy on the given density matrix."
msgstr ""

#: of tensorcircuit.quantum.fidelity:1
msgid "Return fidelity scalar between two states rho and rho0."
msgstr ""

#: of tensorcircuit.quantum.fidelity:3
msgid "\\operatorname{Tr}(\\sqrt{\\sqrt{rho} rho_0 \\sqrt{rho}})"
msgstr ""

#: of tensorcircuit.quantum.fidelity:7 tensorcircuit.quantum.fidelity:9
#: tensorcircuit.quantum.mutual_information:3 tensorcircuit.quantum.taylorlnm:3
#: tensorcircuit.quantum.trace_distance:3
#: tensorcircuit.quantum.trace_distance:5
#: tensorcircuit.quantum.truncated_free_energy:3
msgid "The density matrix in form of Tensor."
msgstr ""

#: of tensorcircuit.quantum.fidelity:11
msgid "The sqrtm of a Hermitian matrix ``a``."
msgstr ""

#: of tensorcircuit.quantum.free_energy:1
msgid "Compute the free energy of the given density matrix."
msgstr ""

#: of tensorcircuit.quantum.free_energy:15
#: tensorcircuit.quantum.renyi_free_energy:14
msgid "Hamiltonian operator in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.free_energy:22
msgid "The free energy of the given density matrix with the Hamiltonian operator."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:1
msgid ""
"Generate a local Hamiltonian operator based on the given sequence of "
"Tensor. Note: further jit is recommended. For large Hilbert space, sparse"
" Hamiltonian is recommended"
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:5
msgid "A sequence of Tensor."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:7
msgid "Return Hamiltonian operator in form of matrix, defaults to True."
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:9
msgid "The Hamiltonian operator in form of QuOperator or matrix."
msgstr ""

#: of tensorcircuit.quantum.gibbs_state:1
msgid "Compute the Gibbs state of the given Hamiltonian operator ``h``."
msgstr ""

#: of tensorcircuit.quantum.gibbs_state:7
msgid "The Gibbs state of ``h`` with the given ``beta``."
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:1
msgid "Generate Heisenberg Hamiltonian with possible external fields."
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:12
msgid "input circuit graph"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:14
msgid "zz coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:16
msgid "xx coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:18
msgid "yy coupling, default is 1.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:20
msgid "External field on z direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:22
msgid "External field on y direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:24
msgid "External field on x direction, default is 0.0"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:26
msgid "Whether to return sparse Hamiltonian operator, default is True."
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:28
msgid "whether return the matrix in numpy or tensorflow form"
msgstr ""

#: of tensorcircuit.quantum.heisenberg_hamiltonian:31
msgid "Hamiltonian measurements"
msgstr ""

#: of tensorcircuit.quantum.identity:1
msgid ""
"Construct a 'QuOperator' representing the identity on a given space. "
"Internally, this is done by constructing 'CopyNode's for each edge, with "
"dimension according to 'space'."
msgstr ""

#: of tensorcircuit.quantum.identity:26
msgid ""
"A sequence of integers for the dimensions of the tensor product factors "
"of the space (the edges in the tensor network)."
msgstr ""

#: of tensorcircuit.quantum.identity:29
msgid ""
"The data type by np.* (for conversion to dense). defaults None to tc "
"dtype."
msgstr ""

#: of tensorcircuit.quantum.identity:31
msgid "The desired identity operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:1
msgid ""
"Simulate the measuring of each qubit of ``p`` in the computational basis,"
" thus producing output like that of ``qiskit``."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:14
msgid ""
"The quantum state, assumed to be normalized, as either a ket or density "
"operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:16
msgid "The number of counts to perform."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:18
msgid ""
"Defaults True. The bool indicating whether the return form is in the form"
" of two array or one of the same length as the ``state`` (if "
"``sparse=False``)."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:21
msgid "The counts for each bit string measured."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:1
msgid "Mutual information between AB subsystem described by ``cut``."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:5
msgid "The AB subsystem."
msgstr ""

#: of tensorcircuit.quantum.mutual_information:7
msgid "The mutual information between AB subsystem described by ``cut``."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:1
msgid ""
"Constructs an appropriately specialized QuOperator. If there are no "
"edges, creates a QuScalar. If the are only output (input) edges, creates "
"a QuVector (QuAdjointVector). Otherwise creates a QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:48
msgid "A list of output edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:50
msgid "A list of input edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:52
msgid ""
"Reference nodes for the tensor network (needed if there is a. scalar "
"component)."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:55
msgid "Edges to ignore when checking the dimensionality of the tensor network."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:58
msgid "The new created QuOperator object."
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:1
msgid "Convert MPO in Quimb package to QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:3
msgid "MPO in the form of Quimb package"
msgstr ""

#: of tensorcircuit.quantum.quimb2qop:5 tensorcircuit.quantum.tn2qop:5
msgid "MPO in the form of QuOperator"
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:1
msgid "Compute the reduced density matrix from the quantum state ``state``."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:3
msgid "The quantum state in form of Tensor or QuOperator."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:5
msgid ""
"the index list that is traced out, if cut is a int, it indicates [0, cut]"
" as the traced out region"
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:8
msgid "probability decoration, default is None."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:10
msgid "The reduced density matrix."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:1
msgid "Compute the Rnyi entropy of order :math:`k` by given density matrix."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:5
#: tensorcircuit.quantum.renyi_free_energy:18
msgid "The order of Rnyi entropy, default is 2."
msgstr ""

#: of tensorcircuit.quantum.renyi_entropy:7
#: tensorcircuit.quantum.renyi_free_energy:20
msgid "The :math:`k` th order of Rnyi entropy."
msgstr ""

#: of tensorcircuit.quantum.renyi_free_energy:1
msgid ""
"Compute the Rnyi free energy of the corresponding density matrix and "
"Hamiltonian."
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:1
msgid ""
"Generate all n-bitstrings as an array, each row is a bitstring basis. "
"Return m-th col."
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:9
msgid "length of a bitstring"
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:11
msgid "m<n,"
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:13
msgid "the binary elements to generate, default is (1, -1)."
msgstr ""

#: of tensorcircuit.quantum.spin_by_basis:15
msgid ""
"The value for the m-th position in bitstring when going through all "
"bitstring basis."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:1
msgid "Taylor expansion of :math:`ln(x+1)`."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:5
msgid "The :math:`k` th order, default is 2."
msgstr ""

#: of tensorcircuit.quantum.taylorlnm:7
msgid "The :math:`k` th order of Taylor expansion of :math:`ln(x+1)`."
msgstr ""

#: of tensorcircuit.quantum.tn2qop:1
msgid "Convert MPO in TensorNetwork package to QuOperator."
msgstr ""

#: of tensorcircuit.quantum.tn2qop:3
msgid "MPO in the form of TensorNetwork package"
msgstr ""

#: of tensorcircuit.quantum.trace_distance:1
msgid ""
"Compute the trace distance between two density matrix ``rho`` and "
"``rho2``."
msgstr ""

#: of tensorcircuit.quantum.trace_distance:7
msgid "Epsilon, defaults to 1e-12"
msgstr ""

#: of tensorcircuit.quantum.trace_distance:9
msgid "The trace distance between two density matrix ``rho`` and ``rho2``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:1
msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:3
msgid "\\operatorname{Tr}(\\prod_i O_i)"
msgstr ""

#: of tensorcircuit.quantum.trace_product:22
msgid "The trace of several inputs."
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:1
msgid "Compute the truncated free energy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:9
msgid "The :math:`k` th order, defaults to 2"
msgstr ""

#: of tensorcircuit.quantum.truncated_free_energy:11
msgid "The :math:`k` th order of the truncated free energy."
msgstr ""

#: ../../source/api/simplify.rst:2
msgid "tensorcircuit.simplify"
msgstr ""

#: of tensorcircuit.simplify:1
msgid "Tensornetwork Simplification"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:1
msgid ""
"Get the new shape of two nodes, also supporting to return original shapes"
" of two nodes."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:13
msgid "node one"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:15
msgid "node two"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:17
msgid "Whether to include original shape of two nodes, default is True."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:19
msgid "The new shape of the two nodes."
msgstr ""

#: of tensorcircuit.simplify.pseudo_contract_between:1
msgid ""
"Contract between Node ``a`` and ``b``, with correct shape only and no "
"calculation"
msgstr ""

#: ../../source/api/templates.rst:2
msgid "tensorcircuit.templates"
msgstr ""

#: ../../source/api/templates/blocks.rst:2
msgid "tensorcircuit.templates.blocks"
msgstr ""

#: of tensorcircuit.templates.blocks:1 tensorcircuit.templates.measurements:1
msgid "Shortcuts for measurement patterns on circuit"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:1
msgid ""
"For each pair in links, the input product state |00> is transformed as "
"(01>-|10>)"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:3
msgid "Circuit in"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:5
msgid ""
"pairs indices for Bell pairs, defaults to None, corresponds to neighbor "
"links"
msgstr ""

#: of tensorcircuit.templates.blocks.Bell_pair_block:7
msgid "Circuit out"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:1
msgid ""
"The circuit ansatz is firstly one layer of Hadamard gates and then we "
"have ``nlayers`` blocks of :math:`e^{i\\theta Z_iZ_{i+1}}` two-qubit gate"
" in ladder layout, following rx gate."
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:5
msgid "The circuit"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:7
msgid "paramter tensor with 2*nlayer*n elements"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:9
msgid "number of ZZ+RX blocks, defaults to 2"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:11
msgid "whether use SVD split to reduce ZZ gate bond dimension, defaults to False"
msgstr ""

#: of tensorcircuit.templates.blocks.example_block:14
msgid "The circuit with example ansatz attached"
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:1
msgid ""
"Function decorator wraps the function with the first input and output in "
"the format of circuit, the wrapped function has the first input and the "
"output as the state tensor."
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:4
msgid "Function with the fist input and the output as ``Circuit`` object."
msgstr ""

#: of tensorcircuit.templates.blocks.state_centric:6
msgid ""
"Wrapped function with the first input and the output as the state tensor "
"correspondingly."
msgstr ""

#: ../../source/api/templates/chems.rst:2
msgid "tensorcircuit.templates.chems"
msgstr ""

#: of tensorcircuit.templates.chems:1
msgid "Useful utilities for quantum chemistry related task"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:1
msgid ""
"Get Pauli string array and weights array for a qubit Hamiltonian as a sum"
" of Pauli strings defined in openfermion QubitOperator."
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:4
msgid "``openfermion.ops.operators.qubit_operator.QubitOperator``"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:6
msgid "The number of qubits"
msgstr ""

#: of tensorcircuit.templates.chems.get_ps:8
msgid "Pauli String array and weights array"
msgstr ""

#: ../../source/api/templates/dataset.rst:2
msgid "tensorcircuit.templates.dataset"
msgstr ""

#: of tensorcircuit.templates.dataset:1
msgid "Quantum machine learning related data preprocessing and embedding"
msgstr ""

#: ../../source/api/templates/graphs.rst:2
msgid "tensorcircuit.templates.graphs"
msgstr ""

#: of tensorcircuit.templates.graphs:1
msgid "Some common graphs and lattices"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Two-dimensional grid lattice"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.__init__:1
msgid "number of rows"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.__init__:3
msgid "number of cols"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:1
msgid "return all col edge with 1d index encoding"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:3
#: tensorcircuit.templates.graphs.Grid2DCoord.all_rows:3
msgid ""
"whether to include pbc edges (periodic boundary condition), defaults to "
"False"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_cols:6
msgid "list of col edge"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_rows:1
msgid "return all row edge with 1d index encoding"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.all_rows:6
msgid "list of row edge"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:1
msgid "Get the 2D grid lattice in ``nx.Graph`` format"
msgstr ""

#: of tensorcircuit.templates.graphs.Grid2DCoord.lattice_graph:3
msgid ""
"whether to include pbc edges (periodic boundary condition), defaults to "
"True"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:1
msgid "1D chain with ``n`` sites"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:5
#: tensorcircuit.templates.measurements.heisenberg_measurements:34
msgid "[description], defaults to True"
msgstr ""

#: ../../source/api/templates/measurements.rst:2
msgid "tensorcircuit.templates.measurements"
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:1
msgid ""
"This measurements pattern is specifically suitable for vmap. Parameterize"
" the Pauli string to be measured."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.heisenberg_measurements
#: tensorcircuit.templates.measurements.spin_glass_measurements
msgid "example"
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:26
msgid "The circuit to be measured"
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:28
msgid ""
"parameter tensors determines what Pauli string to be measured, shape is "
"[nwires, 4] if ``onehot`` is False and [nwires] if ``onehot`` is True."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:31
msgid ""
"defaults to False. If set to be True, structures will first go through "
"onehot procedure."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:34
msgid "The expectation value of given Pauli string by the tensor ``structures``."
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:1
msgid ""
"Evaluate Heisenberg energy expectation, whose Hamiltonian is defined on "
"the lattice graph ``g`` as follows: (e are edges in graph ``g`` where e1 "
"and e2 are two nodes for edge e and v are nodes in graph ``g``)"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:4
msgid ""
"H = \\sum_{e\\in g} w_e (h_{xx} X_{e1}X_{e2} + h_{yy} Y_{e1}Y_{e2} + "
"h_{zz} Z_{e1}Z_{e2})\n"
" + \\sum_{v\\in g} (h_x X_v + h_y Y_v + h_z Z_v)"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:18
msgid "Circuit to be measured"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:20
msgid "Lattice graph defining Heisenberg Hamiltonian"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:22
#: tensorcircuit.templates.measurements.heisenberg_measurements:24
#: tensorcircuit.templates.measurements.heisenberg_measurements:26
msgid "[description], defaults to 1.0"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:28
#: tensorcircuit.templates.measurements.heisenberg_measurements:30
#: tensorcircuit.templates.measurements.heisenberg_measurements:32
msgid "[description], defaults to 0.0"
msgstr ""

#: of tensorcircuit.templates.measurements.heisenberg_measurements:36
msgid "Value of Heisenberg energy"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:1
msgid ""
"Evaluate expectation of operator ``mpo`` defined in ``QuOperator`` MPO "
"format with the output quantum state from circuit ``c``."
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:4
msgid "The circuit for the output state"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:6
msgid "MPO operator"
msgstr ""

#: of tensorcircuit.templates.measurements.mpo_expectation:8
#: tensorcircuit.templates.measurements.operator_expectation:7
#: tensorcircuit.templates.measurements.sparse_expectation:7
msgid "a real and scalar tensor of shape [] as the expectation value"
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:1
msgid ""
"Evaluate Hamiltonian expectation where ``hamiltonian`` can be dense "
"matrix, sparse matrix or MPO."
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:3
#: tensorcircuit.templates.measurements.sparse_expectation:3
msgid "The circuit whose output state is used to evaluate the expectation"
msgstr ""

#: of tensorcircuit.templates.measurements.operator_expectation:5
#: tensorcircuit.templates.measurements.sparse_expectation:5
msgid "Hamiltonian matrix in COO_sparse_matrix form"
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:1
msgid ""
"Evaluate Hamiltonian expectation where ``hamiltonian`` is kept in sparse "
"matrix form to save space"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:1
msgid ""
"Compute spin glass energy defined on graph ``g`` expectation for output "
"state of the circuit ``c``. The Hamiltonian to be evaluated is defined as"
" (first term is determined by node weights while the second term is "
"determined by edge weights of the graph):"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:5
msgid "H = \\sum_{v\\in g} w_v Z_v + \\sum_{e\\in g} w_e Z_{e1} Z_{e2}"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:28
msgid "The quantum circuit"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:30
msgid "The graph for spin glass Hamiltonian definition"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:32
msgid ""
"Whether measure the circuit with reusing the wavefunction, defaults to "
"True"
msgstr ""

#: of tensorcircuit.templates.measurements.spin_glass_measurements:34
msgid "The spin glass energy expectation value"
msgstr ""

#: ../../source/api/translation.rst:2
msgid "tensorcircuit.translation"
msgstr ""

#: of tensorcircuit.translation:1
msgid "Circuit object translation in different packages"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:1
msgid ""
"Generate a permutation matrix P. Due to the different convention or "
"qubits' order in qiskit and tensorcircuit, the unitary represented by the"
" same circuit is different. They are related by this permutation matrix "
"P: P @ U_qiskit @ P = U_tc"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:7
#: tensorcircuit.translation.qir2qiskit:16
#: tensorcircuit.translation.qiskit2tc:14 tensorcircuit.vis.qir2tex:12
msgid "# of qubits"
msgstr ""

#: of tensorcircuit.translation.perm_matrix:9
msgid "The permutation matrix P"
msgstr ""

#: of tensorcircuit.translation.qir2qiskit:1
msgid ""
"Generate a qiskit quantum circuit using the quantum intermediate "
"representation (qir) in tensorcircuit."
msgstr ""

#: of tensorcircuit.translation.qir2qiskit:18
msgid "qiskit QuantumCircuit object"
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:1
msgid "Generate a tensorcircuit circuit using the quantum circuit data in qiskit."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:12
msgid "Quantum circuit data from qiskit."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:16
msgid "Input state of the circuit. Default is None."
msgstr ""

#: of tensorcircuit.translation.qiskit2tc:18
msgid "A quantum circuit in tensorcircuit"
msgstr ""

#: ../../source/api/utils.rst:2
msgid "tensorcircuit.utils"
msgstr ""

#: of tensorcircuit.utils:1
msgid "Helper functions"
msgstr ""

#: of tensorcircuit.utils.append:1
msgid "Functional programming paradigm to build function pipeline"
msgstr ""

#: of tensorcircuit.utils.append:9
msgid "The function which are attached with other functions"
msgstr ""

#: of tensorcircuit.utils.append:11
msgid "Function to be attached"
msgstr ""

#: of tensorcircuit.utils.append:13
msgid "The final results after function pipeline"
msgstr ""

#: of tensorcircuit.utils.return_partial:1
msgid ""
"Return a callable function for output ith parts of the original output "
"along the first axis. Original output supports List and Tensor."
msgstr ""

#: of tensorcircuit.utils.return_partial:20
msgid "The function to be applied this method"
msgstr ""

#: of tensorcircuit.utils.return_partial:22
msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
msgstr ""

#: of tensorcircuit.utils.return_partial:24
msgid "The modified callable function"
msgstr ""

#: ../../source/api/vis.rst:2
msgid "tensorcircuit.vis"
msgstr ""

#: of tensorcircuit.vis:1
msgid "Visualization on circuits"
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:1
msgid ""
"Translating from the gate name to gate information including the number "
"of control qubits and the reduced gate name."
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:10
msgid "String of gate name"
msgstr ""

#: of tensorcircuit.vis.gate_name_trans:12
msgid "# of control qubits, reduced gate name"
msgstr ""

#: of tensorcircuit.vis.qir2tex:1
msgid ""
"Generate Tex code from 'qir' string to illustrate the circuit structure. "
"This visualization is based on quantikz package."
msgstr ""

#: of tensorcircuit.vis.qir2tex:10
msgid "The quantum intermediate representation of a circuit in tensorcircuit."
msgstr ""

#: of tensorcircuit.vis.qir2tex:14
msgid "Initial state, default is an all zero state '000...000'."
msgstr ""

#: of tensorcircuit.vis.qir2tex:16
msgid "Measurement Basis, default is None which means no"
msgstr ""

#: of tensorcircuit.vis.qir2tex:17
msgid ""
"measurement in the end of the circuit. :type measure: Optional[List[str]]"
" :param rcompress: If true, a right compression of the circuit will be "
"conducted. A right compression means we will try to shift gates from "
"right to left if possible. Default is false. :type rcompress: bool :param"
" lcompress: If true, a left compression of the circuit will be conducted."
" A left compression means we will try to shift gates from left to right "
"if possible. Default is false. :type lcompress: bool :param standalone: "
"If true, the tex code will be designed to generate a standalone document."
" Default is false which means the generated tex code is just a quantikz "
"code block. :type standalone: bool :param return_string_table: If true, a"
" string table of tex code will also be returned. Default is false. :type "
"return_string_table: bool :return: Tex code of circuit visualization "
"based on quantikz package. If return_string_table is true, a string table"
" of tex code will also be returned. :rtype: Union[str, Tuple[str, "
"List[List[str]]]]"
msgstr ""

#: of tensorcircuit.vis.render_pdf:1
msgid ""
"Generate the PDF file with given latex string and filename. Latex command"
" and file path can be specified. When notebook is True, convert the "
"output PDF file to image and return a Image object."
msgstr ""

#: of tensorcircuit.vis.render_pdf:15
msgid "String of latex content"
msgstr ""

#: of tensorcircuit.vis.render_pdf:17
msgid "File name, defaults to random UUID `str(uuid4())`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:19
msgid "Executable Latex command, defaults to `pdflatex`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:21
msgid "File path, defaults to current working place `os.getcwd()`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:23
msgid "[description], defaults to False"
msgstr ""

#: of tensorcircuit.vis.render_pdf:25
msgid "if notebook is True, return `Image` object; otherwise return `None`"
msgstr ""

#~ msgid ""
#~ "This is a method that implementers "
#~ "of subclasses of `Layer` or `Model` "
#~ "can override if they need a "
#~ "state-creation step in-between layer "
#~ "instantiation and layer call."
#~ msgstr ""

#~ msgid "This is typically used to create the weights of `Layer` subclasses."
#~ msgstr ""

#~ msgid ""
#~ "Note here that `call()` method in "
#~ "`tf.keras` is little bit different from"
#~ " `keras` API. In `keras` API, you "
#~ "can pass support masking for layers "
#~ "as additional arguments. Whereas `tf.keras`"
#~ " has `compute_mask()` method to support "
#~ "masking."
#~ msgstr ""

#~ msgid "Modules for DQAS framework"
#~ msgstr ""

#~ msgid "DQAS framework entrypoint"
#~ msgstr ""

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid ""
#~ "function with input of data instance,"
#~ " circuit parameters theta and structural"
#~ " paramter k, return tuple of "
#~ "objective value and gradient with "
#~ "respect to theta"
#~ msgstr ""

#~ msgid "data generator as dataset"
#~ msgstr ""

#~ msgid "list of operations as primitive operator pool"
#~ msgstr ""

#~ msgid "the default layer number of the circuit ansatz"
#~ msgstr ""

#~ msgid ""
#~ "shape of circuit parameter pool, in "
#~ "general p_stp*l, where l is the "
#~ "max number of circuit parameters for "
#~ "op in the operator pool"
#~ msgstr ""

#~ msgid "the same as p in the most times"
#~ msgstr ""

#~ msgid "batch size of one epoch"
#~ msgstr ""

#~ msgid "prethermal update times"
#~ msgstr ""

#~ msgid "training epochs"
#~ msgstr ""

#~ msgid "parallel thread number, 0 to disable multiprocessing model by default"
#~ msgstr ""

#~ msgid "set verbose log to print"
#~ msgstr ""

#~ msgid "function to output verbose information"
#~ msgstr ""

#~ msgid "function return intermiediate result for final history list"
#~ msgstr ""

#~ msgid "cutoff probability to avoid peak distribution"
#~ msgstr ""

#~ msgid ""
#~ "function accepting list of objective "
#~ "values and return the baseline value "
#~ "used in the next round"
#~ msgstr ""

#~ msgid "return noise with the same shape as circuit parameter pool"
#~ msgstr ""

#~ msgid "initial values for circuit parameter pool"
#~ msgstr ""

#~ msgid "initial values for probabilistic model parameters"
#~ msgstr ""

#~ msgid "optimizer for circuit parameters theta"
#~ msgstr ""

#~ msgid "optimizer for model parameters alpha"
#~ msgstr ""

#~ msgid "optimizer for circuit parameters in prethermal stage"
#~ msgstr ""

#~ msgid "fixed structural parameters for prethermal training"
#~ msgstr ""

#~ msgid "regularization function for model parameters alpha"
#~ msgstr ""

#~ msgid "regularization function for circuit parameters theta"
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr ""

#~ msgid ""
#~ "The probabilistic model based DQAS, can"
#~ " use extensively for DQAS case for"
#~ " ``NMF`` probabilistic model."
#~ msgstr ""

#~ msgid "vag func, return loss and nabla lnp"
#~ msgstr ""

#~ msgid "keras model"
#~ msgstr ""

#~ msgid "sample func of logic with keras model input"
#~ msgstr ""

#~ msgid "input data pipeline generator"
#~ msgstr ""

#~ msgid "operation pool"
#~ msgstr ""

#~ msgid "depth for DQAS"
#~ msgstr ""

#~ msgid "parallel kernels"
#~ msgstr ""

#~ msgid "final loss function in terms of average of sub loss for each circuit"
#~ msgstr ""

#~ msgid "derivative function for ``loss_func``"
#~ msgstr ""

#~ msgid ""
#~ "Call in customized functions and grab"
#~ " variables within DQAS framework function"
#~ " by var name str."
#~ msgstr ""

#~ msgid "The DQAS framework function"
#~ msgstr ""

#~ msgid "Variables within the DQAS framework"
#~ msgstr ""

#~ msgid "Return type"
#~ msgstr ""

#~ msgid ""
#~ "This function works only when nnp "
#~ "has the same shape as stp, i.e."
#~ " one parameter for each op."
#~ msgstr ""

#~ msgid "The kernel for multiprocess to run parallel in DQAS function/"
#~ msgstr ""

#~ msgid ""
#~ "parallel variational parameter training and"
#~ " search to avoid local minimum not"
#~ " limited to qaoa setup as the "
#~ "function name indicates, as long as "
#~ "you provided suitable `vag_func`"
#~ msgstr ""

#~ msgid "data input generator for vag_func"
#~ msgstr ""

#~ msgid "vag_kernel"
#~ msgstr ""

#~ msgid "number of tries"
#~ msgstr ""

#~ msgid ""
#~ "for optimization problem the input is"
#~ " in general fixed so batch is "
#~ "often 1"
#~ msgstr ""

#~ msgid "number of parallel jobs"
#~ msgstr ""

#~ msgid "mean value of normal distribution for nnp"
#~ msgstr ""

#~ msgid "std deviation of normal distribution for nnp"
#~ msgstr ""

#~ msgid "Doesn't support prob model DQAS search."
#~ msgstr ""

#~ msgid "Modules for graph instance data and more"
#~ msgstr ""

#~ msgid "```python d = nx.to_dict_of_dicts(g) ```"
#~ msgstr ""

#~ msgid "1D PBC chain with n sites."
#~ msgstr ""

#~ msgid "The number of nodes"
#~ msgstr ""

#~ msgid "The resulted graph g"
#~ msgstr ""

#~ msgid "all graphs with m edge out from g"
#~ msgstr ""

#~ msgid ""
#~ "Generate a reduced graph with given "
#~ "ratio of edges compared to the "
#~ "original graph g."
#~ msgstr ""

#~ msgid "The base graph"
#~ msgstr ""

#~ msgid "number of edges kept, default half of the edges"
#~ msgstr ""

#~ msgid "The resulted reduced graph"
#~ msgstr ""

#~ msgid "Split the graph in exactly ``split`` piece evenly."
#~ msgstr ""

#~ msgid "The mother graph"
#~ msgstr ""

#~ msgid "The number of the graph we want to divide into, defaults to 2"
#~ msgstr ""

#~ msgid "List of graph instance of size ``split``"
#~ msgstr ""

#~ msgid "Module for functions adding layers of circuits"
#~ msgstr ""

#~ msgid "Hlayer"
#~ msgstr ""

#~ msgid "anyrxlayer"
#~ msgstr ""

#~ msgid "anyrylayer"
#~ msgstr ""

#~ msgid "anyrzlayer"
#~ msgstr ""

#~ msgid "anyswaplayer"
#~ msgstr ""

#~ msgid "anyxxlayer"
#~ msgstr ""

#~ msgid "anyxylayer"
#~ msgstr ""

#~ msgid "anyxzlayer"
#~ msgstr ""

#~ msgid "anyyxlayer"
#~ msgstr ""

#~ msgid "anyyylayer"
#~ msgstr ""

#~ msgid "anyyzlayer"
#~ msgstr ""

#~ msgid "anyzxlayer"
#~ msgstr ""

#~ msgid "anyzylayer"
#~ msgstr ""

#~ msgid "anyzzlayer"
#~ msgstr ""

#~ msgid "cnotlayer"
#~ msgstr ""

#~ msgid "rxlayer"
#~ msgstr ""

#~ msgid "rylayer"
#~ msgstr ""

#~ msgid "rzlayer"
#~ msgstr ""

#~ msgid "swaplayer"
#~ msgstr ""

#~ msgid "xxgate"
#~ msgstr ""

#~ msgid "xxlayer"
#~ msgstr ""

#~ msgid "xygate"
#~ msgstr ""

#~ msgid "xylayer"
#~ msgstr ""

#~ msgid "xzgate"
#~ msgstr ""

#~ msgid "xzlayer"
#~ msgstr ""

#~ msgid "yxgate"
#~ msgstr ""

#~ msgid "yxlayer"
#~ msgstr ""

#~ msgid "yygate"
#~ msgstr ""

#~ msgid "yylayer"
#~ msgstr ""

#~ msgid "yzgate"
#~ msgstr ""

#~ msgid "yzlayer"
#~ msgstr ""

#~ msgid "zxgate"
#~ msgstr ""

#~ msgid "zxlayer"
#~ msgstr ""

#~ msgid "zygate"
#~ msgstr ""

#~ msgid "zylayer"
#~ msgstr ""

#~ msgid "zzgate"
#~ msgstr ""

#~ msgid "zzlayer"
#~ msgstr ""

#~ msgid "$$e^{-i     heta_i \\sigma}$$"
#~ msgstr ""

#~ msgid ""
#~ "The following function should be used"
#~ " to generate layers with special "
#~ "case. As its soundness depends on "
#~ "the nature of the task or problem,"
#~ " it doesn't always make sense."
#~ msgstr ""

#~ msgid "$$e^{-i heta \\sigma}$$"
#~ msgstr ""

#~ msgid "$$e^{-i     heta \\sigma}$$"
#~ msgstr ""

#~ msgid ""
#~ "A collection of useful function snippets"
#~ " that irrelevant with the main "
#~ "modules or await for furthere refactor"
#~ msgstr ""

#~ msgid "Bases: :py:class:`object`"
#~ msgstr ""

#~ msgid ""
#~ "color cirq circuit SVG for given "
#~ "gates, a small tool to hack the"
#~ " cirq SVG"
#~ msgstr ""

#~ msgid "integer coordinate which gate is colored"
#~ msgstr ""

#~ msgid "transform repr form of an array to real numpy array"
#~ msgstr ""

#~ msgid "DQAS application kernels as vag functions"
#~ msgstr ""

#~ msgid "1D array for full wavefunction, the basis is in lexcical order"
#~ msgstr ""

#~ msgid "nx.Graph"
#~ msgstr ""

#~ msgid "transformation functions before averaged"
#~ msgstr ""

#~ msgid "as f3"
#~ msgstr ""

#~ msgid "maxcut energy for n qubit wavefunction i-th basis"
#~ msgstr ""

#~ msgid "ranged from 0 to 2**n-1"
#~ msgstr ""

#~ msgid "number of qubits"
#~ msgstr ""

#~ msgid ""
#~ "deprecated as non tf and non "
#~ "flexible, use the combination of "
#~ "``reduced_density_matrix`` and ``entropy`` instead."
#~ msgstr ""

#~ msgid "deprecated, current version in tc.quantum"
#~ msgstr ""

#~ msgid ""
#~ "value and gradient, currently only "
#~ "tensorflow backend is supported jax and"
#~ " numpy seems to be slow in "
#~ "circuit simulation anyhow. *deprecated*"
#~ msgstr ""

#~ msgid "if lbd=0, take energy as objective"
#~ msgstr ""

#~ msgid "if as default 0, overlap will not compute in the process"
#~ msgstr ""

#~ msgid "Fill single qubit gates according to placeholder on circuit"
#~ msgstr ""

#~ msgid "Hamiltonian measurements for Heisenberg model on graph lattice g"
#~ msgstr ""

#~ msgid "short cut for ``cirq.LineQubit(i)``"
#~ msgstr ""

#~ msgid "QAOA block encoding kernel, support 2 params in one op"
#~ msgstr ""

#~ msgid ""
#~ "training QAOA with only optimizing "
#~ "circuit parameters, can be well replaced"
#~ " with more general function `DQAS_search`"
#~ msgstr ""

#~ msgid "multi parameter for one layer"
#~ msgstr ""

#~ msgid "kw arguments for measurements_func"
#~ msgstr ""

#~ msgid "loss function, gradient of nnp"
#~ msgstr ""

#~ msgid ""
#~ "tensorflow quantum backend compare to "
#~ "qaoa_vag which is tensorcircuit backend"
#~ msgstr ""

#~ msgid "Hamiltonian for tfim on lattice defined by graph g"
#~ msgstr ""

#~ msgid "cirq.PauliSum as operators for tfq expectation layer"
#~ msgstr ""

#~ msgid ""
#~ "generate random wavefunction from "
#~ "approximately Haar measure, reference:  "
#~ "https://doi.org/10.1063/1.4983266"
#~ msgstr ""

#~ msgid "repetition of the blocks"
#~ msgstr ""

#~ msgid "random Haar measure approximation"
#~ msgstr ""

#~ msgid "cirq.Circuit, empty circuit"
#~ msgstr ""

#~ msgid "# of qubit"
#~ msgstr ""

#~ msgid ""
#~ "One-hot variational autoregressive models "
#~ "for multiple categorical choices beyond "
#~ "binary"
#~ msgstr ""

#~ msgid "Bases: :py:class:`keras.engine.training.Model`"
#~ msgstr ""

#~ msgid "Calls the model on new inputs and returns the outputs as tensors."
#~ msgstr ""

#~ msgid ""
#~ "In this case `call()` just reapplies "
#~ "all ops in the graph to the "
#~ "new inputs (e.g. build a new "
#~ "computational graph from the provided "
#~ "inputs)."
#~ msgstr ""

#~ msgid ""
#~ "Note: This method should not be "
#~ "called directly. It is only meant "
#~ "to be overridden when subclassing "
#~ "`tf.keras.Model`. To call a model on "
#~ "an input, always use the `__call__()`"
#~ " method, i.e. `model(inputs)`, which relies"
#~ " on the underlying `call()` method."
#~ msgstr ""

#~ msgid "Args:"
#~ msgstr ""

#~ msgid ""
#~ "inputs: Input tensor, or dict/list/tuple "
#~ "of input tensors. training: Boolean or"
#~ " boolean scalar tensor, indicating whether"
#~ " to run"
#~ msgstr ""

#~ msgid "the `Network` in training mode or inference mode."
#~ msgstr ""

#~ msgid "mask: A mask or list of masks. A mask can be either a boolean tensor or"
#~ msgstr ""

#~ msgid "None (no mask). For more details, check the guide"
#~ msgstr ""

#~ msgid "[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
#~ msgstr ""

#~ msgid "Returns:"
#~ msgstr ""

#~ msgid ""
#~ "A tensor if there is a single "
#~ "output, or a list of tensors if"
#~ " there are more than one outputs."
#~ msgstr ""

#~ msgid "Bases: :py:class:`keras.engine.base_layer.Layer`"
#~ msgstr ""

#~ msgid ""
#~ "Creates the variables of the layer "
#~ "(optional, for subclass implementers)."
#~ msgstr ""

#~ msgid ""
#~ "This is a method that implementers "
#~ "of subclasses of `Layer` or `Model` "
#~ "can override if they need a "
#~ "state-creation step in-between layer "
#~ "instantiation and layer call. It is "
#~ "invoked automatically before the first "
#~ "execution of `call()`."
#~ msgstr ""

#~ msgid ""
#~ "This is typically used to create "
#~ "the weights of `Layer` subclasses (at"
#~ " the discretion of the subclass "
#~ "implementer)."
#~ msgstr ""

#~ msgid "input_shape: Instance of `TensorShape`, or list of instances of"
#~ msgstr ""

#~ msgid ""
#~ "`TensorShape` if the layer expects a "
#~ "list of inputs (one instance per "
#~ "input)."
#~ msgstr ""

#~ msgid "This is where the layer's logic lives."
#~ msgstr ""

#~ msgid ""
#~ "The `call()` method may not create "
#~ "state (except in its first invocation,"
#~ " wrapping the creation of variables "
#~ "or other resources in `tf.init_scope()`). "
#~ "It is recommended to create state "
#~ "in `__init__()`, or the `build()` method"
#~ " that is called automatically before "
#~ "`call()` executes the first time."
#~ msgstr ""

#~ msgid "inputs: Input tensor, or dict/list/tuple of input tensors."
#~ msgstr ""

#~ msgid ""
#~ "The first positional `inputs` argument "
#~ "is subject to special rules: - "
#~ "`inputs` must be explicitly passed. A"
#~ " layer cannot have zero"
#~ msgstr ""

#~ msgid ""
#~ "arguments, and `inputs` cannot be "
#~ "provided via the default value of "
#~ "a keyword argument."
#~ msgstr ""

#~ msgid "NumPy array or Python scalar values in `inputs` get cast as tensors."
#~ msgstr ""

#~ msgid "Keras mask metadata is only collected from `inputs`."
#~ msgstr ""

#~ msgid ""
#~ "Layers are built (`build(input_shape)` method)"
#~ " using shape info from `inputs` only."
#~ msgstr ""

#~ msgid "`input_spec` compatibility is only checked against `inputs`."
#~ msgstr ""

#~ msgid ""
#~ "Mixed precision input casting is only"
#~ " applied to `inputs`. If a layer "
#~ "has tensor arguments in `*args` or "
#~ "`**kwargs`, their casting behavior in "
#~ "mixed precision should be handled "
#~ "manually."
#~ msgstr ""

#~ msgid "The SavedModel input specification is generated using `inputs` only."
#~ msgstr ""

#~ msgid ""
#~ "Integration with various ecosystem packages"
#~ " like TFMOT, TFLite, TF.js, etc is"
#~ " only supported for `inputs` and not"
#~ " for tensors in positional and "
#~ "keyword arguments."
#~ msgstr ""

#~ msgid "*args: Additional positional arguments. May contain tensors, although"
#~ msgstr ""

#~ msgid "this is not recommended, for the reasons above."
#~ msgstr ""

#~ msgid "**kwargs: Additional keyword arguments. May contain tensors, although"
#~ msgstr ""

#~ msgid ""
#~ "this is not recommended, for the "
#~ "reasons above. The following optional "
#~ "keyword arguments are reserved: - "
#~ "`training`: Boolean scalar tensor of "
#~ "Python boolean indicating"
#~ msgstr ""

#~ msgid "whether the `call` is meant for training or inference."
#~ msgstr ""

#~ msgid ""
#~ "`mask`: Boolean input mask. If the "
#~ "layer's `call()` method takes a `mask`"
#~ " argument, its default value will be"
#~ " set to the mask generated for "
#~ "`inputs` by the previous layer (if "
#~ "`input` did come from a layer that"
#~ " generated a corresponding mask, i.e. "
#~ "if it came from a Keras layer "
#~ "with masking support)."
#~ msgstr ""

#~ msgid "A tensor or list/tuple of tensors."
#~ msgstr ""

#~ msgid "Relevant classes for VQNHE"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule`"
#~ msgstr ""

#~ msgid "Dense layer but with complex weights, used for building complex RBM"
#~ msgstr ""

#~ msgid "VQNHE"
#~ msgstr ""

#~ msgid "[description]"
#~ msgstr ""

#~ msgid "VQE"
#~ msgstr ""

#~ msgid "Backend register"
#~ msgstr ""

#~ msgid "Get the `tc.backend` object."
#~ msgstr ""

#~ msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
#~ msgstr ""

#~ msgid "Raises"
#~ msgstr ""

#~ msgid "Backend doesn't exist for `backend` argument."
#~ msgstr ""

#~ msgid "The `tc.backend` object that with all registered universal functions."
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: jax backend"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensornetwork.backends.jax.jax_backend.JaxBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "``jax backend``. "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
#~ msgstr ""

#~ msgid "Returns the elementwise absolute value of tensor. Args:"
#~ msgstr ""

#~ msgid "tensor: An input tensor."
#~ msgstr ""

#~ msgid "tensor: Its elementwise absolute value."
#~ msgstr ""

#~ msgid "Return the index of maximum of an array an axis."
#~ msgstr ""

#~ msgid "[description], defaults to 0, different behavior from numpy defaults!"
#~ msgstr ""

#~ msgid "Return the index of minimum of an array an axis."
#~ msgstr ""

#~ msgid "Cast the tensor dtype of a ``a``."
#~ msgstr ""

#~ msgid "tensor"
#~ msgstr ""

#~ msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
#~ msgstr ""

#~ msgid "``a`` of new dtype"
#~ msgstr ""

#~ msgid "Join a sequence of arrays along an existing axis."
#~ msgstr ""

#~ msgid "[description], defaults to 0"
#~ msgstr ""

#~ msgid ""
#~ "The native cond for XLA compiling, "
#~ "wrapper for ``tf.cond`` and limited "
#~ "functionality of ``jax.lax.cond``."
#~ msgstr ""

#~ msgid "Convert a np.array or a tensor to a tensor type for the backend."
#~ msgstr ""

#~ msgid ""
#~ "Generate the coo format sparse matrix"
#~ " from indices and values, which is"
#~ " the only sparse format supported in"
#~ " different ML backends."
#~ msgstr ""

#~ msgid "shape [n, 2] for n non zero values in the returned matrix"
#~ msgstr ""

#~ msgid "shape [n]"
#~ msgstr ""

#~ msgid "Tuple[int, ...]"
#~ msgstr ""

#~ msgid "Return the expm of ``a``, matrix exponential."
#~ msgstr ""

#~ msgid "tensor in matrix form"
#~ msgstr ""

#~ msgid "matrix exponential of matrix ``a``"
#~ msgstr ""

#~ msgid "Return the cosine of a tensor ``a``."
#~ msgstr ""

#~ msgid "cosine of ``a``"
#~ msgstr ""

#~ msgid "Return the cumulative sum of the elements along a given axis."
#~ msgstr ""

#~ msgid ""
#~ "The default behavior is the same "
#~ "as numpy, different from tf/torch as "
#~ "cumsum of the flatten 1D array, "
#~ "defaults to None"
#~ msgstr ""

#~ msgid "Return the copy of tensor ''a''."
#~ msgstr ""

#~ msgid "Return an identity matrix of dimension `dim`"
#~ msgstr ""

#~ msgid ""
#~ "Depending on specific backends, `dim` "
#~ "has to be either an int (numpy,"
#~ " torch, tensorflow) or a `ShapeType` "
#~ "object (for block-sparse backends). "
#~ "Block-sparse behavior is currently not "
#~ "supported"
#~ msgstr ""

#~ msgid ""
#~ "N (int): The dimension of the "
#~ "returned matrix. dtype: The dtype of "
#~ "the returned matrix. M (int): The "
#~ "dimension of the returned matrix."
#~ msgstr ""

#~ msgid "Return the function which is the grad function of input ``f``."
#~ msgstr ""

#~ msgid "Example"
#~ msgstr ""

#~ msgid "the function to be differentiated"
#~ msgstr ""

#~ msgid ""
#~ "the position of args in ``f`` that"
#~ " are to be differentiated, defaults "
#~ "to be 0"
#~ msgstr ""

#~ msgid "the grad function of ``f`` with the same set of arguments as ``f``"
#~ msgstr ""

#~ msgid "Return 1.j in as a tensor compatible with the backend."
#~ msgstr ""

#~ msgid "\"complex64\" or \"complex128\""
#~ msgstr ""

#~ msgid "1.j tensor"
#~ msgstr ""

#~ msgid "Return the elementwise imaginary value of a tensor ``a``."
#~ msgstr ""

#~ msgid "imaginary value of ``a``"
#~ msgstr ""

#~ msgid "[summary]"
#~ msgstr ""

#~ msgid "The possible options"
#~ msgstr ""

#~ msgid "Sampling output shape"
#~ msgstr ""

#~ msgid ""
#~ "probability for each option in a, "
#~ "defaults to None, as equal probability"
#~ " distribution"
#~ msgstr ""

#~ msgid ""
#~ "Call the random normal function with "
#~ "the random state management behind the"
#~ " scene."
#~ msgstr ""

#~ msgid "[description], defaults to 1"
#~ msgstr ""

#~ msgid "[description], defaults to \"32\""
#~ msgstr ""

#~ msgid "Determine whether the type of input ``a`` is  ``sparse``."
#~ msgstr ""

#~ msgid "input matrix ``a``"
#~ msgstr ""

#~ msgid "a bool indicating whether the matrix ``a`` is sparse"
#~ msgstr ""

#~ msgid "Return a boolean on whether ``a`` is a tensor in backend package."
#~ msgstr ""

#~ msgid "a tensor to be determined"
#~ msgstr ""

#~ msgid "whether ``a`` is a tensor"
#~ msgstr ""

#~ msgid "Return the jitted version of function ``f``."
#~ msgstr ""

#~ msgid "function to be jitted"
#~ msgstr ""

#~ msgid ""
#~ "index of args that doesn't regarded "
#~ "as tensor, only work for jax "
#~ "backend"
#~ msgstr ""

#~ msgid ""
#~ "whether open XLA compilation, only works"
#~ " for tensorflow backend, defaults False "
#~ "since several ops has no XLA "
#~ "correspondence"
#~ msgstr ""

#~ msgid "jitted version of ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Function that computes a (forward-mode)"
#~ " Jacobian-vector product of ``f``. "
#~ "Strictly speaking, this function is "
#~ "value_and_jvp."
#~ msgstr ""

#~ msgid "The function to compute jvp"
#~ msgstr ""

#~ msgid "input for ``f``"
#~ msgstr ""

#~ msgid "tangents"
#~ msgstr ""

#~ msgid ""
#~ "(``f(*inputs)``, jvp_tensor), where jvp_tensor "
#~ "is the same shape as the output"
#~ " of ``f``"
#~ msgstr ""

#~ msgid "Return the kronecker product of two matrices ``a`` and ``b``."
#~ msgstr ""

#~ msgid "kronecker product of ``a`` and ``b``"
#~ msgstr ""

#~ msgid "Return the maximum of an array or maximum along an axis."
#~ msgstr ""

#~ msgid "[description], defaults to None"
#~ msgstr ""

#~ msgid "Return the minimum of an array or minimum along an axis."
#~ msgstr ""

#~ msgid ""
#~ "Return the numpy array of a tensor"
#~ " ``a``, but may not work in a"
#~ " jitted function."
#~ msgstr ""

#~ msgid "numpy array of ``a``"
#~ msgstr ""

#~ msgid ""
#~ "One-hot encodes the given ``a``. "
#~ "Each index in the input ``a`` is"
#~ " encoded as a vector of zeros "
#~ "of length ``num`` with the element "
#~ "at index set to one:"
#~ msgstr ""

#~ msgid "input tensor"
#~ msgstr ""

#~ msgid "number of features in onehot dimension"
#~ msgstr ""

#~ msgid "onehot tensor with the last extra dimension"
#~ msgstr ""

#~ msgid ""
#~ "Return an ones-matrix of dimension "
#~ "`dim` Depending on specific backends, "
#~ "`dim` has to be either an int "
#~ "(numpy, torch, tensorflow) or a "
#~ "`ShapeType` object (for block-sparse "
#~ "backends). Block-sparse behavior is "
#~ "currently not supported Args:"
#~ msgstr ""

#~ msgid ""
#~ "shape (int): The dimension of the "
#~ "returned matrix. dtype: The dtype of "
#~ "the returned matrix."
#~ msgstr ""

#~ msgid ""
#~ "A jax like split API, but it "
#~ "doesn't split the key generator for "
#~ "other backends. It is just for a"
#~ " consistent interface of random code; "
#~ "make sure you know what the "
#~ "function actually does. This function is"
#~ " mainly a utility to write backend"
#~ " agnostic code instead of doing magic"
#~ " things."
#~ msgstr ""

#~ msgid "Return the elementwise real value of a tensor ``a``."
#~ msgstr ""

#~ msgid "real value of ``a``"
#~ msgstr ""

#~ msgid ""
#~ "Rectified linear unit activation function. "
#~ "Computes the element-wise function:"
#~ msgstr ""

#~ msgid "\\mathrm{relu}(x)=\\max(x,0)"
#~ msgstr ""

#~ msgid "Input tensor"
#~ msgstr ""

#~ msgid "Tensor after relu"
#~ msgstr ""

#~ msgid ""
#~ "Roughly equivalent to operand[indices] = "
#~ "updates, indices only support shape with"
#~ " rank 2 for now."
#~ msgstr ""

#~ msgid "Set the random state attached to the backend."
#~ msgstr ""

#~ msgid "the random seed, defaults to be None"
#~ msgstr ""

#~ msgid ""
#~ "If set to be true, only get "
#~ "the random state in return instead "
#~ "of setting the state on the "
#~ "backend"
#~ msgstr ""

#~ msgid "Return the  elementwise sine of a tensor ``a``."
#~ msgstr ""

#~ msgid "sine of ``a``"
#~ msgstr ""

#~ msgid "Return the total number of elements in ``a`` in tensor form."
#~ msgstr ""

#~ msgid "the total number of elements in ``a``"
#~ msgstr ""

#~ msgid ""
#~ "Softmax function. Computes the function "
#~ "which rescales elements to the range "
#~ "[0,1] such that the elements along "
#~ "axis sum to 1."
#~ msgstr ""

#~ msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
#~ msgstr ""

#~ msgid "Tensor"
#~ msgstr ""

#~ msgid ""
#~ "A dimension along which Softmax will "
#~ "be computed , defaults to None for"
#~ " all axis sum."
#~ msgstr ""

#~ msgid "concatenated tensor"
#~ msgstr ""

#~ msgid "Solve the linear system Ax=b and return the solution x."
#~ msgstr ""

#~ msgid "The multiplied matrix."
#~ msgstr ""

#~ msgid "The resulted matrix."
#~ msgstr ""

#~ msgid "The solution of the linear system."
#~ msgstr ""

#~ msgid "A sparse matrix multiplies a dense matrix."
#~ msgstr ""

#~ msgid "a sparse matrix"
#~ msgstr ""

#~ msgid "a dense matrix"
#~ msgstr ""

#~ msgid "dense matrix"
#~ msgstr ""

#~ msgid ""
#~ "Concatenates a sequence of tensors ``a``"
#~ " along a new dimension ``axis``."
#~ msgstr ""

#~ msgid "List of tensors in the same shape"
#~ msgstr ""

#~ msgid "the stack axis, defaults to 0"
#~ msgstr ""

#~ msgid "stateful register for each package"
#~ msgstr ""

#~ msgid "shape of output sampling tensor"
#~ msgstr ""

#~ msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
#~ msgstr ""

#~ msgid "Uniform random sampler from ``low`` to ``high``."
#~ msgstr ""

#~ msgid "shape of output sampling tensor, defaults to 1"
#~ msgstr ""

#~ msgid "Stop backpropagation from ``a``."
#~ msgstr ""

#~ msgid "``branches[index]()``"
#~ msgstr ""

#~ msgid "Constructs a tensor by tiling a given tensor."
#~ msgstr ""

#~ msgid "1d tensor with length the same as the rank of ``a``"
#~ msgstr ""

#~ msgid "Convert a sparse matrix to dense tensor."
#~ msgstr ""

#~ msgid "the resulted dense matrix"
#~ msgstr ""

#~ msgid ""
#~ "Find the unique elements and their "
#~ "corresponding counts of the given tensor"
#~ " ``a``."
#~ msgstr ""

#~ msgid "Unique elements, corresponding counts"
#~ msgstr ""

#~ msgid "Return the function which returns the value and grad of ``f``."
#~ msgstr ""

#~ msgid ""
#~ "the value and grad function of "
#~ "``f`` with the same set of "
#~ "arguments as ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Return the VVAG function of ``f``. "
#~ "The inputs for ``f`` is (args[0], "
#~ "args[1], args[2], ...), and the output"
#~ " of ``f`` is a scalar. Suppose "
#~ "VVAG(f) is a function with inputs "
#~ "in the form (vargs[0], args[1], args[2],"
#~ " ...), where vagrs[0] has one extra"
#~ " dimension than args[0] in the first"
#~ " axis and consistent with args[0] in"
#~ " shape for remaining dimensions, i.e. "
#~ "shape(vargs[0]) = [batch] + shape(args[0])."
#~ " (We only cover cases where "
#~ "``vectorized_argnums`` defaults to 0 here "
#~ "for demonstration). VVAG(f) returns a "
#~ "tuple as a value tensor with shape"
#~ " [batch, 1] and a gradient tuple "
#~ "with shape: ([batch]+shape(args[argnum]) for "
#~ "argnum in argnums). The gradient for "
#~ "argnums=k is defined as"
#~ msgstr ""

#~ msgid ""
#~ "g^k = \\frac{\\partial \\sum_{i\\in batch} "
#~ "f(vargs[0][i], args[1], ...)}{\\partial args[k]}"
#~ msgstr ""

#~ msgid "Therefore, if argnums=0, the gradient is reduced to"
#~ msgstr ""

#~ msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
#~ msgstr ""

#~ msgid ""
#~ ", which is specifically suitable for "
#~ "batched VQE optimization, where args[0] "
#~ "is the circuit parameters."
#~ msgstr ""

#~ msgid "And if argnums=1, the gradient is like"
#~ msgstr ""

#~ msgid ""
#~ "g^1_i = \\frac{\\partial \\sum_j "
#~ "f(vargs[0][j], args[1])}{\\partial args[1][i]}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ ", which is suitable for quantum "
#~ "machine learning scenarios, where ``f`` "
#~ "is the loss function, args[0] "
#~ "corresponds to the input data and "
#~ "args[1] corresponds to the weights in"
#~ " the QML model."
#~ msgstr ""

#~ msgid ""
#~ "the args to be vectorized, these "
#~ "arguments should share the same batch"
#~ " shape in the fist dimension"
#~ msgstr ""

#~ msgid ""
#~ "Function that computes the dot product"
#~ " between a vector v and the "
#~ "Jacobian of the given function at "
#~ "the point given by the inputs. "
#~ "(reverse mode AD relevant) Strictly "
#~ "speaking, this function is value_and_vjp."
#~ msgstr ""

#~ msgid "the function to carry out vjp calculation"
#~ msgstr ""

#~ msgid ""
#~ "value vector or gradient from downstream"
#~ " in reverse mode AD the same "
#~ "shape as return of function ``f``"
#~ msgstr ""

#~ msgid ""
#~ "(``f(*inputs)``, vjp_tensor), where vjp_tensor "
#~ "is the same shape as inputs"
#~ msgstr ""

#~ msgid ""
#~ "Return the vectorized map or batched "
#~ "version of ``f`` on the first "
#~ "extra axis. The general interface "
#~ "supports ``f`` with multiple arguments "
#~ "and broadcast in the fist dimension."
#~ msgstr ""

#~ msgid "function to be broadcasted."
#~ msgstr ""

#~ msgid "vmap version of ``f``"
#~ msgstr ""

#~ msgid ""
#~ "Return a zeros-matrix of dimension "
#~ "`dim` Depending on specific backends, "
#~ "`dim` has to be either an int "
#~ "(numpy, torch, tensorflow) or a "
#~ "`ShapeType` object (for block-sparse "
#~ "backends)."
#~ msgstr ""

#~ msgid "Block-sparse behavior is currently not supported Args:"
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: numpy backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`"
#~ msgstr ""

#~ msgid ""
#~ "see the original backend API at "
#~ "`numpy backend "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: pytorch backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "``pytorch backend``. "
#~ "`<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
#~ msgstr ""

#~ msgid ""
#~ "Note the functionality provided by "
#~ "pytorch backend is incomplete, it "
#~ "currenly lacks native efficicent jit and"
#~ " vmap support."
#~ msgstr ""

#~ msgid "Backend magic inherited from tensornetwork: tensorflow backend"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`"
#~ msgstr ""

#~ msgid ""
#~ "See the original backend API at "
#~ "`'tensorflow backend''. "
#~ "<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
#~ msgstr ""

#~ msgid "Some common noise quantum channels."
#~ msgstr ""

#~ msgid ""
#~ "Return an amplitude damping channel. "
#~ "Notice: Amplitude damping corrspondings to "
#~ "p = 1."
#~ msgstr ""

#~ msgid ""
#~ "\\sqrt{p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & \\sqrt{1-\\gamma}\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & \\sqrt{\\gamma}\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{1-p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    \\sqrt{1-\\gamma} & 0\\\\\n"
#~ "    0 & 1\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{1-p}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 0\\\\\n"
#~ "    \\sqrt{\\gamma} & 0\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "the damping parameter of amplitude (:math:`\\gamma`)"
#~ msgstr ""

#~ msgid ":math:`p`"
#~ msgstr ""

#~ msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
#~ msgstr ""

#~ msgid "Return a Depolarizing Channel"
#~ msgstr ""

#~ msgid ""
#~ "\\sqrt{1-p_x-p_y-p_z}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & 1\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_x}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 1\\\\\n"
#~ "    1 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_y}\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & -1j\\\\\n"
#~ "    1j & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\sqrt{p_z}\n"
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & -1\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ":math:`p_x`"
#~ msgstr ""

#~ msgid ":math:`p_y`"
#~ msgstr ""

#~ msgid ":math:`p_z`"
#~ msgstr ""

#~ msgid "Sequences of Gates"
#~ msgstr ""

#~ msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
#~ msgstr ""

#~ msgid ""
#~ "\\sum_{k}^{} K_k \\otimes K_k^{\\dagger}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "A sequence of Gate"
#~ msgstr ""

#~ msgid "The corresponding Tensor of the list of Kraus operators"
#~ msgstr ""

#~ msgid "Return a phase damping channel with given :math:`\\gamma`"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & \\sqrt{1-\\gamma}\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 0\\\\\n"
#~ "    0 & \\sqrt{\\gamma}\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "The damping parameter of phase (:math:`\\gamma`)"
#~ msgstr ""

#~ msgid "A phase damping channel with given :math:`\\gamma`"
#~ msgstr ""

#~ msgid "Reset channel"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}\n"
#~ "    1 & 0\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\\qquad\n"
#~ "\\begin{bmatrix}\n"
#~ "    0 & 1\\\\\n"
#~ "    0 & 0\\\\\n"
#~ "\\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "Check identity of a single qubit Kraus operators."
#~ msgstr ""

#~ msgid "Examples:"
#~ msgstr ""

#~ msgid ""
#~ "\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
#~ "\n"
#~ msgstr ""

#~ msgid "List of Kraus operators."
#~ msgstr ""

#~ msgid "Quantum circuit: state simulator"
#~ msgstr ""

#~ msgid "``Circuit`` class. Simple usage demo below."
#~ msgstr ""

#~ msgid "Apply any gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Qubit number than the gate applies on."
#~ msgstr ""

#~ msgid "Parameters for the gate"
#~ msgstr ""

#~ msgid "Apply cnot gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 1.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Qubit number than the gate applies on. The matrix for the gate is"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply cr gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply crx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply cry gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply crz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply cy gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.-1.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+1.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.-1.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply cz gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & -1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply exp gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply exp1 gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply h gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & 0.70710677+0.j\\\\    "
#~ "0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.70710677+0.j & "
#~ "0.70710677+0.j\\\\    0.70710677+0.j & "
#~ "-0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply i gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply iswap gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+1.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j"
#~ " & 0.+1.j & 0.+0.j\\\\    0.+0.j &"
#~ " 0.+1.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply r gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply rx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ry gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply rz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply s gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.+1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+1.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply sd gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & 0.-1.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.-1.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply swap gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j"
#~ " & 1.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j"
#~ " \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply t gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677+0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1. & +0.j & 0. "
#~ "& +0.j\\\\    0. & +0.j & "
#~ "0.70710677+0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply td gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1. &"
#~ " +0.j & 0. & +0.j\\\\    0. &"
#~ " +0.j & 0.70710677-0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1. & +0.j & 0. "
#~ "& +0.j\\\\    0. & +0.j & "
#~ "0.70710677-0.70710677j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply toffoli gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j\\\\    0.+0.j &"
#~ " 1.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply wroot gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    "
#~ "0.70710677+0.j & -0.5 & -0.5j\\\\    0.5"
#~ " & -0.5j & 0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.70710677+0.j & -0.5 &"
#~ " -0.5j\\\\    0.5 & -0.5j & "
#~ "0.70710677+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply x gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j\\\\    1.+0.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    "
#~ "1.+0.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply y gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j\\\\    0.+1.j & 0.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    "
#~ "0.+1.j & 0.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply z gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j\\\\    0.+0.j & -1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & -1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Circuit object based on state simulator."
#~ msgstr ""

#~ msgid "The number of qubits in the circuit."
#~ msgstr ""

#~ msgid ""
#~ "If not None, the initial state of"
#~ " the circuit is taken as ``inputs``"
#~ " instead of :math:`\\vert 0\\rangle^n` "
#~ "qubits, defaults to None"
#~ msgstr ""

#~ msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction"
#~ msgstr ""

#~ msgid ""
#~ "dict if two qubit gate is ready"
#~ " for split, including parameters for "
#~ "at least one of ``max_singular_values`` "
#~ "and ``max_truncation_err``."
#~ msgstr ""

#~ msgid ""
#~ "Monte Carlo trajectory simulation of "
#~ "general Kraus channel whose Kraus "
#~ "operators cannot be amplified to unitary"
#~ " operators. For unitary operators composed"
#~ " Kraus channel, :py:meth:`unitary_kraus` is "
#~ "much faster."
#~ msgstr ""

#~ msgid ""
#~ "This function is jittable in theory. "
#~ "But only jax+GPU combination is "
#~ "recommended for jit since the graph "
#~ "building time is too long for "
#~ "other backend options; though the "
#~ "running time of the function is "
#~ "very fast for every case."
#~ msgstr ""

#~ msgid "list of ``tn.Node`` for Kraus operators"
#~ msgstr ""

#~ msgid "the qubits index that Kraus channel is applied on"
#~ msgstr ""

#~ msgid ""
#~ "random tensor between 0 or 1, "
#~ "defaults to be None, the random "
#~ "number will be generated automatically"
#~ msgstr ""

#~ msgid "Compute the expectation of corresponding operators."
#~ msgstr ""

#~ msgid ""
#~ "operator and its position on the "
#~ "circuit, eg. ``(tc.gates.z(), [1, ]), "
#~ "(tc.gates.x(), [2, ])`` is for operator"
#~ " :math:`Z_1X_2`"
#~ msgstr ""

#~ msgid ""
#~ "if True, then the wavefunction tensor"
#~ " is cached for further expectation "
#~ "evaluation, defaults to be true"
#~ msgstr ""

#~ msgid "Tensor with one element"
#~ msgstr ""

#~ msgid "[WIP], check whether the circuit is legal."
#~ msgstr ""

#~ msgid "the bool indicating whether the circuit is legal"
#~ msgstr ""

#~ msgid "Take measurement to the given quantum lines."
#~ msgstr ""

#~ msgid "measure on which quantum line"
#~ msgstr ""

#~ msgid "if true, theoretical probability is also returned"
#~ msgstr ""

#~ msgid ""
#~ "Middle measurement in z-basis on the "
#~ "circuit, note the wavefunction output is"
#~ " not normalized with ``mid_measurement`` "
#~ "involved, one should normalize the state"
#~ " manually if needed."
#~ msgstr ""

#~ msgid "the index of qubit that the Z direction postselection applied on"
#~ msgstr ""

#~ msgid "0 for spin up, 1 for spin down, defaults to be 0"
#~ msgstr ""

#~ msgid "Reference: arXiv:1201.3974."
#~ msgstr ""

#~ msgid "sampled bit string and the corresponding theoretical probability"
#~ msgstr ""

#~ msgid "Replace the input state with the circuit structure unchanged."
#~ msgstr ""

#~ msgid "Input wavefunction."
#~ msgstr ""

#~ msgid ""
#~ "Replace the input state in MPS "
#~ "representation while keep the circuit "
#~ "structure unchanged."
#~ msgstr ""

#~ msgid "Compute the output wavefunction from the circuit."
#~ msgstr ""

#~ msgid "the str indicating the form of the output wavefunction"
#~ msgstr ""

#~ msgid "Tensor with the corresponding shape"
#~ msgstr ""

#~ msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`"
#~ msgstr ""

#~ msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
#~ msgstr ""

#~ msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
#~ msgstr ""

#~ msgid "[description], defaults to None, which is the same as ``ket``"
#~ msgstr ""

#~ msgid "[description], defaults to True"
#~ msgstr ""

#~ msgid "[description], defaults to False"
#~ msgstr ""

#~ msgid ""
#~ "Not an ideal visualization for quantum"
#~ " circuit, but reserve here as a "
#~ "general approch to show tensornetwork "
#~ "[Deperacted, use ``qir2tex instead``]"
#~ msgstr ""

#~ msgid "Constants and setups"
#~ msgstr ""

#~ msgid ""
#~ "To set runtime contractor of the "
#~ "tensornetwork for a better contraction "
#~ "path."
#~ msgstr ""

#~ msgid ""
#~ "\"auto\", \"greedy\", \"branch\", \"plain\", "
#~ "\"tng\", \"custom\", \"custom_stateful\". defaults"
#~ " to None (\"auto\")"
#~ msgstr ""

#~ msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "It is not very useful, as "
#~ "``memory_limit`` leads to ``branch`` "
#~ "contraction instead of ``greedy`` which "
#~ "is rather slow, defaults to None"
#~ msgstr ""

#~ msgid "Tensornetwork version is too low to support some of the contractors."
#~ msgstr ""

#~ msgid "Unknown method options."
#~ msgstr ""

#~ msgid "The new tensornetwork with its contractor set."
#~ msgstr ""

#~ msgid "To set the runtime numerical dtype of tensors."
#~ msgstr ""

#~ msgid ""
#~ "\"complex64\" or \"complex128\", defaults to"
#~ " None, which is equivalent to "
#~ "\"complex64\"."
#~ msgstr ""

#~ msgid "The naive state-vector simulator contraction path."
#~ msgstr ""

#~ msgid "The list of ``tn.Node``."
#~ msgstr ""

#~ msgid "The list of dangling node edges, defaults to be None."
#~ msgstr ""

#~ msgid "The ``tn.Node`` after contraction"
#~ msgstr ""

#~ msgid "To set the runtime backend of tensorcircuit."
#~ msgstr ""

#~ msgid ""
#~ "Note: ``tc.set_backend`` and "
#~ "``tc.cons.set_tensornetwork_backend`` are the same."
#~ msgstr ""

#~ msgid ""
#~ "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". "
#~ "defaults to None, which gives the "
#~ "same behavior as "
#~ "``tensornetwork.backend_contextmanager.get_default_backend()``."
#~ msgstr ""

#~ msgid "Whether the object should be set as global."
#~ msgstr ""

#~ msgid "Quantum circuit class but with density matrix simulator"
#~ msgstr ""

#~ msgid "Quantum circuit class but with density matrix simulator: v2"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.densitymatrix.DMCircuit`"
#~ msgstr ""

#~ msgid "Experimental features"
#~ msgstr ""

#~ msgid ""
#~ "Declarations of single-qubit and two-"
#~ "qubit gates and their corresponding "
#~ "matrix."
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensornetwork.network_components.Node`"
#~ msgstr ""

#~ msgid "Wrapper of tn.Node, quantum gate"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.gates.GateF`"
#~ msgstr ""

#~ msgid "Note one should provide the gate with properly reshaped."
#~ msgstr ""

#~ msgid "corresponding gate"
#~ msgstr ""

#~ msgid "The name of the gate."
#~ msgstr ""

#~ msgid "the resulted gate"
#~ msgstr ""

#~ msgid "Convert the inputs to Tensor with specified dtype."
#~ msgstr ""

#~ msgid "inputs"
#~ msgstr ""

#~ msgid "dtype of the output Tensors"
#~ msgstr ""

#~ msgid "List of Tensors"
#~ msgstr ""

#~ msgid "Returns a LaTeX bmatrix."
#~ msgstr ""

#~ msgid "Formatted Display:"
#~ msgstr ""

#~ msgid ""
#~ "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & 1.+0.j \\end{bmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "2D numpy array"
#~ msgstr ""

#~ msgid "ValueError(\"bmatrix can at most display two dimensions\")"
#~ msgstr ""

#~ msgid "latex str for bmatrix of array a"
#~ msgstr ""

#~ msgid ""
#~ "Controlled rotation gate, when the "
#~ "control bit is 1, `rgate` is "
#~ "applied on the target gate."
#~ msgstr ""

#~ msgid "angle in radians"
#~ msgstr ""

#~ msgid "CR Gate"
#~ msgstr ""

#~ msgid ""
#~ "Faster exponential gate, directly implemented"
#~ " based on RHS, only work when: "
#~ ":math:`U^2` is identity matrix."
#~ msgstr ""

#~ msgid ""
#~ "\\rm{exp}(U) &= e^{-i \\theta U} \\\\\n"
#~ "        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
#~ "\n"
#~ msgstr ""

#~ msgid "input unitary (U)"
#~ msgstr ""

#~ msgid "suffix of Gate name"
#~ msgstr ""

#~ msgid "Exponential Gate"
#~ msgstr ""

#~ msgid "Exponential gate."
#~ msgstr ""

#~ msgid ""
#~ "\\rm{exp}(U) = e^{-i \\theta U}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "iSwap gate."
#~ msgstr ""

#~ msgid ""
#~ "iSwap(\\theta) =\n"
#~ "\\begin{pmatrix}\n"
#~ "    1 & 0 & 0 & 0\\\\\n"
#~ "    0 & \\cos(\\frac{\\pi}{2} \\theta )"
#~ " & j \\sin(\\frac{\\pi}{2} \\theta ) "
#~ "& 0\\\\\n"
#~ "    0 & j \\sin(\\frac{\\pi}{2} \\theta"
#~ " ) & \\cos(\\frac{\\pi}{2} \\theta ) "
#~ "& 0\\\\\n"
#~ "    0 & 0 & 0 & 1\\\\\n"
#~ "\\end{pmatrix}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "iSwap Gate"
#~ msgstr ""

#~ msgid "Convert Gate to numpy array."
#~ msgstr ""

#~ msgid "input Gate"
#~ msgstr ""

#~ msgid "corresponding Tensor"
#~ msgstr ""

#~ msgid ""
#~ "Inner helper function to generate gate"
#~ " functions, such as ``z()`` from "
#~ "``_z_matrix``"
#~ msgstr ""

#~ msgid "General single qubit rotation gate"
#~ msgstr ""

#~ msgid ""
#~ "R(\\theta, \\phi, \\alpha) = i \\cos(\\theta) I\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "- i \\sin(\\theta) \\cos(\\alpha) Z\n"
#~ "\n"
#~ msgstr ""

#~ msgid "R Gate"
#~ msgstr ""

#~ msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
#~ msgstr ""

#~ msgid "A random single qubit gate"
#~ msgstr ""

#~ msgid "Returns a random two-qubit gate."
#~ msgstr ""

#~ msgid "a random two-qubit gate"
#~ msgstr ""

#~ msgid ""
#~ "Rotation gate, which is in matrix "
#~ "exponential form, shall give the same"
#~ " result as `rgate`."
#~ msgstr ""

#~ msgid ""
#~ "mx = \\sin(\\alpha) \\cos(\\phi) X\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "my = \\sin(\\alpha) \\sin(\\phi) Y\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "mz = \\cos(\\alpha) Z\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "R(\\theta, \\alpha, \\phi) = e^{-i\\theta (mx+my+mz)}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "Rotation Gate"
#~ msgstr ""

#~ msgid "Rotation gate along X axis."
#~ msgstr ""

#~ msgid ""
#~ "RX(\\theta) = e^{-i\\frac{\\theta}{2}X}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RX Gate"
#~ msgstr ""

#~ msgid "Rotation gate along Y axis."
#~ msgstr ""

#~ msgid ""
#~ "RY(\\theta) = e^{-i\\frac{\\theta}{2}Y}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RY Gate"
#~ msgstr ""

#~ msgid "Rotation gate along Z axis."
#~ msgstr ""

#~ msgid ""
#~ "RZ(\\theta) = e^{-i\\frac{\\theta}{2}Z}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "RZ Gate"
#~ msgstr ""

#~ msgid "Interfaces bridging different backends"
#~ msgstr ""

#~ msgid "Keras layer for tc quantum function"
#~ msgstr ""

#~ msgid ""
#~ "`QuantumLayer` wraps the quantum function "
#~ "`f` as a `keras.Layer` so that "
#~ "tensorcircuit is better integrated with "
#~ "tensorflow."
#~ msgstr ""

#~ msgid "[description], defaults to \"glorot_uniform\""
#~ msgstr ""

#~ msgid ""
#~ "Load function from the files in "
#~ "the ``tf.savedmodel`` format. We can "
#~ "load several functions at the same "
#~ "time, as they can be the same "
#~ "function of different input shapes."
#~ msgstr ""

#~ msgid ""
#~ "The fallback function when all functions"
#~ " loaded are failed, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "When there is not legal loaded "
#~ "function of the input shape and no"
#~ " fallback callable."
#~ msgstr ""

#~ msgid ""
#~ "A function that tries all loaded "
#~ "function against the input until the "
#~ "first success one."
#~ msgstr ""

#~ msgid ""
#~ "The keras loss function that directly"
#~ " taking the model output as the "
#~ "loss."
#~ msgstr ""

#~ msgid "Save tf function in the file (``tf.savedmodel`` format)."
#~ msgstr ""

#~ msgid "``tf.function`` ed function with graph building"
#~ msgstr ""

#~ msgid "the dir path to save the function"
#~ msgstr ""

#~ msgid "FiniteMPS from tensornetwork with bug fixed"
#~ msgstr ""

#~ msgid ""
#~ "Bases: "
#~ ":py:class:`tensornetwork.matrixproductstates.finite_mps.FiniteMPS`"
#~ msgstr ""

#~ msgid ""
#~ "Apply a two-site gate to an "
#~ "MPS. This routine will in general "
#~ "destroy any canonical form of the "
#~ "state. If a canonical form is "
#~ "needed, the user can restore it "
#~ "using `FiniteMPS.position`."
#~ msgstr ""

#~ msgid "A two-body gate."
#~ msgstr ""

#~ msgid "The first site where the gate acts."
#~ msgstr ""

#~ msgid "The second site where the gate acts."
#~ msgstr ""

#~ msgid "The maximum number of singular values to keep."
#~ msgstr ""

#~ msgid "The maximum allowed truncation error."
#~ msgstr ""

#~ msgid ""
#~ "An optional value to choose the "
#~ "MPS tensor at `center_position` to be"
#~ " isometric after the application of "
#~ "the gate. Defaults to `site1`. If "
#~ "the MPS is canonical "
#~ "(i.e.`BaseMPS.center_position != None`), and "
#~ "if the orthogonality center coincides "
#~ "with either `site1` or `site2`,  the "
#~ "orthogonality center will be shifted to"
#~ " `center_position` (`site1` by default). If"
#~ " the orthogonality center does not "
#~ "coincide with `(site1, site2)` then "
#~ "`MPS.center_position` is set to `None`."
#~ msgstr ""

#~ msgid "Multiply `max_truncation_err` with the largest singular value."
#~ msgstr ""

#~ msgid ""
#~ "\"rank of gate is {} but has "
#~ "to be 4\", \"site1 = {} is "
#~ "not between 0 <= site < N -"
#~ " 1 = {}\", \"site2 = {} is "
#~ "not between 1 <= site < N ="
#~ " {}\",\"Found site2 ={}, site1={}. Only "
#~ "nearest neighbor gates are currently "
#~ "supported\", \"f center_position = "
#~ "{center_position} not  f in {(site1, "
#~ "site2)} \", or \"center_position = {},"
#~ " but gate is applied at sites "
#~ "{}, {}. Truncation should only be "
#~ "done if the gate is applied at "
#~ "the center position of the MPS.\""
#~ msgstr ""

#~ msgid "A scalar tensor containing the truncated weight of the truncation."
#~ msgstr ""

#~ msgid "Measure the expectation value of local operators `ops` site `sites`."
#~ msgstr ""

#~ msgid "A list Tensors of rank 2; the local operators to be measured."
#~ msgstr ""

#~ msgid "Sites where `ops` act."
#~ msgstr ""

#~ msgid "measurements :math:`\\langle` `ops[n]`:math:`\\rangle` for n in `sites`"
#~ msgstr ""

#~ msgid ""
#~ "Compute the correlator :math:`\\langle` "
#~ "`op1[site1], op2[s]`:math:`\\rangle` between `site1`"
#~ " and all sites `s` in `sites2`. "
#~ "If `s == site1`, `op2[s]` will be"
#~ " applied first."
#~ msgstr ""

#~ msgid "Tensor of rank 2; the local operator at `site1`."
#~ msgstr ""

#~ msgid "Tensor of rank 2; the local operator at `sites2`."
#~ msgstr ""

#~ msgid "The site where `op1`  acts"
#~ msgstr ""

#~ msgid "Sites where operator `op2` acts."
#~ msgstr ""

#~ msgid ""
#~ "Correlator :math:`\\langle` `op1[site1], "
#~ "op2[s]`:math:`\\rangle` for `s` :math:`\\in` "
#~ "`sites2`."
#~ msgstr ""

#~ msgid "Quantum circuit: MPS state simulator"
#~ msgstr ""

#~ msgid "``MPSCircuit`` class. Simple usage demo below."
#~ msgstr ""

#~ msgid "MPSCircuit object based on state simulator."
#~ msgstr ""

#~ msgid ""
#~ "If not None, the initial state of"
#~ " the circuit is taken as ``tensors``"
#~ " instead of :math:`\\vert 0\\rangle^n` "
#~ "qubits, defaults to None"
#~ msgstr ""

#~ msgid "The center position of MPS, default to 0"
#~ msgstr ""

#~ msgid "Apply a general qubit gate on MPS."
#~ msgstr ""

#~ msgid "The Gate to be applied"
#~ msgstr ""

#~ msgid "Qubit indices of the gate"
#~ msgstr ""

#~ msgid "\"MPS does not support application of gate on > 2 qubits.\""
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on "
#~ "adjacent qubits of Matrix Product States"
#~ " (MPS). Truncation rule is specified "
#~ "by `set_truncation_rule`."
#~ msgstr ""

#~ msgid "The first qubit index of the gate"
#~ msgstr ""

#~ msgid "The second qubit index of the gate"
#~ msgstr ""

#~ msgid "Center position of MPS, default is None"
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on MPS."
#~ " Truncation rule is specified by "
#~ "`set_truncation_rule`."
#~ msgstr ""

#~ msgid ""
#~ "Apply a single qubit gate on MPS,"
#~ " and the gate must be unitary; "
#~ "no truncation is needed."
#~ msgstr ""

#~ msgid "gate to be applied"
#~ msgstr ""

#~ msgid "Qubit index of the gate"
#~ msgstr ""

#~ msgid "Compute the conjugate of the current MPS."
#~ msgstr ""

#~ msgid "The constructed MPS"
#~ msgstr ""

#~ msgid "Copy the current MPS."
#~ msgstr ""

#~ msgid "Copy the current MPS without the tensors."
#~ msgstr ""

#~ msgid "Compute the expectation of the corresponding double qubit gate."
#~ msgstr ""

#~ msgid "qubit index of the gate"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of the "
#~ "corresponding single qubit gate in the"
#~ " form of tensor."
#~ msgstr ""

#~ msgid "Gate to be applied"
#~ msgstr ""

#~ msgid "The expectation of the corresponding single qubit gate"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of the direct"
#~ " product of the corresponding two "
#~ "gates."
#~ msgstr ""

#~ msgid "First gate to be applied"
#~ msgstr ""

#~ msgid "Second gate to be applied"
#~ msgstr ""

#~ msgid "Qubit index of the first gate"
#~ msgstr ""

#~ msgid "Qubit index of the second gate"
#~ msgstr ""

#~ msgid "The correlation of the corresponding two qubit gates"
#~ msgstr ""

#~ msgid "Construct the MPS from a given wavefunction."
#~ msgstr ""

#~ msgid "The given wavefunction (any shape is OK)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the expectation of corresponding "
#~ "operators in the form of tensor."
#~ msgstr ""

#~ msgid ""
#~ "Operator and its position on the "
#~ "circuit, eg. ``(gates.Z(), [1]), (gates.X(),"
#~ " [2])`` is for operator :math:`Z_1X_2`"
#~ msgstr ""

#~ msgid "The expectation of corresponding operators"
#~ msgstr ""

#~ msgid "Get the normalized Center Position."
#~ msgstr ""

#~ msgid "Normalized Center Position."
#~ msgstr ""

#~ msgid "Check whether the circuit is legal."
#~ msgstr ""

#~ msgid "Whether the circuit is legal."
#~ msgstr ""

#~ msgid "integer indicating the measure on which quantum line"
#~ msgstr ""

#~ msgid ""
#~ "Middle measurement in the z-basis on "
#~ "the circuit, note the wavefunction "
#~ "output is not normalized with "
#~ "``mid_measurement`` involved, one should "
#~ "normalized the state manually if needed."
#~ msgstr ""

#~ msgid "The index of qubit that the Z direction postselection applied on"
#~ msgstr ""

#~ msgid "0 for spin up, 1 for spin down, defaults to 0"
#~ msgstr ""

#~ msgid "Normalize MPS Circuit according to the center position."
#~ msgstr ""

#~ msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
#~ msgstr ""

#~ msgid "The orthogonality center"
#~ msgstr ""

#~ msgid "Compute the projection between `other` as bra and `self` as ket."
#~ msgstr ""

#~ msgid "ket of the other MPS, which will be converted to bra automatically"
#~ msgstr ""

#~ msgid "The projection in form of tensor"
#~ msgstr ""

#~ msgid ""
#~ "Set truncation rules when double qubit"
#~ " gates are applied. If nothing is "
#~ "specified, no truncation will take place"
#~ " and the bond dimension will keep "
#~ "growing. For more details, refer to "
#~ "`split_tensor`."
#~ msgstr ""

#~ msgid "Tensor with shape [1, -1]"
#~ msgstr ""

#~ msgid ""
#~ "Split the tensor by SVD or QR "
#~ "depends on whether a truncation is "
#~ "required."
#~ msgstr ""

#~ msgid "The input tensor to split."
#~ msgstr ""

#~ msgid ""
#~ "Determine the orthogonal center is on"
#~ " the left tensor or the right "
#~ "tensor."
#~ msgstr ""

#~ msgid "Two tensors after splitting"
#~ msgstr ""

#~ msgid "Quantum state and operator class backend by tensornetwork"
#~ msgstr ""

#~ msgid "Bases: :py:class:`tensorcircuit.quantum.QuOperator`"
#~ msgstr ""

#~ msgid "Represents an adjoint (row) vector via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuAdjointVector` from "
#~ "a tensor network. This encapsulates an"
#~ " existing tensor network, interpreting it"
#~ " as an adjoint vector (row vector)."
#~ msgstr ""

#~ msgid "The edges of the network to be used as the input edges."
#~ msgstr ""

#~ msgid ""
#~ "Nodes used to refer to parts of"
#~ " the tensor network that are not "
#~ "connected to any input or output "
#~ "edges (for example: a scalar factor)."
#~ msgstr ""

#~ msgid ""
#~ "Optional collection of edges to ignore"
#~ " when performing consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuAdjointVector` directly from"
#~ " a single tensor. This first wraps"
#~ " the tensor in a `Node`, then "
#~ "constructs the `QuAdjointVector` from that "
#~ "`Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing an QuAdjointVector."
#~ msgstr ""

#~ msgid ""
#~ "Sequence of integer indices specifying "
#~ "the order in which to interpret "
#~ "the axes as subsystems (input edges)."
#~ " If not specified, the axes are "
#~ "taken in ascending order."
#~ msgstr ""

#~ msgid "The new constructed QuAdjointVector give from the given tensor."
#~ msgstr ""

#~ msgid ""
#~ "Represents a linear operator via a "
#~ "tensor network. To interpret a tensor"
#~ " network as a linear operator, some"
#~ " of the dangling edges must be "
#~ "designated as `out_edges` (output edges) "
#~ "and the rest as `in_edges` (input "
#~ "edges). Considered as a matrix, the "
#~ "`out_edges` represent the row index and"
#~ " the `in_edges` represent the column "
#~ "index. The (right) action of the "
#~ "operator on another then consists of "
#~ "connecting the `in_edges` of the first"
#~ " operator to the `out_edges` of the"
#~ " second. Can be used to do "
#~ "simple linear algebra with tensor "
#~ "networks."
#~ msgstr ""

#~ msgid ""
#~ "Creates a new `QuOperator` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a linear operator. The network "
#~ "is checked for consistency: All dangling"
#~ " edges must either be in `out_edges`,"
#~ " `in_edges`, or `ignore_edges`."
#~ msgstr ""

#~ msgid "The edges of the network to be used as the output edges."
#~ msgstr ""

#~ msgid ""
#~ "Optional collection of dangling edges to"
#~ " ignore when performing consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "At least one reference node is "
#~ "required to specify a scalar. None "
#~ "provided!"
#~ msgstr ""

#~ msgid ""
#~ "The adjoint of the operator. This "
#~ "creates a new `QuOperator` with "
#~ "complex-conjugate copies of all tensors "
#~ "in the network and with the input"
#~ " and output edges switched."
#~ msgstr ""

#~ msgid ""
#~ "Check that the network has the "
#~ "expected dimensionality. This checks that "
#~ "all input and output edges are "
#~ "dangling and that there are no "
#~ "other dangling edges (except any "
#~ "specified in `ignore_edges`). If not, an"
#~ " exception is raised."
#~ msgstr ""

#~ msgid ""
#~ "Contract the tensor network in place."
#~ " This modifies the tensor network "
#~ "representation of the operator (or "
#~ "vector, or scalar), reducing it to "
#~ "a single tensor, without changing the"
#~ " value."
#~ msgstr ""

#~ msgid "Manually specify the axis ordering of the final tensor."
#~ msgstr ""

#~ msgid "The present object."
#~ msgstr ""

#~ msgid ""
#~ "Contracts the tensor network in place"
#~ " and returns the final tensor. Note"
#~ " that this modifies the tensor "
#~ "network representing the operator. The "
#~ "default ordering for the axes of "
#~ "the final tensor is: `*out_edges, "
#~ "*in_edges`. If there are any \"ignored\""
#~ " edges, their axes come first: "
#~ "`*ignored_edges, *out_edges, *in_edges`."
#~ msgstr ""

#~ msgid ""
#~ "Manually specify the axis ordering of"
#~ " the final tensor. The default "
#~ "ordering is determined by `out_edges` "
#~ "and `in_edges` (see above)."
#~ msgstr ""

#~ msgid "Node count '{}' > 1 after contraction!"
#~ msgstr ""

#~ msgid "The final tensor representing the operator."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuOperator` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuOperator` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor."
#~ msgstr ""

#~ msgid "The axis indices of `tensor` to use as `out_edges`."
#~ msgstr ""

#~ msgid "The axis indices of `tensor` to use as `in_edges`."
#~ msgstr ""

#~ msgid "The new operator."
#~ msgstr ""

#~ msgid "All tensor-network nodes involved in the operator."
#~ msgstr ""

#~ msgid ""
#~ "The norm of the operator. This is"
#~ " the 2-norm (also known as the "
#~ "Frobenius or Hilbert-Schmidt norm)."
#~ msgstr ""

#~ msgid ""
#~ "The partial trace of the operator. "
#~ "Subsystems to trace out are supplied "
#~ "as indices, so that dangling edges "
#~ "are connected to each other as: "
#~ "`out_edges[i] ^ in_edges[i] for i in "
#~ "subsystems_to_trace_out` This does not modify"
#~ " the original network. The original "
#~ "ordering of the remaining subsystems is"
#~ " maintained."
#~ msgstr ""

#~ msgid "Indices of subsystems to trace out."
#~ msgstr ""

#~ msgid "A new QuOperator or QuScalar representing the result."
#~ msgstr ""

#~ msgid ""
#~ "Tensor product with another operator. "
#~ "Given two operators `A` and `B`, "
#~ "produces a new operator `AB` "
#~ "representing `A`  `B`. The `out_edges`"
#~ " (`in_edges`) of `AB` is simply the"
#~ " concatenation of the `out_edges` "
#~ "(`in_edges`) of `A.copy()` with that of"
#~ " `B.copy()`: `new_out_edges = [*out_edges_A_copy,"
#~ " *out_edges_B_copy]` `new_in_edges = "
#~ "[*in_edges_A_copy, *in_edges_B_copy]`"
#~ msgstr ""

#~ msgid "The other operator (`B`)."
#~ msgstr ""

#~ msgid "The result (`AB`)."
#~ msgstr ""

#~ msgid "The trace of the operator."
#~ msgstr ""

#~ msgid "Represents a scalar via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuScalar` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a scalar."
#~ msgstr ""

#~ msgid ""
#~ "Nodes used to refer to the tensor"
#~ " network (need not be exhaustive -"
#~ " one node from each disconnected "
#~ "subnetwork is sufficient)."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuScalar` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuScalar` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing a new QuScalar."
#~ msgstr ""

#~ msgid "The new constructed QuScalar from the given tensor."
#~ msgstr ""

#~ msgid "Represents a (column) vector via a tensor network."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuVector` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a (column) vector."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuVector` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuVector` from that `Node`."
#~ msgstr ""

#~ msgid "The tensor for constructing a \"QuVector\"."
#~ msgstr ""

#~ msgid ""
#~ "Sequence of integer indices specifying "
#~ "the order in which to interpret "
#~ "the axes as subsystems (output edges)."
#~ " If not specified, the axes are "
#~ "taken in ascending order."
#~ msgstr ""

#~ msgid "The new constructed QuVector from the given tensor."
#~ msgstr ""

#~ msgid ""
#~ "Check the vector spaces represented by"
#~ " two lists of edges are compatible."
#~ " The number of edges must be "
#~ "the same and the dimensions of "
#~ "each pair of edges must match. "
#~ "Otherwise, an exception is raised. "
#~ ":param edges_1: List of edges "
#~ "representing a many-body Hilbert space."
#~ " :type edges_1: Sequence[Edge] :param "
#~ "edges_2: List of edges representing a"
#~ " many-body Hilbert space. :type "
#~ "edges_2: Sequence[Edge]"
#~ msgstr ""

#~ msgid ""
#~ "Hilbert-space mismatch: \"Cannot connect "
#~ "{} subsystems with {} subsystems\", or"
#~ " \"Input dimension {} != output "
#~ "dimension {}.\""
#~ msgstr ""

#~ msgid ""
#~ "Eliminates any connected CopyNodes that "
#~ "are identity matrices. This will modify"
#~ " the network represented by `nodes`. "
#~ "Only identities that are connected to"
#~ " other nodes are eliminated."
#~ msgstr ""

#~ msgid "Collection of nodes to search."
#~ msgstr ""

#~ msgid ""
#~ "The Dictionary mapping remaining Nodes "
#~ "to any replacements, Dictionary specifying "
#~ "all dangling-edge replacements."
#~ msgstr ""

#~ msgid "Compute the entropy from the given density matrix ``rho``."
#~ msgstr ""

#~ msgid "[description], defaults to 1e-12"
#~ msgstr ""

#~ msgid ""
#~ "Note: further jit is recommended. For"
#~ " large Hilbert space, sparse Hamiltonian"
#~ " is recommended"
#~ msgstr ""

#~ msgid ""
#~ "Construct a 'QuOperator' representing the "
#~ "identity on a given space. Internally,"
#~ " this is done by constructing "
#~ "'CopyNode's for each edge, with "
#~ "dimension according to 'space'."
#~ msgstr ""

#~ msgid ""
#~ "A sequence of integers for the "
#~ "dimensions of the tensor product factors"
#~ " of the space (the edges in the"
#~ " tensor network)."
#~ msgstr ""

#~ msgid "The data type (for conversion to dense)."
#~ msgstr ""

#~ msgid "The desired identity operator."
#~ msgstr ""

#~ msgid ""
#~ "Simulate the measuring of each qubit "
#~ "of ``p`` in the computational basis, "
#~ "thus producing output like that of "
#~ "``qiskit``."
#~ msgstr ""

#~ msgid ""
#~ "The quantum state, assumed to be "
#~ "normalized, as either a ket or "
#~ "density operator."
#~ msgstr ""

#~ msgid "The number of counts to perform."
#~ msgstr ""

#~ msgid ""
#~ "Defaults True. The bool indicating "
#~ "whether the return form is in the"
#~ " form of two array or one of"
#~ " the same length as the ``state`` "
#~ "(if ``sparse=False``)."
#~ msgstr ""

#~ msgid "The counts for each bit string measured."
#~ msgstr ""

#~ msgid ""
#~ "Constructs an appropriately specialized "
#~ "QuOperator. If there are no edges, "
#~ "creates a QuScalar. If the are "
#~ "only output (input) edges, creates a "
#~ "QuVector (QuAdjointVector). Otherwise creates "
#~ "a QuOperator."
#~ msgstr ""

#~ msgid ""
#~ "op = qu.quantum_constructor([], [psi_node[0], "
#~ "psi_node[1]]) >>> show_attributes(op) op.is_scalar()"
#~ "          -> False op.is_vector()          -> "
#~ "False op.is_adjoint_vector()  -> True "
#~ "len(op.out_edges)       -> 0 len(op.in_edges)"
#~ "        -> 2 >>> # psi_node[0] -> "
#~ "op.in_edges[0] >>> # psi_node[1] -> "
#~ "op.in_edges[1]"
#~ msgstr ""

#~ msgid "output edges."
#~ msgstr ""

#~ msgid "in edges."
#~ msgstr ""

#~ msgid ""
#~ "reference nodes for the tensor network"
#~ " (needed if there is a scalar "
#~ "component)."
#~ msgstr ""

#~ msgid "edges to ignore when checking the dimensionality of the tensor network."
#~ msgstr ""

#~ msgid "The new created QuOperator object."
#~ msgstr ""

#~ msgid "Compute the reduced density matrix from the quantum state ``state``."
#~ msgstr ""

#~ msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
#~ msgstr ""

#~ msgid "\\mathrm{Tr}(\\prod_i O_i)"
#~ msgstr ""

#~ msgid "the trace of several inputs"
#~ msgstr ""

#~ msgid "Tensornetwork Simplification"
#~ msgstr ""

#~ msgid ""
#~ "Get the new shape of two nodes,"
#~ " also supporting to return original "
#~ "shapes of two nodes."
#~ msgstr ""

#~ msgid "node one"
#~ msgstr ""

#~ msgid "node two"
#~ msgstr ""

#~ msgid "Whether to include original shape of two nodes, default is True."
#~ msgstr ""

#~ msgid "The new shape of the two nodes."
#~ msgstr ""

#~ msgid ""
#~ "Contract between Node ``a`` and ``b``,"
#~ " with correct shape only and no "
#~ "calculation"
#~ msgstr ""

#~ msgid "Shortcuts for measurement patterns on circuit"
#~ msgstr ""

#~ msgid "Some common graphs and lattices"
#~ msgstr ""

#~ msgid "1D chain with ``n`` sites"
#~ msgstr ""

#~ msgid ""
#~ "This measurements pattern is specifically "
#~ "suitable for vmap. Parameterize the "
#~ "Pauli string to be measured."
#~ msgstr ""

#~ msgid ""
#~ "parameter tensors determines what Pauli "
#~ "string to be measured, shape is "
#~ "[nwires, 4] if onehot is False."
#~ msgstr ""

#~ msgid ""
#~ "[description], defaults to False. If set"
#~ " to be True, structures will first"
#~ " go through onehot procedure."
#~ msgstr ""

#~ msgid "COO_sparse_matrix"
#~ msgstr ""

#~ msgid "a real and scalar tensor of shape []"
#~ msgstr ""

#~ msgid "Helper functions"
#~ msgstr ""

#~ msgid ""
#~ "Return a callable function for output"
#~ " ith parts of the original output "
#~ "along the first axis. Original output"
#~ " supports List and Tensor."
#~ msgstr ""

#~ msgid "The function to be applied this method"
#~ msgstr ""

#~ msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
#~ msgstr ""

#~ msgid "The modified callable function"
#~ msgstr ""

#~ msgid "Visualization on circuits"
#~ msgstr ""

#~ msgid "# TODO(@YHPeter): add examples"
#~ msgstr ""

#~ msgid ""
#~ "Generate the PDF file with given "
#~ "latex string and filename. Latex command"
#~ " and file path can be specified. "
#~ "When notebook is True, convert the "
#~ "output PDF file to image and "
#~ "return a Image object."
#~ msgstr ""

#~ msgid "String of latex content"
#~ msgstr ""

#~ msgid "File name, defaults to random UUID `str(uuid4())`"
#~ msgstr ""

#~ msgid "Executable Latex command, defaults to `pdflatex`"
#~ msgstr ""

#~ msgid "File path, defaults to current working place `os.getcwd()`"
#~ msgstr ""

#~ msgid "if notebook is True, return `Image` object; otherwise return `None`"
#~ msgstr ""

#~ msgid "_summary_"
#~ msgstr ""

#~ msgid "_description_"
#~ msgstr ""

#~ msgid "[description], default is None."
#~ msgstr ""

#~ msgid "_description_, default is (1, -1)."
#~ msgstr ""

#~ msgid "Apply fredkin gate on the circuit."
#~ msgstr ""

#~ msgid "Apply orx gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ory gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply orz gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply ox gate on the circuit."
#~ msgstr ""

#~ msgid "Apply oy gate on the circuit."
#~ msgstr ""

#~ msgid "Apply oz gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Random tensor between 0 or 1, "
#~ "defaults to be None, the random "
#~ "number will be generated automatically"
#~ msgstr ""

#~ msgid "The str indicating the form of the output wavefunction."
#~ msgstr ""

#~ msgid ""
#~ "A collection of useful function snippets"
#~ " that irrelevant with the main "
#~ "modules or await for further refactor"
#~ msgstr ""

#~ msgid "VQNHE application"
#~ msgstr ""

#~ msgid "Apply **ANY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CR** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **CRZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **EXP** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **EXP1** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **FREDKIN** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
#~ "    0.+0.j & 0.+0.j & 0.+0.j & "
#~ "1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 0.+0.j & 1.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j &"
#~ " 0.+0.j & 0.+0.j\\\\    0.+0.j & "
#~ "0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j"
#~ " & 0.+0.j & 0.+0.j & 1.+0.j "
#~ "\\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **ISWAP** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **ORZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **OX** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "1.+0.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **OY** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    0.+0.j "
#~ "& 0.-1.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+1.j & 0.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **OZ** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Qubit number than the gate applies "
#~ "on. The matrix for the gate is"
#~ "  .. math::        \\begin{bmatrix}    1.+0.j "
#~ "& 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
#~ "0.+0.j & -1.+0.j & 0.+0.j & "
#~ "0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j"
#~ " & 0.+0.j\\\\    0.+0.j & 0.+0.j &"
#~ " 0.+0.j & 1.+0.j \\end{bmatrix}"
#~ msgstr ""

#~ msgid "Apply **R** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RX** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RY** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **RZ** gate with parameters on the circuit."
#~ msgstr ""

#~ msgid "Apply **SD** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **TD** gate on the circuit."
#~ msgstr ""

#~ msgid "Apply **TOFFOLI** gate on the circuit."
#~ msgstr ""

#~ msgid ""
#~ "Apply unitary gates in ``kraus`` "
#~ "randomly based on corresponding ``prob``."
#~ msgstr ""


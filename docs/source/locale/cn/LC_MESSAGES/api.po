# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-15 10:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/api/applications.rst:2
msgid "tensorcircuit.applications"
msgstr ""

#: ../../source/api/applications/dqas.rst:2
msgid "tensorcircuit.applications.dqas"
msgstr ""

#: of tensorcircuit.applications.dqas:1
msgid "Modules for DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:1
msgid "DQAS framework entrypoint"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.concat
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.single_qubit_kraus_identity_check
#: tensorcircuit.circuit.Circuit.__init__
#: tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.general_kraus
#: tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.mid_measurement
#: tensorcircuit.circuit.Circuit.replace_inputs
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.rgate_theoretical
#: tensorcircuit.gates.rx_gate tensorcircuit.gates.ry_gate
#: tensorcircuit.gates.rz_gate tensorcircuit.keras.QuantumLayer.__init__
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.keras.save_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.__init__
#: tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.mid_measurement
#: tensorcircuit.mpscircuit.MPSCircuit.position
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.__init__
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.__init__
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.__init__
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Parameters"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:3
msgid ""
"function with input of data instance, circuit parameters theta and "
"structural paramter k, return tuple of objective value and gradient with "
"respect to theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:5
msgid "data generator as dataset"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:6
msgid "list of operations as primitive operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:7
msgid "the default layer number of the circuit ansatz"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:8
msgid ""
"shape of circuit parameter pool, in general p_stp*l, where l is the max "
"number of circuit parameters for op in the operator pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:10
msgid "the same as p in the most times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:11
msgid "batch size of one epoch"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:12
msgid "prethermal update times"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:13
msgid "training epochs"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:14
msgid "parallel thread number, 0 to disable multiprocessing model by default"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:15
msgid "set verbose log to print"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:16
msgid "function to output verbose information"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:17
msgid "function return intermiediate result for final history list"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:18
msgid "cutoff probability to avoid peak distribution"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:19
msgid ""
"function accepting list of objective values and return the baseline value"
" used in the next round"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:21
msgid "return noise with the same shape as circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:22
msgid "initial values for circuit parameter pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:23
msgid "initial values for probabilistic model parameters"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:24
msgid "optimizer for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:25
msgid "optimizer for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:26
msgid "optimizer for circuit parameters in prethermal stage"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:27
msgid "fixed structural parameters for prethermal training"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:28
msgid "regularization function for model parameters alpha"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search:29
msgid "regularization function for circuit parameters theta"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search
#: tensorcircuit.applications.dqas.DQAS_search_pmb
#: tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.dqas.get_weights
#: tensorcircuit.applications.dqas.parallel_kernel
#: tensorcircuit.applications.dqas.parallel_qaoa_train
#: tensorcircuit.applications.dqas.verbose_output
#: tensorcircuit.applications.graphdata.dict2graph
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduce_edges
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.layers.generate_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer
#: tensorcircuit.applications.layers.generate_cirq_gate_layer
#: tensorcircuit.applications.layers.generate_gate_layer
#: tensorcircuit.applications.utils.color_svg
#: tensorcircuit.applications.utils.repr2array
#: tensorcircuit.applications.vags.ave_func
#: tensorcircuit.applications.vags.cvar tensorcircuit.applications.vags.energy
#: tensorcircuit.applications.vags.evaluate_vag
#: tensorcircuit.applications.vags.gapfilling
#: tensorcircuit.applications.vags.heisenberg_measurements
#: tensorcircuit.applications.vags.q
#: tensorcircuit.applications.vags.qaoa_block_vag
#: tensorcircuit.applications.vags.qaoa_block_vag_energy
#: tensorcircuit.applications.vags.qaoa_train
#: tensorcircuit.applications.vags.quantum_mp_qaoa_vag
#: tensorcircuit.applications.vags.quantum_qaoa_vag
#: tensorcircuit.applications.vags.tfim_measurements
#: tensorcircuit.applications.vags.unitary_design
#: tensorcircuit.applications.vags.unitary_design_block
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.plain_contractor
#: tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.any_gate
#: tensorcircuit.gates.bmatrix tensorcircuit.gates.cr_gate
#: tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Returns"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:1
msgid ""
"The probabilistic model based DQAS, can use extensively for DQAS case for"
" ``NMF`` probabilistic model."
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:3
msgid "vag func, return loss and nabla lnp"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:4
msgid "keras model"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:5
msgid "sample func of logic with keras model input"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:6
msgid "input data pipeline generator"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:7
msgid "operation pool"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:8
msgid "depth for DQAS"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:12
msgid "parallel kernels"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:23
msgid "final loss function in terms of average of sub loss for each circuit"
msgstr ""

#: of tensorcircuit.applications.dqas.DQAS_search_pmb:24
msgid "derivative function for ``loss_func``"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:1
msgid ""
"Call in customized functions and grab variables within DQAS framework "
"function by var name str."
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:3
msgid "The DQAS framework function"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var:5
msgid "Variables within the DQAS framework"
msgstr ""

#: of tensorcircuit.applications.dqas.get_var
#: tensorcircuit.applications.graphdata.graph1D
#: tensorcircuit.applications.graphdata.reduced_ansatz
#: tensorcircuit.applications.graphdata.split_ansatz
#: tensorcircuit.applications.vqes.VQNHE.evaluation
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation
#: tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.plain_contractor
#: tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.any_gate
#: tensorcircuit.gates.bmatrix tensorcircuit.gates.cr_gate
#: tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mps_base.FiniteMPS.measure_local_operator
#: tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Return type"
msgstr ""

#: of tensorcircuit.applications.dqas.get_weights:1
msgid ""
"This function works only when nnp has the same shape as stp, i.e. one "
"parameter for each op."
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_kernel:1
msgid "The kernel for multiprocess to run parallel in DQAS function/"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:1
msgid ""
"parallel variational parameter training and search to avoid local minimum"
" not limited to qaoa setup as the function name indicates, as long as you"
" provided suitable `vag_func`"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:6
msgid "data input generator for vag_func"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:7
msgid "vag_kernel"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:10
msgid "number of tries"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:11
msgid "for optimization problem the input is in general fixed so batch is often 1"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:12
msgid "number of parallel jobs"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:13
msgid "mean value of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.parallel_qaoa_train:14
msgid "std deviation of normal distribution for nnp"
msgstr ""

#: of tensorcircuit.applications.dqas.verbose_output:1
msgid "Doesn't support prob model DQAS search."
msgstr ""

#: ../../source/api/applications/graphdata.rst:2
msgid "tensorcircuit.applications.graphdata"
msgstr ""

#: of tensorcircuit.applications.graphdata:1
msgid "Modules for graph instance data and more"
msgstr ""

#: of tensorcircuit.applications.graphdata.dict2graph:1
msgid "```python d = nx.to_dict_of_dicts(g) ```"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:1
msgid "1D PBC chain with n sites."
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:3
msgid "The number of nodes"
msgstr ""

#: of tensorcircuit.applications.graphdata.graph1D:5
msgid "The resulted graph g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduce_edges:3
msgid "all graphs with m edge out from g"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:1
msgid ""
"Generate a reduced graph with given ratio of edges compared to the "
"original graph g."
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:3
msgid "The base graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:5
msgid "number of edges kept, default half of the edges"
msgstr ""

#: of tensorcircuit.applications.graphdata.reduced_ansatz:6
msgid "The resulted reduced graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:1
msgid "Split the graph in exactly ``split`` piece evenly."
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:3
msgid "The mother graph"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:5
msgid "The number of the graph we want to divide into, defaults to 2"
msgstr ""

#: of tensorcircuit.applications.graphdata.split_ansatz:7
msgid "List of graph instance of size ``split``"
msgstr ""

#: ../../source/api/applications/layers.rst:2
msgid "tensorcircuit.applications.layers"
msgstr ""

#: of tensorcircuit.applications.layers:1
msgid "Module for functions adding layers of circuits"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "Hlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_gate_layer.<locals>.f:1
msgid "anyrzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyswaplayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyxzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyyzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzxlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzylayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer.<locals>.f:1
msgid "anyzzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "cnotlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_gate_layer.<locals>.f:1
msgid "rzlayer"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "swaplayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "xzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "xzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "yzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "yzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zxgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zxlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zygate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zylayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_double_gate.<locals>.f:1
msgid "zzgate"
msgstr ""

#: of
#: tensorcircuit.applications.layers.generate_cirq_double_gate_layer.<locals>.f:1
msgid "zzlayer"
msgstr ""

#: of tensorcircuit.applications.layers.generate_any_gate_layer:1
msgid "$$e^{-i     heta_i \\sigma}$$"
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_double_gate_layer:1
msgid ""
"The following function should be used to generate layers with special "
"case. As its soundness depends on the nature of the task or problem, it "
"doesn't always make sense."
msgstr ""

#: of tensorcircuit.applications.layers.generate_cirq_any_gate_layer:1
#: tensorcircuit.applications.layers.generate_cirq_gate_layer:1
msgid "$$e^{-i heta \\sigma}$$"
msgstr ""

#: of tensorcircuit.applications.layers.generate_gate_layer:1
msgid "$$e^{-i     heta \\sigma}$$"
msgstr ""

#: ../../source/api/applications/utils.rst:2
msgid "tensorcircuit.applications.utils"
msgstr ""

#: of tensorcircuit.applications.utils:1
msgid ""
"A collection of useful function snippets that irrelevant with the main "
"modules or await for furthere refactor"
msgstr ""

#: of tensorcircuit.applications.utils.FakeModule:1
#: tensorcircuit.applications.vqes.VQNHE:1
#: tensorcircuit.backends.jax_backend.optax_optimizer:1
#: tensorcircuit.backends.tensorflow_backend.keras_optimizer:1
#: tensorcircuit.circuit.Circuit:1 tensorcircuit.densitymatrix.DMCircuit:1
#: tensorcircuit.gates.GateF:1 tensorcircuit.mpscircuit.MPSCircuit:1
#: tensorcircuit.quantum.QuOperator:1
#: tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Bases: :py:class:`object`"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:1
msgid "color cirq circuit SVG for given gates, a small tool to hack the cirq SVG"
msgstr ""

#: of tensorcircuit.applications.utils.color_svg:5
msgid "integer coordinate which gate is colored"
msgstr ""

#: of tensorcircuit.applications.utils.repr2array:1
msgid "transform repr form of an array to real numpy array"
msgstr ""

#: ../../source/api/applications/vags.rst:2
msgid "tensorcircuit.applications.vags"
msgstr ""

#: of tensorcircuit.applications.vags:1
msgid "DQAS application kernels as vag functions"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:1
msgid "1D array for full wavefunction, the basis is in lexcical order"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:2
#: tensorcircuit.applications.vags.energy:5
msgid "nx.Graph"
msgstr ""

#: of tensorcircuit.applications.vags.ave_func:3
msgid "transformation functions before averaged"
msgstr ""

#: of tensorcircuit.applications.vags.cvar:1
msgid "as f3"
msgstr ""

#: of tensorcircuit.applications.vags.energy:1
msgid "maxcut energy for n qubit wavefunction i-th basis"
msgstr ""

#: of tensorcircuit.applications.vags.energy:3
msgid "ranged from 0 to 2**n-1"
msgstr ""

#: of tensorcircuit.applications.vags.energy:4
#: tensorcircuit.applications.vags.unitary_design:4
msgid "number of qubits"
msgstr ""

#: of tensorcircuit.applications.vags.entanglement_entropy:1
msgid ""
"deprecated as non tf and non flexible, use the combination of "
"``reduced_density_matrix`` and ``entropy`` instead."
msgstr ""

#: of tensorcircuit.applications.vags.entropy:1
#: tensorcircuit.applications.vags.reduced_density_matrix:1
msgid "deprecated, current version in tc.quantum"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:1
msgid ""
"value and gradient, currently only tensorflow backend is supported jax "
"and numpy seems to be slow in circuit simulation anyhow. *deprecated*"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:8
msgid "if lbd=0, take energy as objective"
msgstr ""

#: of tensorcircuit.applications.vags.evaluate_vag:9
msgid "if as default 0, overlap will not compute in the process"
msgstr ""

#: of tensorcircuit.applications.vags.gapfilling:1
msgid "Fill single qubit gates according to placeholder on circuit"
msgstr ""

#: of tensorcircuit.applications.vags.heisenberg_measurements:1
msgid "Hamiltonian measurements for Heisenberg model on graph lattice g"
msgstr ""

#: of tensorcircuit.applications.vags.q:1
msgid "short cut for ``cirq.LineQubit(i)``"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_block_vag:1
#: tensorcircuit.applications.vags.qaoa_block_vag_energy:1
msgid "QAOA block encoding kernel, support 2 params in one op"
msgstr ""

#: of tensorcircuit.applications.vags.qaoa_train:1
msgid ""
"training QAOA with only optimizing circuit parameters, can be well "
"replaced with more general function `DQAS_search`"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:1
msgid "multi parameter for one layer"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:7
#: tensorcircuit.applications.vags.quantum_qaoa_vag:7
msgid "kw arguments for measurements_func"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_mp_qaoa_vag:8
msgid "loss function, gradient of nnp"
msgstr ""

#: of tensorcircuit.applications.vags.quantum_qaoa_vag:1
msgid ""
"tensorflow quantum backend compare to qaoa_vag which is tensorcircuit "
"backend"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:1
msgid "Hamiltonian for tfim on lattice defined by graph g"
msgstr ""

#: of tensorcircuit.applications.vags.tfim_measurements:8
msgid "cirq.PauliSum as operators for tfq expectation layer"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:1
msgid ""
"generate random wavefunction from approximately Haar measure, reference:"
"  https://doi.org/10.1063/1.4983266"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design:5
msgid "repetition of the blocks"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:1
msgid "random Haar measure approximation"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:3
msgid "cirq.Circuit, empty circuit"
msgstr ""

#: of tensorcircuit.applications.vags.unitary_design_block:4
msgid "# of qubit"
msgstr ""

#: ../../source/api/applications/van.rst:2
msgid "tensorcircuit.applications.van"
msgstr ""

#: of tensorcircuit.applications.van:1
msgid ""
"One-hot variational autoregressive models for multiple categorical "
"choices beyond binary"
msgstr ""

#: of tensorcircuit.applications.van.MADE:1
#: tensorcircuit.applications.van.NMF:1
#: tensorcircuit.applications.van.PixelCNN:1
msgid "Bases: :py:class:`keras.engine.training.Model`"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:1
#: tensorcircuit.applications.van.NMF.call:1
#: tensorcircuit.applications.van.PixelCNN.call:1
msgid "Calls the model on new inputs and returns the outputs as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:3
#: tensorcircuit.applications.van.NMF.call:3
#: tensorcircuit.applications.van.PixelCNN.call:3
msgid ""
"In this case `call()` just reapplies all ops in the graph to the new "
"inputs (e.g. build a new computational graph from the provided inputs)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:7
#: tensorcircuit.applications.van.NMF.call:7
#: tensorcircuit.applications.van.PixelCNN.call:7
msgid ""
"Note: This method should not be called directly. It is only meant to be "
"overridden when subclassing `tf.keras.Model`. To call a model on an "
"input, always use the `__call__()` method, i.e. `model(inputs)`, which "
"relies on the underlying `call()` method."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.applications.van.MaskedConv2D.call:37
#: tensorcircuit.applications.van.MaskedLinear.call:37
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
#: tensorcircuit.applications.van.ResidualBlock.call:37
#: tensorcircuit.applications.vqes.Linear.call:37
#: tensorcircuit.backends.jax_backend.JaxBackend.eye:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:8
#: tensorcircuit.keras.QuantumLayer.build:11
msgid "Args:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:13
#: tensorcircuit.applications.van.NMF.call:13
#: tensorcircuit.applications.van.PixelCNN.call:13
msgid ""
"inputs: Input tensor, or dict/list/tuple of input tensors. training: "
"Boolean or boolean scalar tensor, indicating whether to run"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:15
#: tensorcircuit.applications.van.NMF.call:15
#: tensorcircuit.applications.van.PixelCNN.call:15
msgid "the `Network` in training mode or inference mode."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "mask: A mask or list of masks. A mask can be either a boolean tensor or"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "None (no mask). For more details, check the guide"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:18
#: tensorcircuit.applications.van.NMF.call:18
#: tensorcircuit.applications.van.PixelCNN.call:18
msgid "[here](https://www.tensorflow.org/guide/keras/masking_and_padding)."
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:21
#: tensorcircuit.applications.van.MaskedConv2D.call:39
#: tensorcircuit.applications.van.MaskedLinear.call:39
#: tensorcircuit.applications.van.NMF.call:21
#: tensorcircuit.applications.van.PixelCNN.call:21
#: tensorcircuit.applications.van.ResidualBlock.call:39
#: tensorcircuit.applications.vqes.Linear.call:39
#: tensorcircuit.backends.jax_backend.JaxBackend.abs:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:4
msgid "Returns:"
msgstr ""

#: of tensorcircuit.applications.van.MADE.call:21
#: tensorcircuit.applications.van.NMF.call:21
#: tensorcircuit.applications.van.PixelCNN.call:21
msgid ""
"A tensor if there is a single output, or a list of tensors if there are "
"more than one outputs."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D:1
#: tensorcircuit.applications.van.MaskedLinear:1
#: tensorcircuit.applications.van.ResidualBlock:1
#: tensorcircuit.applications.vqes.Linear:1 tensorcircuit.keras.QuantumLayer:1
msgid "Bases: :py:class:`keras.engine.base_layer.Layer`"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:1
#: tensorcircuit.keras.QuantumLayer.build:1
msgid "Creates the variables of the layer (optional, for subclass implementers)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:3
#: tensorcircuit.keras.QuantumLayer.build:3
msgid ""
"This is a method that implementers of subclasses of `Layer` or `Model` "
"can override if they need a state-creation step in-between layer "
"instantiation and layer call."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:7
#: tensorcircuit.keras.QuantumLayer.build:7
msgid "This is typically used to create the weights of `Layer` subclasses."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.keras.QuantumLayer.build:11
msgid "input_shape: Instance of `TensorShape`, or list of instances of"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.build:11
#: tensorcircuit.keras.QuantumLayer.build:11
msgid ""
"`TensorShape` if the layer expects a list of inputs (one instance per "
"input)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:1
#: tensorcircuit.applications.van.MaskedLinear.call:1
#: tensorcircuit.applications.van.ResidualBlock.call:1
#: tensorcircuit.applications.vqes.Linear.call:1
msgid "This is where the layer's logic lives."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:3
#: tensorcircuit.applications.van.MaskedLinear.call:3
#: tensorcircuit.applications.van.ResidualBlock.call:3
#: tensorcircuit.applications.vqes.Linear.call:3
msgid ""
"Note here that `call()` method in `tf.keras` is little bit different from"
" `keras` API. In `keras` API, you can pass support masking for layers as "
"additional arguments. Whereas `tf.keras` has `compute_mask()` method to "
"support masking."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:24
#: tensorcircuit.applications.van.MaskedLinear.call:24
#: tensorcircuit.applications.van.ResidualBlock.call:24
#: tensorcircuit.applications.vqes.Linear.call:24
msgid "inputs: Input tensor, or dict/list/tuple of input tensors."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:10
#: tensorcircuit.applications.van.MaskedLinear.call:10
#: tensorcircuit.applications.van.ResidualBlock.call:10
#: tensorcircuit.applications.vqes.Linear.call:10
msgid ""
"The first positional `inputs` argument is subject to special rules: - "
"`inputs` must be explicitly passed. A layer cannot have zero"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:12
#: tensorcircuit.applications.van.MaskedLinear.call:12
#: tensorcircuit.applications.van.ResidualBlock.call:12
#: tensorcircuit.applications.vqes.Linear.call:12
msgid ""
"arguments, and `inputs` cannot be provided via the default value of a "
"keyword argument."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:14
#: tensorcircuit.applications.van.MaskedLinear.call:14
#: tensorcircuit.applications.van.ResidualBlock.call:14
#: tensorcircuit.applications.vqes.Linear.call:14
msgid "NumPy array or Python scalar values in `inputs` get cast as tensors."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:15
#: tensorcircuit.applications.van.MaskedLinear.call:15
#: tensorcircuit.applications.van.ResidualBlock.call:15
#: tensorcircuit.applications.vqes.Linear.call:15
msgid "Keras mask metadata is only collected from `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:16
#: tensorcircuit.applications.van.MaskedLinear.call:16
#: tensorcircuit.applications.van.ResidualBlock.call:16
#: tensorcircuit.applications.vqes.Linear.call:16
msgid ""
"Layers are built (`build(input_shape)` method) using shape info from "
"`inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:18
#: tensorcircuit.applications.van.MaskedLinear.call:18
#: tensorcircuit.applications.van.ResidualBlock.call:18
#: tensorcircuit.applications.vqes.Linear.call:18
msgid "`input_spec` compatibility is only checked against `inputs`."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:19
#: tensorcircuit.applications.van.MaskedLinear.call:19
#: tensorcircuit.applications.van.ResidualBlock.call:19
#: tensorcircuit.applications.vqes.Linear.call:19
msgid ""
"Mixed precision input casting is only applied to `inputs`. If a layer has"
" tensor arguments in `*args` or `**kwargs`, their casting behavior in "
"mixed precision should be handled manually."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:22
#: tensorcircuit.applications.van.MaskedLinear.call:22
#: tensorcircuit.applications.van.ResidualBlock.call:22
#: tensorcircuit.applications.vqes.Linear.call:22
msgid "The SavedModel input specification is generated using `inputs` only."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:23
#: tensorcircuit.applications.van.MaskedLinear.call:23
#: tensorcircuit.applications.van.ResidualBlock.call:23
#: tensorcircuit.applications.vqes.Linear.call:23
msgid ""
"Integration with various ecosystem packages like TFMOT, TFLite, TF.js, "
"etc is only supported for `inputs` and not for tensors in positional and "
"keyword arguments."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:26
#: tensorcircuit.applications.van.MaskedLinear.call:26
#: tensorcircuit.applications.van.ResidualBlock.call:26
#: tensorcircuit.applications.vqes.Linear.call:26
msgid "*args: Additional positional arguments. May contain tensors, although"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:27
#: tensorcircuit.applications.van.MaskedLinear.call:27
#: tensorcircuit.applications.van.ResidualBlock.call:27
#: tensorcircuit.applications.vqes.Linear.call:27
msgid "this is not recommended, for the reasons above."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:37
#: tensorcircuit.applications.van.MaskedLinear.call:37
#: tensorcircuit.applications.van.ResidualBlock.call:37
#: tensorcircuit.applications.vqes.Linear.call:37
msgid "**kwargs: Additional keyword arguments. May contain tensors, although"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:29
#: tensorcircuit.applications.van.MaskedLinear.call:29
#: tensorcircuit.applications.van.ResidualBlock.call:29
#: tensorcircuit.applications.vqes.Linear.call:29
msgid ""
"this is not recommended, for the reasons above. The following optional "
"keyword arguments are reserved: - `training`: Boolean scalar tensor of "
"Python boolean indicating"
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:32
#: tensorcircuit.applications.van.MaskedLinear.call:32
#: tensorcircuit.applications.van.ResidualBlock.call:32
#: tensorcircuit.applications.vqes.Linear.call:32
msgid "whether the `call` is meant for training or inference."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:33
#: tensorcircuit.applications.van.MaskedLinear.call:33
#: tensorcircuit.applications.van.ResidualBlock.call:33
#: tensorcircuit.applications.vqes.Linear.call:33
msgid ""
"`mask`: Boolean input mask. If the layer's `call()` method takes a `mask`"
" argument, its default value will be set to the mask generated for "
"`inputs` by the previous layer (if `input` did come from a layer that "
"generated a corresponding mask, i.e. if it came from a Keras layer with "
"masking support)."
msgstr ""

#: of tensorcircuit.applications.van.MaskedConv2D.call:40
#: tensorcircuit.applications.van.MaskedLinear.call:40
#: tensorcircuit.applications.van.ResidualBlock.call:40
#: tensorcircuit.applications.vqes.Linear.call:40
msgid "A tensor or list/tuple of tensors."
msgstr ""

#: ../../source/api/applications/vqes.rst:2
msgid "tensorcircuit.applications.vqes"
msgstr ""

#: of tensorcircuit.applications.vqes:1
msgid "Relevant classes for VQNHE"
msgstr ""

#: of tensorcircuit.applications.vqes.JointSchedule:1
msgid ""
"Bases: "
":py:class:`keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule`"
msgstr ""

#: of tensorcircuit.applications.vqes.Linear:1
msgid "Dense layer but with complex weights, used for building complex RBM"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:1
msgid "VQNHE"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.evaluation:5
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:3
#: tensorcircuit.applications.vqes.VQNHE.plain_evaluation:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:7
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:7
#: tensorcircuit.backends.jax_backend.JaxBackend.concat:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:5
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:7
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:9
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:8
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:11
#: tensorcircuit.backends.jax_backend.JaxBackend.max:3
#: tensorcircuit.backends.jax_backend.JaxBackend.max:7
#: tensorcircuit.backends.jax_backend.JaxBackend.min:3
#: tensorcircuit.backends.jax_backend.JaxBackend.min:7
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:6
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:8
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:3
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:5
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:7
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:15
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:3
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:7
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:3
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:36
#: tensorcircuit.circuit.Circuit.expectation:16
#: tensorcircuit.circuit.expectation:42 tensorcircuit.circuit.expectation:50
#: tensorcircuit.circuit.expectation:51
#: tensorcircuit.keras.QuantumLayer.__init__:4
#: tensorcircuit.keras.QuantumLayer.__init__:6
#: tensorcircuit.keras.output_asis_loss:3
#: tensorcircuit.keras.output_asis_loss:5
#: tensorcircuit.keras.output_asis_loss:7 tensorcircuit.quantum.entropy:3
#: tensorcircuit.quantum.entropy:7
#: tensorcircuit.quantum.generate_local_hamiltonian:4
#: tensorcircuit.quantum.generate_local_hamiltonian:8
#: tensorcircuit.quantum.reduced_density_matrix:3
#: tensorcircuit.quantum.reduced_density_matrix:5
#: tensorcircuit.quantum.reduced_density_matrix:9
#: tensorcircuit.simplify.pseudo_contract_between:3
#: tensorcircuit.simplify.pseudo_contract_between:5
#: tensorcircuit.simplify.pseudo_contract_between:7
#: tensorcircuit.templates.graphs.Line1D:3
#: tensorcircuit.templates.graphs.Line1D:7
#: tensorcircuit.templates.measurements.any_measurements:4
#: tensorcircuit.templates.measurements.any_measurements:12
#: tensorcircuit.templates.measurements.sparse_expectation:3
msgid "[description]"
msgstr ""

#: of tensorcircuit.applications.vqes.VQNHE.plain_evaluation:1
msgid "VQE"
msgstr ""

#: ../../source/api/backends.rst:2
msgid "tensorcircuit.backends"
msgstr ""

#: ../../source/api/backends/backend_factory.rst:2
msgid "tensorcircuit.backends.backend_factory"
msgstr ""

#: of tensorcircuit.backends.backend_factory:1
msgid "Backend register"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:1
msgid "Get the `tc.backend` object."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.circuit.Circuit.expectation tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.gates.bmatrix tensorcircuit.keras.load_func
#: tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.eval tensorcircuit.quantum.check_spaces
msgid "Raises"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:5
msgid "Backend doesn't exist for `backend` argument."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:6
#: tensorcircuit.cons.set_tensornetwork_backend:32
msgid "The `tc.backend` object that with all registered universal functions."
msgstr ""

#: ../../source/api/backends/jax_backend.rst:2
msgid "tensorcircuit.backends.jax_backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend:1
msgid "Backend magic inherited from tensornetwork: jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid "Bases: :py:class:`tensornetwork.backends.jax.jax_backend.JaxBackend`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid ""
"See the original backend API at ``jax backend``. "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:1
msgid "Returns the elementwise absolute value of tensor. Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:3
msgid "tensor: An input tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:5
msgid "tensor: Its elementwise absolute value."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:1
msgid "Return the index of maximum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:5
msgid "[description], defaults to 0, different behavior from numpy defaults!"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:1
msgid "Return the index of minimum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:1
msgid "Cast the tensor dtype of a ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:3
#: tensorcircuit.backends.jax_backend.JaxBackend.imag:3
#: tensorcircuit.backends.jax_backend.JaxBackend.real:3
#: tensorcircuit.backends.jax_backend.JaxBackend.size:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:3
msgid "tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:5
msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:7
msgid "``a`` of new dtype"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:31
msgid "[description], defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cond:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:1
msgid ""
"The native cond for XLA compiling, wrapper for ``tf.cond`` and limited "
"functionality of ``jax.lax.cond``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.convert_to_tensor:1
msgid "Convert a np.array or a tensor to a tensor type for the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:1
msgid ""
"Generate the coo format sparse matrix from indices and values, which is "
"the only sparse format supported in different ML backends."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:4
msgid "shape [n, 2] for n non zero values in the returned matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:6
msgid "shape [n]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:8
msgid "Tuple[int, ...]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:1
msgid "Return the expm of ``a``, matrix exponential."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cos:3
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:5
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:3
msgid "tensor in matrix form"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:5
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:5
msgid "matrix exponential of matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:1
msgid "Return the cosine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:5
msgid "cosine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:1
msgid "Return the cumulative sum of the elements along a given axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:5
msgid ""
"The default behavior is the same as numpy, different from tf/torch as "
"cumsum of the flatten 1D array, defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.expm:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:1
msgid "Return the copy of tensor ''a''."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:4
msgid "Return an identity matrix of dimension `dim`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:2
msgid ""
"Depending on specific backends, `dim` has to be either an int (numpy, "
"torch, tensorflow) or a `ShapeType` object (for block-sparse backends). "
"Block-sparse behavior is currently not supported"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:7
msgid ""
"N (int): The dimension of the returned matrix. dtype: The dtype of the "
"returned matrix. M (int): The dimension of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:1
msgid "Return the function which is the grad function of input ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.keras.load_func tensorcircuit.keras.save_func
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor tensorcircuit.quantum.identity
#: tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.simplify.infer_new_shape tensorcircuit.utils.return_partial
#: tensorcircuit.vis.qir2tex tensorcircuit.vis.render_pdf
msgid "Example"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:13
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:13
msgid "the function to be differentiated"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:15
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:15
msgid ""
"the position of args in ``f`` that are to be differentiated, defaults to "
"be 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:17
msgid "the grad function of ``f`` with the same set of arguments as ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:1
msgid "Return 1.j in as a tensor compatible with the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:3
msgid "\"complex64\" or \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:5
msgid "1.j tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:1
msgid "Return the elementwise imaginary value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:5
msgid "imaginary value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:1
#: tensorcircuit.templates.measurements.sparse_expectation:1
msgid "[summary]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:5
msgid "The possible options"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:7
msgid "Sampling output shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:9
msgid ""
"probability for each option in a, defaults to None, as equal probability "
"distribution"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:1
msgid ""
"Call the random normal function with the random state management behind "
"the scene."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:7
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:9
msgid "[description], defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:9
msgid "[description], defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:1
msgid "Determine whether the type of input ``a`` is  ``sparse``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:3
msgid "input matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:5
msgid "a bool indicating whether the matrix ``a`` is sparse"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:1
msgid "Return a boolean on whether ``a`` is a tensor in backend package."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:3
msgid "a tensor to be determined"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:5
msgid "whether ``a`` is a tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:1
msgid "Return the jitted version of function ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:3
msgid "function to be jitted"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:5
msgid "index of args that doesn't regarded as tensor, only work for jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:8
msgid ""
"whether open XLA compilation, only works for tensorflow backend, defaults"
" False since several ops has no XLA correspondence"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:11
msgid "jitted version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:1
msgid ""
"Function that computes a (forward-mode) Jacobian-vector product of ``f``."
" Strictly speaking, this function is value_and_jvp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:4
msgid "The function to compute jvp"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:6
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:7
msgid "input for ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:8
msgid "tangents"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:10
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:10
msgid ""
"(``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as the "
"output of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:1
msgid "Return the kronecker product of two matrices ``a`` and ``b``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:7
msgid "kronecker product of ``a`` and ``b``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:1
msgid "Return the maximum of an array or maximum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:5
#: tensorcircuit.backends.jax_backend.JaxBackend.min:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:5
#: tensorcircuit.keras.QuantumLayer.__init__:10
#: tensorcircuit.quantum.reduced_density_matrix:7
msgid "[description], defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.min:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:1
msgid "Return the minimum of an array or minimum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:1
msgid ""
"Return the numpy array of a tensor ``a``, but may not work in a jitted "
"function."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:5
msgid "numpy array of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:1
msgid ""
"One-hot encodes the given ``a``. Each index in the input ``a`` is encoded"
" as a vector of zeros of length ``num`` with the element at index set to "
"one:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:5
msgid "input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:7
msgid "number of features in onehot dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:9
msgid "onehot tensor with the last extra dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:1
msgid ""
"Return an ones-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends). Block-sparse behavior is currently "
"not supported Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:7
#: tensorcircuit.backends.jax_backend.JaxBackend.zeros:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:8
msgid ""
"shape (int): The dimension of the returned matrix. dtype: The dtype of "
"the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.random_split:1
msgid ""
"A jax like split API, but it doesn't split the key generator for other "
"backends. It is just for a consistent interface of random code; make sure"
" you know what the function actually does. This function is mainly a "
"utility to write backend agnostic code instead of doing magic things."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:1
msgid "Return the elementwise real value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:5
msgid "real value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:1
msgid ""
"Rectified linear unit activation function. Computes the element-wise "
"function:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:4
msgid "\\mathrm{relu}(x)=\\max(x,0)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:9
msgid "Input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:11
msgid "Tensor after relu"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.scatter:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:1
msgid ""
"Roughly equivalent to operand[indices] = updates, indices only support "
"shape with rank 2 for now."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:1
msgid "Set the random state attached to the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:3
msgid "the random seed, defaults to be None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:5
msgid ""
"If set to be true, only get the random state in return instead of setting"
" the state on the backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:1
msgid "Return the  elementwise sine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:5
msgid "sine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:1
msgid "Return the total number of elements in ``a`` in tensor form."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:5
msgid "the total number of elements in ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:1
msgid ""
"Softmax function. Computes the function which rescales elements to the "
"range [0,1] such that the elements along axis sum to 1."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:4
msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:9
msgid "Tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:11
msgid ""
"A dimension along which Softmax will be computed , defaults to None for "
"all axis sum."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stack:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:7
msgid "concatenated tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:1
msgid "Solve the linear system Ax=b and return the solution x."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:3
msgid "The multiplied matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:5
msgid "The resulted matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:7
msgid "The solution of the linear system."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:1
msgid "A sparse matrix multiplies a dense matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:3
msgid "a sparse matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:5
msgid "a dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:7
msgid "dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:1
msgid "Concatenates a sequence of tensors ``a`` along a new dimension ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:3
msgid "List of tensors in the same shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:5
msgid "the stack axis, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:3
msgid "stateful register for each package"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:7
msgid "shape of output sampling tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:11
msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:1
msgid "Uniform random sampler from ``low`` to ``high``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:5
msgid "shape of output sampling tensor, defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:1
msgid "Stop backpropagation from ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.switch:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:1
msgid "``branches[index]()``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:1
msgid "Constructs a tensor by tiling a given tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:5
msgid "1d tensor with length the same as the rank of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.to_dense:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:1
msgid "Convert a sparse matrix to dense tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.to_dense:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:5
msgid "the resulted dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:1
msgid ""
"Find the unique elements and their corresponding counts of the given "
"tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:5
msgid "Unique elements, corresponding counts"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:1
msgid "Return the function which returns the value and grad of ``f``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:17
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:17
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:17
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:17
msgid ""
"the value and grad function of ``f`` with the same set of arguments as "
"``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:1
msgid ""
"Return the VVAG function of ``f``. The inputs for ``f`` is (args[0], "
"args[1], args[2], ...), and the output of ``f`` is a scalar. Suppose "
"VVAG(f) is a function with inputs in the form (vargs[0], args[1], "
"args[2], ...), where vagrs[0] has one extra dimension than args[0] in the"
" first axis and consistent with args[0] in shape for remaining "
"dimensions, i.e. shape(vargs[0]) = [batch] + shape(args[0]). (We only "
"cover cases where ``vectorized_argnums`` defaults to 0 here for "
"demonstration). VVAG(f) returns a tuple as a value tensor with shape "
"[batch, 1] and a gradient tuple with shape: ([batch]+shape(args[argnum]) "
"for argnum in argnums). The gradient for argnums=k is defined as"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:9
msgid ""
"g^k = \\frac{\\partial \\sum_{i\\in batch} f(vargs[0][i], args[1], "
"...)}{\\partial args[k]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:13
msgid "Therefore, if argnums=0, the gradient is reduced to"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:15
msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:19
msgid ""
", which is specifically suitable for batched VQE optimization, where "
"args[0] is the circuit parameters."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:21
msgid "And if argnums=1, the gradient is like"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:23
msgid ""
"g^1_i = \\frac{\\partial \\sum_j f(vargs[0][j], args[1])}{\\partial "
"args[1][i]}\n"
"\n"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:26
msgid ""
", which is suitable for quantum machine learning scenarios, where ``f`` "
"is the loss function, args[0] corresponds to the input data and args[1] "
"corresponds to the weights in the QML model."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:6
msgid ""
"the args to be vectorized, these arguments should share the same batch "
"shape in the fist dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:1
msgid ""
"Function that computes the dot product between a vector v and the "
"Jacobian of the given function at the point given by the inputs. (reverse"
" mode AD relevant) Strictly speaking, this function is value_and_vjp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:5
msgid "the function to carry out vjp calculation"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:9
msgid ""
"value vector or gradient from downstream in reverse mode AD the same "
"shape as return of function ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:12
msgid "(``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:1
msgid ""
"Return the vectorized map or batched version of ``f`` on the first extra "
"axis. The general interface supports ``f`` with multiple arguments and "
"broadcast in the fist dimension."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:4
msgid "function to be broadcasted."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:9
msgid "vmap version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:1
msgid ""
"Return a zeros-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:5
msgid "Block-sparse behavior is currently not supported Args:"
msgstr ""

#: ../../source/api/backends/numpy_backend.rst:2
msgid "tensorcircuit.backends.numpy_backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend:1
msgid "Backend magic inherited from tensornetwork: numpy backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid "Bases: :py:class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid ""
"see the original backend API at `numpy backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
msgstr ""

#: ../../source/api/backends/pytorch_backend.rst:2
msgid "tensorcircuit.backends.pytorch_backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend:1
msgid "Backend magic inherited from tensornetwork: pytorch backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"See the original backend API at ``pytorch backend``. "
"`<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:4
msgid ""
"Note the functionality provided by pytorch backend is incomplete, it "
"currenly lacks native efficicent jit and vmap support."
msgstr ""

#: ../../source/api/backends/tensorflow_backend.rst:2
msgid "tensorcircuit.backends.tensorflow_backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend:1
msgid "Backend magic inherited from tensornetwork: tensorflow backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"Bases: "
":py:class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"See the original backend API at `'tensorflow backend''. "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
msgstr ""

#: ../../source/api/channels.rst:2
msgid "tensorcircuit.channels"
msgstr ""

#: of tensorcircuit.channels:1
msgid "Some common noise quantum channels."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:1
msgid ""
"Return an amplitude damping channel. Notice: Amplitude damping "
"corrspondings to p = 1."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:4
msgid ""
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    \\sqrt{1-\\gamma} & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    \\sqrt{\\gamma} & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:31
msgid "the damping parameter of amplitude (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:33
msgid ":math:`p`"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:35
msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:1
msgid "Return a Depolarizing Channel"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:3
msgid ""
"\\sqrt{1-p_x-p_y-p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_x}\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    1 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_y}\n"
"\\begin{bmatrix}\n"
"    0 & -1j\\\\\n"
"    1j & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & -1\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:30
msgid ":math:`p_x`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:32
msgid ":math:`p_y`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:34
msgid ":math:`p_z`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:36
msgid "Sequences of Gates"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:1
msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:3
msgid ""
"\\sum_{k}^{} K_k \\otimes K_k^{\\dagger}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:6
msgid "A sequence of Gate"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:8
msgid "The corresponding Tensor of the list of Kraus operators"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:1
msgid "Return a phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:18
msgid "The damping parameter of phase (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:20
msgid "A phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.resetchannel:1
#: tensorcircuit.channels.resetchannel:18
msgid "Reset channel"
msgstr ""

#: of tensorcircuit.channels.resetchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:1
msgid "Check identity of a single qubit Kraus operators."
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:3
msgid "Examples:"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:8
msgid ""
"\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:11
msgid "List of Kraus operators."
msgstr ""

#: ../../source/api/circuit.rst:2
msgid "tensorcircuit.circuit"
msgstr ""

#: of tensorcircuit.circuit:1
msgid "Quantum circuit: state simulator"
msgstr ""

#: of tensorcircuit.circuit.Circuit:1
msgid "``Circuit`` class. Simple usage demo below."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply any gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:3
msgid "Qubit number than the gate applies on."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:5
msgid "Parameters for the gate"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cnot gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid "Qubit number than the gate applies on. The matrix for the gate is"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply cr gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply crx gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply cry gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply crz gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cy gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"0.-1.j\\\\    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.-1.j\\\\    "
"0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cz gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp1 gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply h gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    "
"0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    0.70710677+0.j"
" & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply i gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply iswap gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+1.j & 0.+0.j\\\\    0.+0.j & 0.+1.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply r gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rx gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply ry gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rz gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply s gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply sd gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.-1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply swap gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply t gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply td gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply toffoli gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply wroot gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    "
"0.5 & -0.5j & 0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    0.5 & -0.5j & "
"0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply x gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply y gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & 0.-1.j\\\\    0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply z gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:1
msgid "Circuit object based on state simulator."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:3
#: tensorcircuit.mpscircuit.MPSCircuit.__init__:3
msgid "The number of qubits in the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``inputs`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:8
#: tensorcircuit.circuit.Circuit.replace_mps_inputs:3
msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:10
msgid ""
"dict if two qubit gate is ready for split, including parameters for at "
"least one of ``max_singular_values`` and ``max_truncation_err``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:1
msgid ""
"Monte Carlo trajectory simulation of general Kraus channel whose Kraus "
"operators cannot be amplified to unitary operators. For unitary operators"
" composed Kraus channel, :py:meth:`unitary_kraus` is much faster."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:5
msgid ""
"This function is jittable in theory. But only jax+GPU combination is "
"recommended for jit since the graph building time is too long for other "
"backend options; though the running time of the function is very fast for"
" every case."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:9
msgid "list of ``tn.Node`` for Kraus operators"
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:11
msgid "the qubits index that Kraus channel is applied on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:13
msgid ""
"random tensor between 0 or 1, defaults to be None, the random number will"
" be generated automatically"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:1
msgid "Compute the expectation of corresponding operators."
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:10
msgid ""
"operator and its position on the circuit, eg. ``(tc.gates.z(), [1, ]), "
"(tc.gates.x(), [2, ])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:13
msgid ""
"if True, then the wavefunction tensor is cached for further expectation "
"evaluation, defaults to be true"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:17
msgid "Tensor with one element"
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:1
msgid "[WIP], check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:3
msgid "the bool indicating whether the circuit is legal"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:1
msgid "Take measurement to the given quantum lines."
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:16
#: tensorcircuit.circuit.Circuit.measure_jit:1
msgid "measure on which quantum line"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:17
#: tensorcircuit.circuit.Circuit.measure_jit:3
#: tensorcircuit.mpscircuit.MPSCircuit.measure:2
msgid "if true, theoretical probability is also returned"
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:1
msgid ""
"Middle measurement in z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalize the state manually if needed."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:4
msgid "the index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to be 0"
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:1
msgid "Reference: arXiv:1201.3974."
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:3
msgid "sampled bit string and the corresponding theoretical probability"
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:1
msgid "Replace the input state with the circuit structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:3
msgid "Input wavefunction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_mps_inputs:1
msgid ""
"Replace the input state in MPS representation while keep the circuit "
"structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:1
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:1
msgid "Compute the output wavefunction from the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:3
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:3
msgid "the str indicating the form of the output wavefunction"
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:5
msgid "Tensor with the corresponding shape"
msgstr ""

#: of tensorcircuit.circuit.expectation:1
msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`"
msgstr ""

#: of tensorcircuit.circuit.expectation:3
msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:24
msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:44
msgid "[description], defaults to None, which is the same as ``ket``"
msgstr ""

#: of tensorcircuit.circuit.expectation:46
#: tensorcircuit.quantum.generate_local_hamiltonian:6
#: tensorcircuit.templates.graphs.Line1D:5
msgid "[description], defaults to True"
msgstr ""

#: of tensorcircuit.circuit.expectation:48 tensorcircuit.vis.render_pdf:23
msgid "[description], defaults to False"
msgstr ""

#: of tensorcircuit.circuit.to_graphviz:1
msgid ""
"Not an ideal visualization for quantum circuit, but reserve here as a "
"general approch to show tensornetwork [Deperacted, use ``qir2tex "
"instead``]"
msgstr ""

#: ../../source/api/cons.rst:2
msgid "tensorcircuit.cons"
msgstr ""

#: of tensorcircuit.cons:1
msgid "Constants and setups"
msgstr ""

#: of tensorcircuit.cons.get_contractor:1 tensorcircuit.cons.set_contractor:1
msgid ""
"To set runtime contractor of the tensornetwork for a better contraction "
"path."
msgstr ""

#: of tensorcircuit.cons.get_contractor:3 tensorcircuit.cons.set_contractor:3
msgid ""
"\"auto\", \"greedy\", \"branch\", \"plain\", \"tng\", \"custom\", "
"\"custom_stateful\". defaults to None (\"auto\")"
msgstr ""

#: of tensorcircuit.cons.get_contractor:5 tensorcircuit.cons.set_contractor:5
msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:7 tensorcircuit.cons.set_contractor:7
msgid ""
"It is not very useful, as ``memory_limit`` leads to ``branch`` "
"contraction instead of ``greedy`` which is rather slow, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:10 tensorcircuit.cons.set_contractor:10
msgid "Tensornetwork version is too low to support some of the contractors."
msgstr ""

#: of tensorcircuit.cons.get_contractor:11 tensorcircuit.cons.set_contractor:11
msgid "Unknown method options."
msgstr ""

#: of tensorcircuit.cons.get_contractor:12 tensorcircuit.cons.set_contractor:12
msgid "The new tensornetwork with its contractor set."
msgstr ""

#: of tensorcircuit.cons.get_dtype:1 tensorcircuit.cons.set_dtype:1
msgid "To set the runtime numerical dtype of tensors."
msgstr ""

#: of tensorcircuit.cons.get_dtype:3 tensorcircuit.cons.set_dtype:3
msgid ""
"\"complex64\" or \"complex128\", defaults to None, which is equivalent to"
" \"complex64\"."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:1
msgid "The naive state-vector simulator contraction path."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:3
msgid "The list of ``tn.Node``."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:5
msgid "The list of dangling node edges, defaults to be None."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:7
msgid "The ``tn.Node`` after contraction"
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:1
msgid "To set the runtime backend of tensorcircuit."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:3
msgid ""
"Note: ``tc.set_backend`` and ``tc.cons.set_tensornetwork_backend`` are "
"the same."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:27
msgid ""
"\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". defaults to None, which "
"gives the same behavior as "
"``tensornetwork.backend_contextmanager.get_default_backend()``."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:30
msgid "Whether the object should be set as global."
msgstr ""

#: ../../source/api/densitymatrix.rst:2
msgid "tensorcircuit.densitymatrix"
msgstr ""

#: of tensorcircuit.densitymatrix:1
msgid "Quantum circuit class but with density matrix simulator"
msgstr ""

#: ../../source/api/densitymatrix2.rst:2
msgid "tensorcircuit.densitymatrix2"
msgstr ""

#: of tensorcircuit.densitymatrix2:1
msgid "Quantum circuit class but with density matrix simulator: v2"
msgstr ""

#: of tensorcircuit.densitymatrix2.DMCircuit2:1
msgid "Bases: :py:class:`tensorcircuit.densitymatrix.DMCircuit`"
msgstr ""

#: ../../source/api/experimental.rst:2
msgid "tensorcircuit.experimental"
msgstr ""

#: of tensorcircuit.experimental:1
msgid "Experimental features"
msgstr ""

#: ../../source/api/gates.rst:2
msgid "tensorcircuit.gates"
msgstr ""

#: of tensorcircuit.gates:1
msgid ""
"Declarations of single-qubit and two-qubit gates and their corresponding "
"matrix."
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Bases: :py:class:`tensornetwork.network_components.Node`"
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Wrapper of tn.Node, quantum gate"
msgstr ""

#: of tensorcircuit.gates.GateVF:1
msgid "Bases: :py:class:`tensorcircuit.gates.GateF`"
msgstr ""

#: of tensorcircuit.gates.any_gate:1
msgid "Note one should provide the gate with properly reshaped."
msgstr ""

#: of tensorcircuit.gates.any_gate:3
msgid "corresponding gate"
msgstr ""

#: of tensorcircuit.gates.any_gate:5
msgid "The name of the gate."
msgstr ""

#: of tensorcircuit.gates.any_gate:7
msgid "the resulted gate"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:1
msgid "Convert the inputs to Tensor with specified dtype."
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:35
msgid "inputs"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:37
msgid "dtype of the output Tensors"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:39
msgid "List of Tensors"
msgstr ""

#: of tensorcircuit.gates.bmatrix:1
msgid "Returns a LaTeX bmatrix."
msgstr ""

#: of tensorcircuit.gates.bmatrix:13
msgid "Formatted Display:"
msgstr ""

#: of tensorcircuit.gates.bmatrix:15
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
"\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.bmatrix:18
msgid "2D numpy array"
msgstr ""

#: of tensorcircuit.gates.bmatrix:20
msgid "ValueError(\"bmatrix can at most display two dimensions\")"
msgstr ""

#: of tensorcircuit.gates.bmatrix:21
msgid "latex str for bmatrix of array a"
msgstr ""

#: of tensorcircuit.gates.cr_gate:1
msgid ""
"Controlled rotation gate, when the control bit is 1, `rgate` is applied "
"on the target gate."
msgstr ""

#: of tensorcircuit.gates.cr_gate:3 tensorcircuit.gates.cr_gate:5
#: tensorcircuit.gates.cr_gate:7 tensorcircuit.gates.exponential_gate:8
#: tensorcircuit.gates.exponential_gate_unity:9
#: tensorcircuit.gates.iswap_gate:12 tensorcircuit.gates.r_gate:12
#: tensorcircuit.gates.r_gate:14 tensorcircuit.gates.r_gate:16
#: tensorcircuit.gates.rgate_theoretical:12
#: tensorcircuit.gates.rgate_theoretical:14
#: tensorcircuit.gates.rgate_theoretical:16 tensorcircuit.gates.rx_gate:6
#: tensorcircuit.gates.ry_gate:6 tensorcircuit.gates.rz_gate:6
msgid "angle in radians"
msgstr ""

#: of tensorcircuit.gates.cr_gate:10
msgid "CR Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:1
msgid ""
"Faster exponential gate, directly implemented based on RHS, only work "
"when: :math:`U^2` is identity matrix."
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:3
msgid ""
"\\rm{exp}(U) &= e^{-i \\theta U} \\\\\n"
"        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:6
#: tensorcircuit.gates.exponential_gate_unity:7
msgid "input unitary (U)"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:10
#: tensorcircuit.gates.exponential_gate_unity:11
msgid "suffix of Gate name"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:11
#: tensorcircuit.gates.exponential_gate_unity:13
msgid "Exponential Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:1
msgid "Exponential gate."
msgstr ""

#: of tensorcircuit.gates.exponential_gate:3
msgid ""
"\\rm{exp}(U) = e^{-i \\theta U}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:1
msgid "iSwap gate."
msgstr ""

#: of tensorcircuit.gates.iswap_gate:3
msgid ""
"iSwap(\\theta) =\n"
"\\begin{pmatrix}\n"
"    1 & 0 & 0 & 0\\\\\n"
"    0 & \\cos(\\frac{\\pi}{2} \\theta ) & j \\sin(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & j \\sin(\\frac{\\pi}{2} \\theta ) & \\cos(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & 0 & 0 & 1\\\\\n"
"\\end{pmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:14
msgid "iSwap Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:1
msgid "Convert Gate to numpy array."
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:10
msgid "input Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:12
msgid "corresponding Tensor"
msgstr ""

#: of tensorcircuit.gates.meta_gate:1
msgid ""
"Inner helper function to generate gate functions, such as ``z()`` from "
"``_z_matrix``"
msgstr ""

#: of tensorcircuit.gates.r_gate:1
msgid "General single qubit rotation gate"
msgstr ""

#: of tensorcircuit.gates.r_gate:3
msgid ""
"R(\\theta, \\phi, \\alpha) = i \\cos(\\theta) I\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:5
msgid ""
"- i \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:7
msgid ""
"- i \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:9
msgid ""
"- i \\sin(\\theta) \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:19
msgid "R Gate"
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:1
msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:3
msgid "A random single qubit gate"
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:1
msgid "Returns a random two-qubit gate."
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:3
msgid "a random two-qubit gate"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:1
msgid ""
"Rotation gate, which is in matrix exponential form, shall give the same "
"result as `rgate`."
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:3
msgid ""
"mx = \\sin(\\alpha) \\cos(\\phi) X\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:5
msgid ""
"my = \\sin(\\alpha) \\sin(\\phi) Y\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:7
msgid ""
"mz = \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:9
msgid ""
"R(\\theta, \\alpha, \\phi) = e^{-i\\theta (mx+my+mz)}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:18
msgid "Rotation Gate"
msgstr ""

#: of tensorcircuit.gates.rx_gate:1
msgid "Rotation gate along X axis."
msgstr ""

#: of tensorcircuit.gates.rx_gate:3
msgid ""
"RX(\\theta) = e^{-i\\frac{\\theta}{2}X}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rx_gate:8
msgid "RX Gate"
msgstr ""

#: of tensorcircuit.gates.ry_gate:1
msgid "Rotation gate along Y axis."
msgstr ""

#: of tensorcircuit.gates.ry_gate:3
msgid ""
"RY(\\theta) = e^{-i\\frac{\\theta}{2}Y}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.ry_gate:8
msgid "RY Gate"
msgstr ""

#: of tensorcircuit.gates.rz_gate:1
msgid "Rotation gate along Z axis."
msgstr ""

#: of tensorcircuit.gates.rz_gate:3
msgid ""
"RZ(\\theta) = e^{-i\\frac{\\theta}{2}Z}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rz_gate:8
msgid "RZ Gate"
msgstr ""

#: ../../source/api/interfaces.rst:2
msgid "tensorcircuit.interfaces"
msgstr ""

#: of tensorcircuit.interfaces:1
msgid "Interfaces bridging different backends"
msgstr ""

#: ../../source/api/keras.rst:2
msgid "tensorcircuit.keras"
msgstr ""

#: of tensorcircuit.keras:1
msgid "Keras layer for tc quantum function"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:1
msgid ""
"`QuantumLayer` wraps the quantum function `f` as a `keras.Layer` so that "
"tensorcircuit is better integrated with tensorflow."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:8
msgid "[description], defaults to \"glorot_uniform\""
msgstr ""

#: of tensorcircuit.keras.load_func:1
msgid ""
"Load function from the files in the ``tf.savedmodel`` format. We can load"
" several functions at the same time, as they can be the same function of "
"different input shapes."
msgstr ""

#: of tensorcircuit.keras.load_func:24
msgid ""
"The fallback function when all functions loaded are failed, defaults to "
"None"
msgstr ""

#: of tensorcircuit.keras.load_func:26
msgid ""
"When there is not legal loaded function of the input shape and no "
"fallback callable."
msgstr ""

#: of tensorcircuit.keras.load_func:27
msgid ""
"A function that tries all loaded function against the input until the "
"first success one."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:1
msgid "The keras loss function that directly taking the model output as the loss."
msgstr ""

#: of tensorcircuit.keras.save_func:1
msgid "Save tf function in the file (``tf.savedmodel`` format)."
msgstr ""

#: of tensorcircuit.keras.save_func:30
msgid "``tf.function`` ed function with graph building"
msgstr ""

#: of tensorcircuit.keras.save_func:32
msgid "the dir path to save the function"
msgstr ""

#: ../../source/api/mps_base.rst:2
msgid "tensorcircuit.mps_base"
msgstr ""

#: of tensorcircuit.mps_base:1
msgid "FiniteMPS from tensornetwork with bug fixed"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS:1
msgid "Bases: :py:class:`tensornetwork.matrixproductstates.finite_mps.FiniteMPS`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:1
msgid ""
"Apply a two-site gate to an MPS. This routine will in general destroy any"
" canonical form of the state. If a canonical form is needed, the user can"
" restore it using `FiniteMPS.position`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:5
msgid "A two-body gate."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:7
msgid "The first site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:9
msgid "The second site where the gate acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:11
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:5
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:5
#: tensorcircuit.mpscircuit.split_tensor:7
msgid "The maximum number of singular values to keep."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:13
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:7
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:7
#: tensorcircuit.mpscircuit.split_tensor:9
msgid "The maximum allowed truncation error."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:15
msgid ""
"An optional value to choose the MPS tensor at `center_position` to be "
"isometric after the application of the gate. Defaults to `site1`. If the "
"MPS is canonical (i.e.`BaseMPS.center_position != None`), and if the "
"orthogonality center coincides with either `site1` or `site2`,  the "
"orthogonality center will be shifted to `center_position` (`site1` by "
"default). If the orthogonality center does not coincide with `(site1, "
"site2)` then `MPS.center_position` is set to `None`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:24
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:9
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:9
#: tensorcircuit.mpscircuit.split_tensor:11
msgid "Multiply `max_truncation_err` with the largest singular value."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:26
msgid ""
"\"rank of gate is {} but has to be 4\", \"site1 = {} is not between 0 <= "
"site < N - 1 = {}\", \"site2 = {} is not between 1 <= site < N = "
"{}\",\"Found site2 ={}, site1={}. Only nearest neighbor gates are "
"currently supported\", \"f center_position = {center_position} not  f in "
"{(site1, site2)} \", or \"center_position = {}, but gate is applied at "
"sites {}, {}. Truncation should only be done if the gate is applied at "
"the center position of the MPS.\""
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.apply_two_site_gate:32
msgid "A scalar tensor containing the truncated weight of the truncation."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:1
msgid "Measure the expectation value of local operators `ops` site `sites`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:3
msgid "A list Tensors of rank 2; the local operators to be measured."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:5
msgid "Sites where `ops` act."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_local_operator:7
msgid "measurements :math:`\\langle` `ops[n]`:math:`\\rangle` for n in `sites`"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:1
msgid ""
"Compute the correlator :math:`\\langle` `op1[site1], "
"op2[s]`:math:`\\rangle` between `site1` and all sites `s` in `sites2`. If"
" `s == site1`, `op2[s]` will be applied first."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:6
msgid "Tensor of rank 2; the local operator at `site1`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:8
msgid "Tensor of rank 2; the local operator at `sites2`."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:10
msgid "The site where `op1`  acts"
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:12
msgid "Sites where operator `op2` acts."
msgstr ""

#: of tensorcircuit.mps_base.FiniteMPS.measure_two_body_correlator:14
msgid ""
"Correlator :math:`\\langle` `op1[site1], op2[s]`:math:`\\rangle` for `s` "
":math:`\\in` `sites2`."
msgstr ""

#: ../../source/api/mpscircuit.rst:2
msgid "tensorcircuit.mpscircuit"
msgstr ""

#: of tensorcircuit.mpscircuit:1
msgid "Quantum circuit: MPS state simulator"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit:1
msgid "``MPSCircuit`` class. Simple usage demo below."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:1
msgid "MPSCircuit object based on state simulator."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``tensors`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:8
msgid "The center position of MPS, default to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:1
msgid "Apply a general qubit gate on MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:4
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:3
msgid "The Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:6
msgid "Qubit indices of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:5
msgid "\"MPS does not support application of gate on > 2 qubits.\""
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:1
msgid ""
"Apply a double qubit gate on adjacent qubits of Matrix Product States "
"(MPS). Truncation rule is specified by `set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:5
msgid "The first qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:7
msgid "The second qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:10
msgid "Center position of MPS, default is None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:1
msgid ""
"Apply a double qubit gate on MPS. Truncation rule is specified by "
"`set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:1
msgid ""
"Apply a single qubit gate on MPS, and the gate must be unitary; no "
"truncation is needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:3
msgid "gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:5
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:5
msgid "Qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:1
msgid "Compute the conjugate of the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:3
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:11
msgid "The constructed MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy:1
msgid "Copy the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:1
msgid "Copy the current MPS without the tensors."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:1
msgid "Compute the expectation of the corresponding double qubit gate."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:5
msgid "qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:1
msgid ""
"Compute the expectation of the corresponding single qubit gate in the "
"form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:3
msgid "Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:7
msgid "The expectation of the corresponding single qubit gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:1
msgid ""
"Compute the expectation of the direct product of the corresponding two "
"gates."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:3
msgid "First gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:5
msgid "Second gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:7
msgid "Qubit index of the first gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:9
msgid "Qubit index of the second gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:11
msgid "The correlation of the corresponding two qubit gates"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:1
msgid "Construct the MPS from a given wavefunction."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:3
msgid "The given wavefunction (any shape is OK)"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:1
msgid "Compute the expectation of corresponding operators in the form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(gates.Z(), [1]), "
"(gates.X(), [2])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:6
msgid "The expectation of corresponding operators"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:1
msgid "Get the normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:3
msgid "Normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:1
msgid "Check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:3
msgid "Whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.measure:1
msgid "integer indicating the measure on which quantum line"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:1
msgid ""
"Middle measurement in the z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalized the state manually if needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:4
msgid "The index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.normalize:1
msgid "Normalize MPS Circuit according to the center position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:1
msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:4
msgid "The orthogonality center"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:1
msgid "Compute the projection between `other` as bra and `self` as ket."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:3
msgid "ket of the other MPS, which will be converted to bra automatically"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:5
msgid "The projection in form of tensor"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:1
msgid ""
"Set truncation rules when double qubit gates are applied. If nothing is "
"specified, no truncation will take place and the bond dimension will keep"
" growing. For more details, refer to `split_tensor`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:5
msgid "Tensor with shape [1, -1]"
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:1
msgid "Split the tensor by SVD or QR depends on whether a truncation is required."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:3
msgid "The input tensor to split."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:5
msgid "Determine the orthogonal center is on the left tensor or the right tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:13
msgid "Two tensors after splitting"
msgstr ""

#: ../../source/api/quantum.rst:2
msgid "tensorcircuit.quantum"
msgstr ""

#: of tensorcircuit.quantum:1
msgid "Quantum state and operator class backend by tensornetwork"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1 tensorcircuit.quantum.QuScalar:1
#: tensorcircuit.quantum.QuVector:1
msgid "Bases: :py:class:`tensorcircuit.quantum.QuOperator`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1
msgid "Represents an adjoint (row) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:1
msgid ""
"Constructs a new `QuAdjointVector` from a tensor network. This "
"encapsulates an existing tensor network, interpreting it as an adjoint "
"vector (row vector)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:5
#: tensorcircuit.quantum.QuOperator.__init__:9
msgid "The edges of the network to be used as the input edges."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:7
#: tensorcircuit.quantum.QuOperator.__init__:11
#: tensorcircuit.quantum.QuVector.__init__:6
msgid ""
"Nodes used to refer to parts of the tensor network that are not connected"
" to any input or output edges (for example: a scalar factor)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:10
#: tensorcircuit.quantum.QuScalar.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:9
msgid "Optional collection of edges to ignore when performing consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:1
msgid ""
"Construct a `QuAdjointVector` directly from a single tensor. This first "
"wraps the tensor in a `Node`, then constructs the `QuAdjointVector` from "
"that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:27
msgid "The tensor for constructing an QuAdjointVector."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:29
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (input edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:33
msgid "The new constructed QuAdjointVector give from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator:1
msgid ""
"Represents a linear operator via a tensor network. To interpret a tensor "
"network as a linear operator, some of the dangling edges must be "
"designated as `out_edges` (output edges) and the rest as `in_edges` "
"(input edges). Considered as a matrix, the `out_edges` represent the row "
"index and the `in_edges` represent the column index. The (right) action "
"of the operator on another then consists of connecting the `in_edges` of "
"the first operator to the `out_edges` of the second. Can be used to do "
"simple linear algebra with tensor networks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:1
msgid ""
"Creates a new `QuOperator` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a linear operator. The "
"network is checked for consistency: All dangling edges must either be in "
"`out_edges`, `in_edges`, or `ignore_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:4
msgid "The edges of the network to be used as the output edges."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:15
msgid ""
"Optional collection of dangling edges to ignore when performing "
"consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:18
msgid ""
"At least one reference node is required to specify a scalar. None "
"provided!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:1
msgid ""
"The adjoint of the operator. This creates a new `QuOperator` with "
"complex-conjugate copies of all tensors in the network and with the input"
" and output edges switched."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.check_network:1
msgid ""
"Check that the network has the expected dimensionality. This checks that "
"all input and output edges are dangling and that there are no other "
"dangling edges (except any specified in `ignore_edges`). If not, an "
"exception is raised."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:1
msgid ""
"Contract the tensor network in place. This modifies the tensor network "
"representation of the operator (or vector, or scalar), reducing it to a "
"single tensor, without changing the value."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:5
msgid "Manually specify the axis ordering of the final tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:7
msgid "The present object."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:1
msgid ""
"Contracts the tensor network in place and returns the final tensor. Note "
"that this modifies the tensor network representing the operator. The "
"default ordering for the axes of the final tensor is: `*out_edges, "
"*in_edges`. If there are any \"ignored\" edges, their axes come first: "
"`*ignored_edges, *out_edges, *in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:8
msgid ""
"Manually specify the axis ordering of the final tensor. The default "
"ordering is determined by `out_edges` and `in_edges` (see above)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:11
msgid "Node count '{}' > 1 after contraction!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:12
msgid "The final tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:1
msgid ""
"Construct a `QuOperator` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuOperator` from that "
"`Node`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:28
msgid "The tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:30
msgid "The axis indices of `tensor` to use as `out_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:32
msgid "The axis indices of `tensor` to use as `in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:34
msgid "The new operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.nodes:1
msgid "All tensor-network nodes involved in the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.norm:1
msgid ""
"The norm of the operator. This is the 2-norm (also known as the Frobenius"
" or Hilbert-Schmidt norm)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:1
msgid ""
"The partial trace of the operator. Subsystems to trace out are supplied "
"as indices, so that dangling edges are connected to each other as: "
"`out_edges[i] ^ in_edges[i] for i in subsystems_to_trace_out` This does "
"not modify the original network. The original ordering of the remaining "
"subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:8
msgid "Indices of subsystems to trace out."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:10
msgid "A new QuOperator or QuScalar representing the result."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:1
msgid ""
"Tensor product with another operator. Given two operators `A` and `B`, "
"produces a new operator `AB` representing `A`  `B`. The `out_edges` "
"(`in_edges`) of `AB` is simply the concatenation of the `out_edges` "
"(`in_edges`) of `A.copy()` with that of `B.copy()`: `new_out_edges = "
"[*out_edges_A_copy, *out_edges_B_copy]` `new_in_edges = "
"[*in_edges_A_copy, *in_edges_B_copy]`"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:9
msgid "The other operator (`B`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:11
msgid "The result (`AB`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.trace:1
msgid "The trace of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuScalar:1
msgid "Represents a scalar via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:1
msgid ""
"Constructs a new `QuScalar` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a scalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:4
msgid ""
"Nodes used to refer to the tensor network (need not be exhaustive - one "
"node from each disconnected subnetwork is sufficient)."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:1
msgid ""
"Construct a `QuScalar` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuScalar` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:22
msgid "The tensor for constructing a new QuScalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:24
msgid "The new constructed QuScalar from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector:1
msgid "Represents a (column) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuVector.__init__:1
msgid ""
"Constructs a new `QuVector` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a (column) vector."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:1
msgid ""
"Construct a `QuVector` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuVector` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:28
msgid "The tensor for constructing a \"QuVector\"."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:30
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (output edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:34
msgid "The new constructed QuVector from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:1
msgid ""
"Check the vector spaces represented by two lists of edges are compatible."
" The number of edges must be the same and the dimensions of each pair of "
"edges must match. Otherwise, an exception is raised. :param edges_1: List"
" of edges representing a many-body Hilbert space. :type edges_1: "
"Sequence[Edge] :param edges_2: List of edges representing a many-body "
"Hilbert space. :type edges_2: Sequence[Edge]"
msgstr ""

#: of tensorcircuit.quantum.check_spaces:9
msgid ""
"Hilbert-space mismatch: \"Cannot connect {} subsystems with {} "
"subsystems\", or \"Input dimension {} != output dimension {}.\""
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:1
msgid ""
"Eliminates any connected CopyNodes that are identity matrices. This will "
"modify the network represented by `nodes`. Only identities that are "
"connected to other nodes are eliminated."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:5
msgid "Collection of nodes to search."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:7
msgid ""
"The Dictionary mapping remaining Nodes to any replacements, Dictionary "
"specifying all dangling-edge replacements."
msgstr ""

#: of tensorcircuit.quantum.entropy:1
msgid "Compute the entropy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.entropy:5
msgid "[description], defaults to 1e-12"
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:1
msgid ""
"Note: further jit is recommended. For large Hilbert space, sparse "
"Hamiltonian is recommended"
msgstr ""

#: of tensorcircuit.quantum.identity:1
msgid ""
"Construct a 'QuOperator' representing the identity on a given space. "
"Internally, this is done by constructing 'CopyNode's for each edge, with "
"dimension according to 'space'."
msgstr ""

#: of tensorcircuit.quantum.identity:26
msgid ""
"A sequence of integers for the dimensions of the tensor product factors "
"of the space (the edges in the tensor network)."
msgstr ""

#: of tensorcircuit.quantum.identity:29
msgid "The data type (for conversion to dense)."
msgstr ""

#: of tensorcircuit.quantum.identity:31
msgid "The desired identity operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:1
msgid ""
"Simulate the measuring of each qubit of ``p`` in the computational basis,"
" thus producing output like that of ``qiskit``."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:14
msgid ""
"The quantum state, assumed to be normalized, as either a ket or density "
"operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:16
msgid "The number of counts to perform."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:18
msgid ""
"Defaults True. The bool indicating whether the return form is in the form"
" of two array or one of the same length as the ``state`` (if "
"``sparse=False``)."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:21
msgid "The counts for each bit string measured."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:1
msgid ""
"Constructs an appropriately specialized QuOperator. If there are no "
"edges, creates a QuScalar. If the are only output (input) edges, creates "
"a QuVector (QuAdjointVector). Otherwise creates a QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:38
msgid ""
"op = qu.quantum_constructor([], [psi_node[0], psi_node[1]]) >>> "
"show_attributes(op) op.is_scalar()          -> False op.is_vector()"
"          -> False op.is_adjoint_vector()  -> True len(op.out_edges)"
"       -> 0 len(op.in_edges)        -> 2 >>> # psi_node[0] -> "
"op.in_edges[0] >>> # psi_node[1] -> op.in_edges[1]"
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:48
msgid "output edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:50
msgid "in edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:52
msgid ""
"reference nodes for the tensor network (needed if there is a scalar "
"component)."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:55
msgid "edges to ignore when checking the dimensionality of the tensor network."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:58
msgid "The new created QuOperator object."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:1
msgid "Compute the reduced density matrix from the quantum state ``state``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:1
msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:3
msgid "\\mathrm{Tr}(\\prod_i O_i)"
msgstr ""

#: of tensorcircuit.quantum.trace_product:7
msgid "the trace of several inputs"
msgstr ""

#: ../../source/api/simplify.rst:2
msgid "tensorcircuit.simplify"
msgstr ""

#: of tensorcircuit.simplify:1
msgid "Tensornetwork Simplification"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:1
msgid ""
"Get the new shape of two nodes, also supporting to return original shapes"
" of two nodes."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:13
msgid "node one"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:15
msgid "node two"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:17
msgid "Whether to include original shape of two nodes, default is True."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:19
msgid "The new shape of the two nodes."
msgstr ""

#: of tensorcircuit.simplify.pseudo_contract_between:1
msgid ""
"Contract between Node ``a`` and ``b``, with correct shape only and no "
"calculation"
msgstr ""

#: ../../source/api/templates.rst:2
msgid "tensorcircuit.templates"
msgstr ""

#: ../../source/api/templates/blocks.rst:2
msgid "tensorcircuit.templates.blocks"
msgstr ""

#: of tensorcircuit.templates.blocks:1 tensorcircuit.templates.measurements:1
msgid "Shortcuts for measurement patterns on circuit"
msgstr ""

#: ../../source/api/templates/graphs.rst:2
msgid "tensorcircuit.templates.graphs"
msgstr ""

#: of tensorcircuit.templates.graphs:1
msgid "Some common graphs and lattices"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:1
msgid "1D chain with ``n`` sites"
msgstr ""

#: ../../source/api/templates/measurements.rst:2
msgid "tensorcircuit.templates.measurements"
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:1
msgid ""
"This measurements pattern is specifically suitable for vmap. Parameterize"
" the Pauli string to be measured."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:6
msgid ""
"parameter tensors determines what Pauli string to be measured, shape is "
"[nwires, 4] if onehot is False."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:9
msgid ""
"[description], defaults to False. If set to be True, structures will "
"first go through onehot procedure."
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:5
msgid "COO_sparse_matrix"
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:7
msgid "a real and scalar tensor of shape []"
msgstr ""

#: ../../source/api/utils.rst:2
msgid "tensorcircuit.utils"
msgstr ""

#: of tensorcircuit.utils:1
msgid "Helper functions"
msgstr ""

#: of tensorcircuit.utils.return_partial:1
msgid ""
"Return a callable function for output ith parts of the original output "
"along the first axis. Original output supports List and Tensor."
msgstr ""

#: of tensorcircuit.utils.return_partial:20
msgid "The function to be applied this method"
msgstr ""

#: of tensorcircuit.utils.return_partial:22
msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
msgstr ""

#: of tensorcircuit.utils.return_partial:24
msgid "The modified callable function"
msgstr ""

#: ../../source/api/vis.rst:2
msgid "tensorcircuit.vis"
msgstr ""

#: of tensorcircuit.vis:1
msgid "Visualization on circuits"
msgstr ""

#: of tensorcircuit.vis.qir2tex:2
msgid "# TODO(@YHPeter): add examples"
msgstr ""

#: of tensorcircuit.vis.render_pdf:1
msgid ""
"Generate the PDF file with given latex string and filename. Latex command"
" and file path can be specified. When notebook is True, convert the "
"output PDF file to image and return a Image object."
msgstr ""

#: of tensorcircuit.vis.render_pdf:15
msgid "String of latex content"
msgstr ""

#: of tensorcircuit.vis.render_pdf:17
msgid "File name, defaults to random UUID `str(uuid4())`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:19
msgid "Executable Latex command, defaults to `pdflatex`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:21
msgid "File path, defaults to current working place `os.getcwd()`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:25
msgid "if notebook is True, return `Image` object; otherwise return `None`"
msgstr ""

